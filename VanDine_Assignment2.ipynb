{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"C:/Users/lvand/OneDrive/Desktop/Applied Text Mining/M2/M1 Assignment Data/M1 Results/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True, top_n=10) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens) # total tokens\n",
    "    num_unique_tokens = len(set(tokens)) # unique tokens\n",
    "    lexical_diversity = num_unique_tokens / num_tokens if num_tokens > 0 else 0.0 # ratio of unique to total tokens\n",
    "    num_characters = sum(len(token) for token in tokens) # total characters\n",
    "    # Count the most common tokens\n",
    "    token_counts = Counter(tokens)\n",
    "    most_common_tokens = token_counts.most_common(top_n)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(f\"The {num_tokens} most common tokens are:\")\n",
    "        for token, count in most_common_tokens:\n",
    "            print(f\"{token}: {count}\")\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The 13 most common tokens are:\n",
      "text: 3\n",
      "here: 2\n",
      "example: 2\n",
      "is: 1\n",
      "some: 1\n",
      "with: 1\n",
      "other: 1\n",
      "in: 1\n",
      "this: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: In  this scenario, assertion statements are beneficial to detect potential errors in the values returned. An assertion guarantees that what is returned is true, otherwise errors will arise. Assertions are also good practice in general to assist in debugging scenarios and also properly document the coding as it is completed. In a way, assertions provide documentation by clarifying the values that are expected in the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "474346c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics data for artists: ['cher', 'robyn']\n",
      "Twitter descriptions files: ['cher', 'robyn']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>screen_name\\tname\\tid\\tlocation\\tfollowers_cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>hsmcnp\\tCountry Girl\\t35152213\\t\\t1302\\t1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>horrormomy\\tJeny\\t742153090850164742\\tEarth\\t8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>anju79990584\\tanju\\t1496463006451974150\\t\\t13\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>gallionjenna\\tJ\\t3366479914\\t\\t752\\t556\\tcsu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                                        Description\n",
       "0   cher  screen_name\\tname\\tid\\tlocation\\tfollowers_cou...\n",
       "1   cher       hsmcnp\\tCountry Girl\\t35152213\\t\\t1302\\t1014\n",
       "2   cher  horrormomy\\tJeny\\t742153090850164742\\tEarth\\t8...\n",
       "3   cher  anju79990584\\tanju\\t1496463006451974150\\t\\t13\\...\n",
       "4   cher       gallionjenna\\tJ\\t3366479914\\t\\t752\\t556\\tcsu"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set your data location\n",
    "data_location = \"C:/Users/lvand/OneDrive/Desktop/Applied Text Mining/M2/M1 Assignment Data/M1 Results/\"\n",
    "\n",
    "# Subfolders for the lyrics and Twitter data\n",
    "twitter_folder = os.path.join(data_location, \"twitter/\")\n",
    "lyrics_folder = os.path.join(data_location, \"lyrics/\")\n",
    "\n",
    "# Dictionary to store the lyrics data\n",
    "lyrics_data = {}\n",
    "\n",
    "# Read lyrics data\n",
    "for artist_folder in os.listdir(lyrics_folder):\n",
    "    artist_path = os.path.join(lyrics_folder, artist_folder)\n",
    "    \n",
    "    if os.path.isdir(artist_path):\n",
    "        lyrics_data[artist_folder] = {}\n",
    "        for song_file in os.listdir(artist_path):\n",
    "            song_path = os.path.join(artist_path, song_file)\n",
    "            with open(song_path, 'r', encoding='utf-8') as file:\n",
    "                song_name = song_file.replace(\".txt\", \"\")\n",
    "                lyrics_data[artist_folder][song_name] = file.read()\n",
    "\n",
    "# Dictionary to store Twitter descriptions\n",
    "twitter_data = defaultdict(list)\n",
    "\n",
    "# Separate artists\n",
    "artist_files = {\n",
    "    'cher' : 'cher_followers_data.txt',\n",
    "    'robyn' : 'robyn_followers_data.txt'\n",
    "    }\n",
    "\n",
    "# Read Twitter data\n",
    "for artist, file_name in artist_files.items():\n",
    "    artist_path = os.path.join(twitter_folder, file_name)\n",
    "    \n",
    "    if os.path.isfile(artist_path):\n",
    "        with open(artist_path, 'r') as file:\n",
    "            descriptions = file.readlines()\n",
    "            twitter_data[artist] = [desc.strip() for desc in descriptions]\n",
    "\n",
    "# Convert to DF\n",
    "twitter = pd.DataFrame([\n",
    "    {'artist': artist, 'Description': description}\n",
    "    for artist, descriptions in twitter_data.items()\n",
    "    for description in descriptions\n",
    "])\n",
    "\n",
    "# Example usage:\n",
    "print(f\"Lyrics data for artists: {list(lyrics_data.keys())[:2]}\")\n",
    "print(f\"Twitter descriptions files: {list(artist_files)[:]}\")\n",
    "\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9faadd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove punctuation and lowercase the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    \n",
    "    # Tokenize (split on whitespace)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [token for token in tokens if token not in sw]\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b327033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean twitter data here\n",
    "clean_twitter_data = defaultdict(list)\n",
    "for artist, descriptions in twitter_data.items():\n",
    "    for description in descriptions:\n",
    "        clean_twitter_data[artist].append(clean_and_tokenize(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean lyrics data here\n",
    "clean_lyrics_data = {}\n",
    "for artist, songs in lyrics_data.items():\n",
    "    clean_lyrics_data[artist] = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        clean_lyrics_data[artist][song] = clean_and_tokenize(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5af701b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive stats for Twitter descriptions of cher:\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "The 7 most common tokens are:\n",
      "screen_name: 1\n",
      "name: 1\n",
      "id: 1\n",
      "location: 1\n",
      "followers_count: 1\n",
      "friends_count: 1\n",
      "description: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 7, 1.0, 64]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Twitter data\n",
    "artist = list(clean_twitter_data.keys())[0]\n",
    "print(f\"\\nDescriptive stats for Twitter descriptions of {artist}:\")\n",
    "descriptive_stats(clean_twitter_data[artist][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive statistics for cher - cher_88degrees:\n",
      "There are 182 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 831 characters in the data.\n",
      "The lexical diversity is 0.451 in the data.\n",
      "The 182 most common tokens are:\n",
      "cause: 9\n",
      "hot: 8\n",
      "im: 8\n",
      "yeah: 8\n",
      "88: 6\n",
      "degrees: 5\n",
      "deal: 5\n",
      "time: 4\n",
      "lord: 4\n",
      "damn: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_adifferentkindoflovesong:\n",
      "There are 137 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 691 characters in the data.\n",
      "The lexical diversity is 0.299 in the data.\n",
      "The 137 most common tokens are:\n",
      "kind: 17\n",
      "different: 16\n",
      "love: 16\n",
      "song: 16\n",
      "ooh: 14\n",
      "oh: 8\n",
      "part: 3\n",
      "everything: 3\n",
      "higher: 3\n",
      "world: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_afterall:\n",
      "There are 120 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.492 in the data.\n",
      "The 120 most common tokens are:\n",
      "two: 8\n",
      "weve: 6\n",
      "back: 6\n",
      "guess: 5\n",
      "stops: 4\n",
      "starts: 4\n",
      "keep: 4\n",
      "coming: 4\n",
      "hearts: 4\n",
      "angels: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_again:\n",
      "There are 34 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 143 characters in the data.\n",
      "The lexical diversity is 0.824 in the data.\n",
      "The 34 most common tokens are:\n",
      "dont: 3\n",
      "door: 2\n",
      "know: 2\n",
      "never: 2\n",
      "see: 2\n",
      "evening: 1\n",
      "finds: 1\n",
      "ask: 1\n",
      "could: 1\n",
      "try: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_alfie:\n",
      "There are 67 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 339 characters in the data.\n",
      "The lexical diversity is 0.687 in the data.\n",
      "The 67 most common tokens are:\n",
      "alfie: 11\n",
      "love: 4\n",
      "believe: 3\n",
      "whats: 2\n",
      "meant: 2\n",
      "kind: 2\n",
      "theres: 2\n",
      "something: 2\n",
      "youve: 2\n",
      "moment: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_aliveagain:\n",
      "There are 104 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 491 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The 104 most common tokens are:\n",
      "need: 7\n",
      "wanna: 6\n",
      "like: 5\n",
      "alive: 3\n",
      "rain: 3\n",
      "leave: 3\n",
      "chorus: 3\n",
      "coz: 3\n",
      "must: 2\n",
      "keep: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_allbecauseofyou:\n",
      "There are 94 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 461 characters in the data.\n",
      "The lexical diversity is 0.574 in the data.\n",
      "The 94 most common tokens are:\n",
      "heart: 6\n",
      "feel: 6\n",
      "cant: 5\n",
      "way: 5\n",
      "wants: 5\n",
      "break: 5\n",
      "every: 4\n",
      "love: 4\n",
      "sometimes: 3\n",
      "forgotten: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_allireallywanttodo:\n",
      "There are 83 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 462 characters in the data.\n",
      "The lexical diversity is 0.578 in the data.\n",
      "The 83 most common tokens are:\n",
      "baby: 10\n",
      "friends: 10\n",
      "want: 8\n",
      "really: 6\n",
      "aint: 3\n",
      "lookin: 3\n",
      "dont: 2\n",
      "compete: 1\n",
      "beat: 1\n",
      "cheat: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_allornothing:\n",
      "There are 134 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 663 characters in the data.\n",
      "The lexical diversity is 0.321 in the data.\n",
      "The 134 most common tokens are:\n",
      "nothing: 11\n",
      "youre: 11\n",
      "baby: 10\n",
      "dont: 8\n",
      "wanna: 8\n",
      "cant: 6\n",
      "think: 6\n",
      "fooling: 6\n",
      "heart: 5\n",
      "ive: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_amiblue:\n",
      "There are 68 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 241 characters in the data.\n",
      "The lexical diversity is 0.426 in the data.\n",
      "The 68 most common tokens are:\n",
      "blue: 9\n",
      "im: 8\n",
      "one: 6\n",
      "hes: 5\n",
      "time: 4\n",
      "sad: 4\n",
      "oh: 3\n",
      "gone: 3\n",
      "ah: 2\n",
      "said: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_angelsrunning:\n",
      "There are 101 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 490 characters in the data.\n",
      "The lexical diversity is 0.564 in the data.\n",
      "The 101 most common tokens are:\n",
      "thing: 8\n",
      "know: 5\n",
      "bad: 5\n",
      "let: 4\n",
      "go: 4\n",
      "running: 3\n",
      "ive: 3\n",
      "good: 3\n",
      "see: 3\n",
      "angels: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_applesdontfallfarfromthetree:\n",
      "There are 137 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 642 characters in the data.\n",
      "The lexical diversity is 0.540 in the data.\n",
      "The 137 most common tokens are:\n",
      "apples: 7\n",
      "dont: 7\n",
      "fall: 7\n",
      "far: 7\n",
      "tree: 7\n",
      "mamas: 4\n",
      "said: 4\n",
      "hey: 3\n",
      "honey: 3\n",
      "come: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_awomansstory:\n",
      "There are 110 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 490 characters in the data.\n",
      "The lexical diversity is 0.509 in the data.\n",
      "The 110 most common tokens are:\n",
      "love: 12\n",
      "cant: 7\n",
      "one: 5\n",
      "way: 4\n",
      "spend: 4\n",
      "lives: 4\n",
      "every: 3\n",
      "found: 3\n",
      "make: 3\n",
      "mistake: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_aworldwithoutheroes:\n",
      "There are 73 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 390 characters in the data.\n",
      "The lexical diversity is 0.452 in the data.\n",
      "The 73 most common tokens are:\n",
      "without: 11\n",
      "world: 8\n",
      "heroes: 7\n",
      "dont: 6\n",
      "know: 6\n",
      "like: 4\n",
      "never: 2\n",
      "thing: 2\n",
      "youre: 2\n",
      "somethings: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ayounggirluneenfante:\n",
      "There are 123 tokens in the data.\n",
      "There are 93 unique tokens in the data.\n",
      "There are 667 characters in the data.\n",
      "The lexical diversity is 0.756 in the data.\n",
      "The 123 most common tokens are:\n",
      "young: 7\n",
      "girl: 6\n",
      "left: 4\n",
      "love: 4\n",
      "made: 2\n",
      "never: 2\n",
      "sixteen: 2\n",
      "child: 2\n",
      "springtime: 2\n",
      "still: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_backonthestreetagain:\n",
      "There are 110 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 498 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "The 110 most common tokens are:\n",
      "back: 13\n",
      "im: 12\n",
      "street: 7\n",
      "feet: 6\n",
      "get: 3\n",
      "youre: 3\n",
      "lonely: 2\n",
      "met: 2\n",
      "told: 2\n",
      "id: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_bangbang:\n",
      "There are 174 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 752 characters in the data.\n",
      "The lexical diversity is 0.408 in the data.\n",
      "The 174 most common tokens are:\n",
      "bang: 62\n",
      "shot: 10\n",
      "baby: 9\n",
      "ground: 6\n",
      "hit: 4\n",
      "awful: 4\n",
      "sound: 4\n",
      "oh: 4\n",
      "wore: 2\n",
      "would: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_bangbangmybabyshotmedown:\n",
      "There are 100 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 445 characters in the data.\n",
      "The lexical diversity is 0.530 in the data.\n",
      "The 100 most common tokens are:\n",
      "bang: 26\n",
      "shot: 6\n",
      "baby: 3\n",
      "hit: 3\n",
      "ground: 3\n",
      "awful: 3\n",
      "sound: 3\n",
      "wore: 2\n",
      "would: 2\n",
      "always: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_behindthedoor:\n",
      "There are 93 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.581 in the data.\n",
      "The 93 most common tokens are:\n",
      "every: 9\n",
      "behind: 4\n",
      "door: 4\n",
      "asking: 4\n",
      "house: 3\n",
      "street: 3\n",
      "town: 3\n",
      "playing: 3\n",
      "one: 3\n",
      "happy: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_believe:\n",
      "There are 177 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 885 characters in the data.\n",
      "The lexical diversity is 0.294 in the data.\n",
      "The 177 most common tokens are:\n",
      "dont: 12\n",
      "love: 11\n",
      "believe: 10\n",
      "youre: 10\n",
      "strong: 10\n",
      "feel: 9\n",
      "think: 9\n",
      "life: 8\n",
      "something: 8\n",
      "inside: 8\n",
      "\n",
      "Descriptive statistics for cher - cher_bellbottomblues:\n",
      "There are 156 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 665 characters in the data.\n",
      "The lexical diversity is 0.359 in the data.\n",
      "The 156 most common tokens are:\n",
      "want: 16\n",
      "dont: 12\n",
      "fade: 9\n",
      "away: 9\n",
      "wanna: 6\n",
      "arms: 4\n",
      "baby: 4\n",
      "give: 4\n",
      "one: 4\n",
      "day: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_blowininthewind:\n",
      "There are 96 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 449 characters in the data.\n",
      "The lexical diversity is 0.490 in the data.\n",
      "The 96 most common tokens are:\n",
      "many: 10\n",
      "blowin: 7\n",
      "wind: 7\n",
      "yes: 7\n",
      "answer: 6\n",
      "man: 5\n",
      "must: 4\n",
      "times: 3\n",
      "friend: 3\n",
      "people: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_bodytobodyhearttoheart:\n",
      "There are 113 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 526 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "The 113 most common tokens are:\n",
      "body: 26\n",
      "heart: 19\n",
      "im: 4\n",
      "eyes: 3\n",
      "feeling: 3\n",
      "night: 3\n",
      "need: 2\n",
      "breathing: 2\n",
      "melt: 2\n",
      "together: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_bornwiththehunger:\n",
      "There are 84 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 439 characters in the data.\n",
      "The lexical diversity is 0.655 in the data.\n",
      "The 84 most common tokens are:\n",
      "hunger: 11\n",
      "born: 7\n",
      "never: 5\n",
      "youre: 4\n",
      "dies: 4\n",
      "hear: 2\n",
      "one: 2\n",
      "eyes: 2\n",
      "coyote: 1\n",
      "howl: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_borrowedtime:\n",
      "There are 155 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 783 characters in the data.\n",
      "The lexical diversity is 0.342 in the data.\n",
      "The 155 most common tokens are:\n",
      "borrowed: 10\n",
      "time: 10\n",
      "living: 8\n",
      "love: 8\n",
      "another: 8\n",
      "mans: 8\n",
      "woman: 8\n",
      "thats: 8\n",
      "loving: 8\n",
      "like: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_boysandgirls:\n",
      "There are 185 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 880 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "The 185 most common tokens are:\n",
      "boys: 6\n",
      "go: 6\n",
      "girls: 5\n",
      "cant: 5\n",
      "shine: 4\n",
      "shoes: 4\n",
      "run: 4\n",
      "powder: 4\n",
      "nose: 4\n",
      "cause: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_buticantloveyoumore:\n",
      "There are 80 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 403 characters in the data.\n",
      "The lexical diversity is 0.650 in the data.\n",
      "The 80 most common tokens are:\n",
      "love: 6\n",
      "cant: 4\n",
      "morei: 4\n",
      "stop: 4\n",
      "know: 3\n",
      "dont: 2\n",
      "die: 2\n",
      "without: 2\n",
      "moreand: 2\n",
      "would: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_bymyself:\n",
      "There are 102 tokens in the data.\n",
      "There are 31 unique tokens in the data.\n",
      "There are 411 characters in the data.\n",
      "The lexical diversity is 0.304 in the data.\n",
      "The 102 most common tokens are:\n",
      "gotta: 13\n",
      "go: 10\n",
      "im: 9\n",
      "way: 6\n",
      "end: 5\n",
      "gonna: 5\n",
      "feel: 5\n",
      "try: 3\n",
      "like: 3\n",
      "nobody: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_canyoufool:\n",
      "There are 115 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 496 characters in the data.\n",
      "The lexical diversity is 0.539 in the data.\n",
      "The 115 most common tokens are:\n",
      "ya: 8\n",
      "fool: 6\n",
      "cant: 6\n",
      "cher: 5\n",
      "greg: 4\n",
      "take: 4\n",
      "forget: 4\n",
      "well: 3\n",
      "figure: 3\n",
      "cruel: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_carnival:\n",
      "There are 45 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 200 characters in the data.\n",
      "The lexical diversity is 0.622 in the data.\n",
      "The 45 most common tokens are:\n",
      "carnival: 4\n",
      "ill: 4\n",
      "sing: 3\n",
      "time: 3\n",
      "sun: 2\n",
      "heart: 2\n",
      "dream: 2\n",
      "love: 2\n",
      "come: 2\n",
      "way: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_carouselman:\n",
      "There are 146 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 758 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "The 146 most common tokens are:\n",
      "man: 15\n",
      "carousel: 14\n",
      "around: 10\n",
      "know: 5\n",
      "going: 5\n",
      "kept: 4\n",
      "round: 4\n",
      "always: 4\n",
      "id: 4\n",
      "stay: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_catchthewind:\n",
      "There are 79 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 352 characters in the data.\n",
      "The lexical diversity is 0.671 in the data.\n",
      "The 79 most common tokens are:\n",
      "catch: 5\n",
      "wind: 5\n",
      "ah: 4\n",
      "may: 4\n",
      "well: 4\n",
      "try: 4\n",
      "want: 3\n",
      "love: 2\n",
      "behind: 2\n",
      "would: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_chastityssongbandofthieves:\n",
      "There are 124 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 592 characters in the data.\n",
      "The lexical diversity is 0.379 in the data.\n",
      "The 124 most common tokens are:\n",
      "good: 14\n",
      "times: 8\n",
      "long: 7\n",
      "life: 6\n",
      "gonna: 4\n",
      "never: 4\n",
      "feel: 4\n",
      "like: 4\n",
      "expecting: 3\n",
      "band: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_chastitysun:\n",
      "There are 99 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 455 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "The 99 most common tokens are:\n",
      "sun: 4\n",
      "chastity: 3\n",
      "one: 3\n",
      "make: 3\n",
      "hate: 3\n",
      "love: 3\n",
      "torn: 3\n",
      "smile: 2\n",
      "im: 2\n",
      "come: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_chiquitita:\n",
      "There are 136 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 688 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "The 136 most common tokens are:\n",
      "chiquitita: 13\n",
      "sing: 7\n",
      "like: 6\n",
      "new: 5\n",
      "song: 5\n",
      "see: 4\n",
      "tell: 3\n",
      "youre: 3\n",
      "cry: 3\n",
      "try: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_chiquititaspanishversion:\n",
      "There are 183 tokens in the data.\n",
      "There are 85 unique tokens in the data.\n",
      "There are 937 characters in the data.\n",
      "The lexical diversity is 0.464 in the data.\n",
      "The 183 most common tokens are:\n",
      "chiquitita: 14\n",
      "tu: 9\n",
      "que: 8\n",
      "quiero: 7\n",
      "compartir: 6\n",
      "alegr√£a: 6\n",
      "para: 5\n",
      "otra: 5\n",
      "vez: 5\n",
      "en: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_classified1a:\n",
      "There are 73 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 311 characters in the data.\n",
      "The lexical diversity is 0.685 in the data.\n",
      "The 73 most common tokens are:\n",
      "love: 7\n",
      "time: 6\n",
      "one: 4\n",
      "wish: 3\n",
      "know: 2\n",
      "said: 2\n",
      "gonna: 2\n",
      "mrs: 2\n",
      "could: 2\n",
      "see: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_clicksong:\n",
      "There are 22 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 145 characters in the data.\n",
      "The lexical diversity is 0.455 in the data.\n",
      "The 22 most common tokens are:\n",
      "nguqo: 4\n",
      "ngqothwane: 4\n",
      "igqira: 2\n",
      "lendlela: 2\n",
      "sebeqabele: 2\n",
      "gqi: 2\n",
      "thapha: 2\n",
      "bathi: 2\n",
      "click: 1\n",
      "song: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_comeandstaywithme:\n",
      "There are 78 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 342 characters in the data.\n",
      "The lexical diversity is 0.603 in the data.\n",
      "The 78 most common tokens are:\n",
      "ill: 8\n",
      "stay: 7\n",
      "come: 5\n",
      "youll: 5\n",
      "true: 4\n",
      "see: 3\n",
      "life: 2\n",
      "yes: 2\n",
      "leave: 2\n",
      "youre: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_cometoyourwindow:\n",
      "There are 124 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 577 characters in the data.\n",
      "The lexical diversity is 0.702 in the data.\n",
      "The 124 most common tokens are:\n",
      "come: 7\n",
      "window: 4\n",
      "dont: 4\n",
      "let: 4\n",
      "im: 3\n",
      "youre: 3\n",
      "know: 3\n",
      "time: 3\n",
      "leaving: 2\n",
      "early: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_couldvebeenyou:\n",
      "There are 143 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 716 characters in the data.\n",
      "The lexical diversity is 0.434 in the data.\n",
      "The 143 most common tokens are:\n",
      "couldve: 19\n",
      "baby: 13\n",
      "see: 7\n",
      "say: 5\n",
      "standing: 5\n",
      "said: 4\n",
      "smile: 4\n",
      "face: 4\n",
      "remember: 4\n",
      "hes: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_crylikeababy:\n",
      "There are 93 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 430 characters in the data.\n",
      "The lexical diversity is 0.376 in the data.\n",
      "The 93 most common tokens are:\n",
      "cry: 11\n",
      "like: 10\n",
      "baby: 10\n",
      "think: 5\n",
      "love: 5\n",
      "lord: 3\n",
      "knows: 3\n",
      "plaything: 3\n",
      "toy: 3\n",
      "puppet: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_crymyselftosleep:\n",
      "There are 62 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 273 characters in the data.\n",
      "The lexical diversity is 0.371 in the data.\n",
      "The 62 most common tokens are:\n",
      "cry: 11\n",
      "sleep: 8\n",
      "hes: 5\n",
      "gone: 5\n",
      "still: 4\n",
      "night: 3\n",
      "ah: 3\n",
      "fight: 2\n",
      "tears: 2\n",
      "fall: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_dancingqueen:\n",
      "There are 116 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 589 characters in the data.\n",
      "The lexical diversity is 0.483 in the data.\n",
      "The 116 most common tokens are:\n",
      "dancing: 10\n",
      "queen: 10\n",
      "dance: 5\n",
      "digging: 5\n",
      "youre: 4\n",
      "jive: 3\n",
      "time: 3\n",
      "life: 3\n",
      "ooh: 3\n",
      "see: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_dangeroustimes:\n",
      "There are 92 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.620 in the data.\n",
      "The 92 most common tokens are:\n",
      "dangerous: 6\n",
      "times: 6\n",
      "would: 6\n",
      "keep: 4\n",
      "know: 3\n",
      "dont: 2\n",
      "cause: 2\n",
      "could: 2\n",
      "got: 2\n",
      "im: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_dannyboy:\n",
      "There are 80 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 358 characters in the data.\n",
      "The lexical diversity is 0.650 in the data.\n",
      "The 80 most common tokens are:\n",
      "danny: 4\n",
      "boy: 4\n",
      "come: 4\n",
      "oh: 3\n",
      "find: 3\n",
      "youre: 3\n",
      "gonna: 3\n",
      "pipes: 2\n",
      "glen: 2\n",
      "summers: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_darklady:\n",
      "There are 131 tokens in the data.\n",
      "There are 101 unique tokens in the data.\n",
      "There are 655 characters in the data.\n",
      "The lexical diversity is 0.771 in the data.\n",
      "The 131 most common tokens are:\n",
      "dark: 4\n",
      "lady: 4\n",
      "said: 4\n",
      "black: 3\n",
      "chorus: 3\n",
      "till: 3\n",
      "saw: 3\n",
      "fortune: 2\n",
      "queen: 2\n",
      "couldnt: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_davidssong:\n",
      "There are 94 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 462 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "The 94 most common tokens are:\n",
      "could: 12\n",
      "start: 6\n",
      "singing: 6\n",
      "song: 5\n",
      "finish: 4\n",
      "ole: 4\n",
      "love: 4\n",
      "baby: 3\n",
      "havent: 3\n",
      "long: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_disastercake:\n",
      "There are 195 tokens in the data.\n",
      "There are 116 unique tokens in the data.\n",
      "There are 964 characters in the data.\n",
      "The lexical diversity is 0.595 in the data.\n",
      "The 195 most common tokens are:\n",
      "youre: 14\n",
      "disaster: 7\n",
      "read: 7\n",
      "lips: 7\n",
      "cake: 5\n",
      "babe: 5\n",
      "young: 5\n",
      "break: 4\n",
      "cause: 4\n",
      "yeah: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_dixie:\n",
      "There are 104 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 521 characters in the data.\n",
      "The lexical diversity is 0.837 in the data.\n",
      "The 104 most common tokens are:\n",
      "dixie: 4\n",
      "wanna: 3\n",
      "like: 3\n",
      "land: 2\n",
      "cotton: 2\n",
      "man: 2\n",
      "go: 2\n",
      "early: 2\n",
      "morning: 2\n",
      "beside: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_dixiegirl:\n",
      "There are 145 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 744 characters in the data.\n",
      "The lexical diversity is 0.572 in the data.\n",
      "The 145 most common tokens are:\n",
      "dixie: 7\n",
      "girl: 6\n",
      "small: 6\n",
      "day: 4\n",
      "waiting: 3\n",
      "tables: 3\n",
      "passing: 3\n",
      "around: 3\n",
      "youre: 3\n",
      "talk: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_doesanybodyreallyfallinloveanymore:\n",
      "There are 207 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 1028 characters in the data.\n",
      "The lexical diversity is 0.343 in the data.\n",
      "The 207 most common tokens are:\n",
      "anybody: 16\n",
      "really: 16\n",
      "know: 15\n",
      "love: 13\n",
      "somebody: 12\n",
      "hey: 10\n",
      "got: 7\n",
      "fall: 6\n",
      "anymore: 6\n",
      "give: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_doievercrossyourmind:\n",
      "There are 75 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 399 characters in the data.\n",
      "The lexical diversity is 0.373 in the data.\n",
      "The 75 most common tokens are:\n",
      "ever: 16\n",
      "cross: 8\n",
      "mind: 8\n",
      "darling: 7\n",
      "time: 4\n",
      "wonder: 3\n",
      "became: 3\n",
      "somehow: 2\n",
      "youre: 2\n",
      "lonely: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_dontcomearoundtonight:\n",
      "There are 102 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 562 characters in the data.\n",
      "The lexical diversity is 0.794 in the data.\n",
      "The 102 most common tokens are:\n",
      "dont: 4\n",
      "come: 4\n",
      "around: 4\n",
      "im: 3\n",
      "used: 3\n",
      "chorus: 3\n",
      "tonight: 2\n",
      "things: 2\n",
      "tonite: 2\n",
      "every: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_donthideyourlove:\n",
      "There are 124 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 557 characters in the data.\n",
      "The lexical diversity is 0.492 in the data.\n",
      "The 124 most common tokens are:\n",
      "love: 17\n",
      "dont: 15\n",
      "hide: 11\n",
      "care: 4\n",
      "heart: 3\n",
      "fair: 3\n",
      "well: 3\n",
      "baby: 3\n",
      "never: 2\n",
      "let: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_dontthinktwice:\n",
      "There are 134 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 622 characters in the data.\n",
      "The lexical diversity is 0.604 in the data.\n",
      "The 134 most common tokens are:\n",
      "dont: 8\n",
      "aint: 7\n",
      "use: 6\n",
      "think: 5\n",
      "twice: 5\n",
      "baby: 4\n",
      "right: 4\n",
      "im: 3\n",
      "light: 3\n",
      "never: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_donttrytoclosearose:\n",
      "There are 121 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 537 characters in the data.\n",
      "The lexical diversity is 0.463 in the data.\n",
      "The 121 most common tokens are:\n",
      "dont: 5\n",
      "time: 5\n",
      "try: 4\n",
      "close: 4\n",
      "rose: 4\n",
      "say: 4\n",
      "spring: 4\n",
      "sun: 4\n",
      "ah: 3\n",
      "baby: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_dorightwomandorightman:\n",
      "There are 101 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.406 in the data.\n",
      "The 101 most common tokens are:\n",
      "right: 12\n",
      "alls: 10\n",
      "man: 8\n",
      "woman: 6\n",
      "want: 5\n",
      "day: 5\n",
      "gotta: 5\n",
      "night: 5\n",
      "shes: 4\n",
      "take: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_dovelamore:\n",
      "There are 153 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 692 characters in the data.\n",
      "The lexical diversity is 0.399 in the data.\n",
      "The 153 most common tokens are:\n",
      "love: 15\n",
      "song: 9\n",
      "dove: 7\n",
      "ill: 7\n",
      "lamore: 5\n",
      "come: 5\n",
      "keep: 5\n",
      "away: 4\n",
      "baby: 4\n",
      "another: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_dowhatyougottado:\n",
      "There are 112 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.598 in the data.\n",
      "The 112 most common tokens are:\n",
      "see: 6\n",
      "love: 4\n",
      "sweet: 4\n",
      "never: 4\n",
      "come: 4\n",
      "back: 4\n",
      "gotta: 3\n",
      "know: 3\n",
      "make: 3\n",
      "like: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_downdowndown:\n",
      "There are 78 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 389 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "The 78 most common tokens are:\n",
      "rock: 6\n",
      "know: 4\n",
      "wont: 4\n",
      "push: 3\n",
      "mountain: 3\n",
      "step: 3\n",
      "aside: 3\n",
      "begins: 3\n",
      "slide: 3\n",
      "bottom: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_doyoubelieveinmagic:\n",
      "There are 122 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 591 characters in the data.\n",
      "The lexical diversity is 0.598 in the data.\n",
      "The 122 most common tokens are:\n",
      "believe: 16\n",
      "magic: 10\n",
      "like: 6\n",
      "music: 5\n",
      "go: 4\n",
      "free: 3\n",
      "well: 3\n",
      "young: 2\n",
      "girls: 2\n",
      "ill: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_dreambaby:\n",
      "There are 91 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 383 characters in the data.\n",
      "The lexical diversity is 0.440 in the data.\n",
      "The 91 most common tokens are:\n",
      "dream: 8\n",
      "hes: 7\n",
      "oh: 7\n",
      "baby: 4\n",
      "love: 4\n",
      "feel: 4\n",
      "good: 4\n",
      "around: 4\n",
      "pray: 4\n",
      "says: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_dressedtokill:\n",
      "There are 113 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 521 characters in the data.\n",
      "The lexical diversity is 0.460 in the data.\n",
      "The 113 most common tokens are:\n",
      "dressed: 8\n",
      "kill: 8\n",
      "im: 7\n",
      "know: 6\n",
      "one: 4\n",
      "resist: 4\n",
      "baby: 4\n",
      "fall: 3\n",
      "dancing: 3\n",
      "dark: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_earlymorningstrangers:\n",
      "There are 104 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 548 characters in the data.\n",
      "The lexical diversity is 0.481 in the data.\n",
      "The 104 most common tokens are:\n",
      "mornin: 7\n",
      "love: 7\n",
      "early: 6\n",
      "strangers: 6\n",
      "without: 4\n",
      "hope: 4\n",
      "find: 4\n",
      "wake: 3\n",
      "theres: 3\n",
      "life: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_easytobehard:\n",
      "There are 57 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 301 characters in the data.\n",
      "The lexical diversity is 0.404 in the data.\n",
      "The 57 most common tokens are:\n",
      "easy: 14\n",
      "people: 7\n",
      "proud: 4\n",
      "cold: 3\n",
      "say: 3\n",
      "care: 3\n",
      "hard: 2\n",
      "heartless: 2\n",
      "cruel: 2\n",
      "feelings: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_elusivebutterfly:\n",
      "There are 106 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 605 characters in the data.\n",
      "The lexical diversity is 0.689 in the data.\n",
      "The 106 most common tokens are:\n",
      "might: 5\n",
      "something: 5\n",
      "elusive: 3\n",
      "butterfly: 3\n",
      "dreams: 3\n",
      "brightest: 3\n",
      "wind: 2\n",
      "see: 2\n",
      "distant: 2\n",
      "footsteps: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_emotionalfire:\n",
      "There are 143 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 703 characters in the data.\n",
      "The lexical diversity is 0.350 in the data.\n",
      "The 143 most common tokens are:\n",
      "fire: 16\n",
      "emotional: 13\n",
      "cant: 8\n",
      "see: 6\n",
      "baby: 6\n",
      "hold: 6\n",
      "every: 6\n",
      "time: 6\n",
      "feel: 6\n",
      "burning: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_fastcompany:\n",
      "There are 132 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 637 characters in the data.\n",
      "The lexical diversity is 0.462 in the data.\n",
      "The 132 most common tokens are:\n",
      "company: 10\n",
      "fast: 7\n",
      "youre: 7\n",
      "life: 4\n",
      "lord: 4\n",
      "way: 4\n",
      "today: 3\n",
      "hot: 3\n",
      "living: 3\n",
      "fire: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_favouritescars:\n",
      "There are 167 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 862 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "The 167 most common tokens are:\n",
      "love: 16\n",
      "scars: 13\n",
      "favorite: 12\n",
      "loves: 6\n",
      "oh: 6\n",
      "heart: 5\n",
      "like: 4\n",
      "dont: 3\n",
      "covers: 3\n",
      "landslide: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_fernando:\n",
      "There are 143 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 814 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "The 143 most common tokens are:\n",
      "fernando: 18\n",
      "night: 5\n",
      "would: 5\n",
      "friend: 5\n",
      "could: 4\n",
      "hear: 3\n",
      "drums: 3\n",
      "something: 3\n",
      "air: 3\n",
      "stars: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_fernando710922:\n",
      "There are 143 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 813 characters in the data.\n",
      "The lexical diversity is 0.580 in the data.\n",
      "The 143 most common tokens are:\n",
      "fernando: 17\n",
      "night: 5\n",
      "would: 5\n",
      "friend: 5\n",
      "could: 4\n",
      "hear: 3\n",
      "drums: 3\n",
      "something: 3\n",
      "air: 3\n",
      "stars: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_fireandrain:\n",
      "There are 157 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 705 characters in the data.\n",
      "The lexical diversity is 0.522 in the data.\n",
      "The 157 most common tokens are:\n",
      "ive: 12\n",
      "seen: 12\n",
      "thought: 7\n",
      "see: 5\n",
      "time: 5\n",
      "fire: 4\n",
      "rain: 4\n",
      "end: 4\n",
      "id: 4\n",
      "sunny: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_firesofeden:\n",
      "There are 147 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 723 characters in the data.\n",
      "The lexical diversity is 0.537 in the data.\n",
      "The 147 most common tokens are:\n",
      "still: 8\n",
      "fires: 7\n",
      "eden: 7\n",
      "love: 7\n",
      "remember: 6\n",
      "know: 6\n",
      "burn: 6\n",
      "heart: 6\n",
      "mine: 6\n",
      "innocent: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_fittofly:\n",
      "There are 113 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 506 characters in the data.\n",
      "The lexical diversity is 0.726 in the data.\n",
      "The 113 most common tokens are:\n",
      "im: 8\n",
      "fit: 7\n",
      "fly: 7\n",
      "cant: 3\n",
      "see: 3\n",
      "oh: 2\n",
      "man: 2\n",
      "cause: 2\n",
      "blood: 2\n",
      "like: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_flashback:\n",
      "There are 124 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 676 characters in the data.\n",
      "The lexical diversity is 0.597 in the data.\n",
      "The 124 most common tokens are:\n",
      "travis: 12\n",
      "wish: 3\n",
      "could: 3\n",
      "helped: 3\n",
      "somehow: 3\n",
      "wonder: 3\n",
      "late: 3\n",
      "saw: 3\n",
      "pain: 3\n",
      "come: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_forwhatitsworth:\n",
      "There are 118 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 638 characters in the data.\n",
      "The lexical diversity is 0.542 in the data.\n",
      "The 118 most common tokens are:\n",
      "whats: 14\n",
      "stop: 7\n",
      "sound: 7\n",
      "everybody: 7\n",
      "look: 7\n",
      "going: 7\n",
      "hey: 4\n",
      "theres: 3\n",
      "time: 3\n",
      "man: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_games:\n",
      "There are 78 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 404 characters in the data.\n",
      "The lexical diversity is 0.731 in the data.\n",
      "The 78 most common tokens are:\n",
      "whatever: 4\n",
      "games: 3\n",
      "youre: 3\n",
      "love: 3\n",
      "im: 2\n",
      "take: 2\n",
      "running: 2\n",
      "break: 2\n",
      "ooh: 2\n",
      "gonna: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_geronimoscadillac:\n",
      "There are 117 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.513 in the data.\n",
      "The 117 most common tokens are:\n",
      "oh: 10\n",
      "cadillac: 8\n",
      "geronimos: 7\n",
      "back: 7\n",
      "boys: 6\n",
      "take: 6\n",
      "wanna: 6\n",
      "ride: 6\n",
      "geronimo: 4\n",
      "took: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_gimmegimmegimmeamanaftermidnight:\n",
      "There are 156 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 793 characters in the data.\n",
      "The lexical diversity is 0.378 in the data.\n",
      "The 156 most common tokens are:\n",
      "gimme: 33\n",
      "man: 11\n",
      "midnight: 11\n",
      "theres: 4\n",
      "one: 4\n",
      "wont: 4\n",
      "somebody: 4\n",
      "help: 4\n",
      "chase: 4\n",
      "shadows: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_girldontcome:\n",
      "There are 120 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 500 characters in the data.\n",
      "The lexical diversity is 0.283 in the data.\n",
      "The 120 most common tokens are:\n",
      "wait: 12\n",
      "girl: 11\n",
      "dont: 11\n",
      "come: 11\n",
      "wanna: 11\n",
      "see: 8\n",
      "oh: 7\n",
      "yeah: 4\n",
      "date: 2\n",
      "half: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_gitdownguitargroupie:\n",
      "There are 134 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 562 characters in the data.\n",
      "The lexical diversity is 0.328 in the data.\n",
      "The 134 most common tokens are:\n",
      "away: 28\n",
      "wanna: 12\n",
      "take: 8\n",
      "get: 6\n",
      "old: 6\n",
      "dont: 4\n",
      "grow: 4\n",
      "hey: 4\n",
      "shout: 4\n",
      "round: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_giveourloveafightinchance:\n",
      "There are 111 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 509 characters in the data.\n",
      "The lexical diversity is 0.640 in the data.\n",
      "The 111 most common tokens are:\n",
      "love: 8\n",
      "give: 5\n",
      "fightin: 5\n",
      "chance: 5\n",
      "away: 3\n",
      "gotta: 3\n",
      "think: 2\n",
      "say: 2\n",
      "tell: 2\n",
      "run: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_gonow:\n",
      "There are 92 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 373 characters in the data.\n",
      "The lexical diversity is 0.380 in the data.\n",
      "The 92 most common tokens are:\n",
      "go: 18\n",
      "darlin: 7\n",
      "dont: 6\n",
      "see: 5\n",
      "want: 5\n",
      "better: 4\n",
      "tell: 4\n",
      "oh: 3\n",
      "still: 3\n",
      "love: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_gypsiestrampsandthieves:\n",
      "There are 110 tokens in the data.\n",
      "There are 75 unique tokens in the data.\n",
      "There are 575 characters in the data.\n",
      "The lexical diversity is 0.682 in the data.\n",
      "The 110 most common tokens are:\n",
      "chorus: 4\n",
      "tramps: 3\n",
      "thieves: 3\n",
      "money: 3\n",
      "theyd: 3\n",
      "born: 2\n",
      "wagon: 2\n",
      "travellin: 2\n",
      "show: 2\n",
      "mama: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_halfbreed:\n",
      "There are 73 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 391 characters in the data.\n",
      "The lexical diversity is 0.808 in the data.\n",
      "The 73 most common tokens are:\n",
      "man: 3\n",
      "chorus: 3\n",
      "halfbreed: 3\n",
      "cherokee: 2\n",
      "ashamed: 2\n",
      "white: 2\n",
      "always: 2\n",
      "shes: 2\n",
      "since: 2\n",
      "town: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_happinessisjustathingcalledjoe:\n",
      "There are 92 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 457 characters in the data.\n",
      "The lexical diversity is 0.511 in the data.\n",
      "The 92 most common tokens are:\n",
      "joe: 10\n",
      "president: 5\n",
      "happiness: 4\n",
      "thing: 4\n",
      "called: 4\n",
      "seems: 4\n",
      "like: 3\n",
      "know: 3\n",
      "us: 3\n",
      "hes: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_happywasthedaywemet:\n",
      "There are 125 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 568 characters in the data.\n",
      "The lexical diversity is 0.520 in the data.\n",
      "The 125 most common tokens are:\n",
      "happy: 7\n",
      "day: 7\n",
      "met: 6\n",
      "ever: 6\n",
      "never: 5\n",
      "forget: 5\n",
      "oh: 5\n",
      "could: 4\n",
      "away: 3\n",
      "believe: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_hardenoughgettingoveryou:\n",
      "There are 136 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 670 characters in the data.\n",
      "The lexical diversity is 0.529 in the data.\n",
      "The 136 most common tokens are:\n",
      "hard: 7\n",
      "enough: 7\n",
      "getting: 7\n",
      "dont: 5\n",
      "could: 5\n",
      "say: 5\n",
      "goodbye: 5\n",
      "ive: 4\n",
      "think: 4\n",
      "time: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_heaintheavyhesmybrother:\n",
      "There are 70 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 338 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "The 70 most common tokens are:\n",
      "hes: 7\n",
      "brother: 7\n",
      "aint: 5\n",
      "heavy: 5\n",
      "long: 3\n",
      "im: 3\n",
      "road: 2\n",
      "knows: 2\n",
      "strong: 2\n",
      "laden: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_heartofstone:\n",
      "There are 153 tokens in the data.\n",
      "There are 93 unique tokens in the data.\n",
      "There are 774 characters in the data.\n",
      "The lexical diversity is 0.608 in the data.\n",
      "The 153 most common tokens are:\n",
      "heart: 14\n",
      "stone: 10\n",
      "wish: 7\n",
      "dont: 5\n",
      "sometimes: 5\n",
      "mercy: 5\n",
      "hurt: 4\n",
      "made: 4\n",
      "together: 3\n",
      "love: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_hellneverknow:\n",
      "There are 110 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 529 characters in the data.\n",
      "The lexical diversity is 0.645 in the data.\n",
      "The 110 most common tokens are:\n",
      "hell: 9\n",
      "never: 9\n",
      "know: 9\n",
      "mine: 3\n",
      "son: 3\n",
      "night: 3\n",
      "see: 2\n",
      "hand: 2\n",
      "long: 2\n",
      "ago: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_hellonwheels:\n",
      "There are 231 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 1026 characters in the data.\n",
      "The lexical diversity is 0.221 in the data.\n",
      "The 231 most common tokens are:\n",
      "see: 16\n",
      "something: 16\n",
      "go: 12\n",
      "lets: 12\n",
      "roll: 12\n",
      "im: 11\n",
      "hell: 10\n",
      "wheels: 10\n",
      "rock: 9\n",
      "like: 8\n",
      "\n",
      "Descriptive statistics for cher - cher_hewasbeautiful:\n",
      "There are 95 tokens in the data.\n",
      "There are 78 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.821 in the data.\n",
      "The 95 most common tokens are:\n",
      "beautiful: 4\n",
      "like: 3\n",
      "ill: 3\n",
      "made: 2\n",
      "music: 2\n",
      "close: 2\n",
      "eyes: 2\n",
      "see: 2\n",
      "till: 2\n",
      "knew: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_heyjoe:\n",
      "There are 107 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 433 characters in the data.\n",
      "The lexical diversity is 0.299 in the data.\n",
      "The 107 most common tokens are:\n",
      "shot: 11\n",
      "hey: 10\n",
      "joe: 9\n",
      "goin: 6\n",
      "said: 6\n",
      "cause: 4\n",
      "caught: 4\n",
      "messin: 4\n",
      "round: 4\n",
      "heard: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_holdinoutforlove:\n",
      "There are 150 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 707 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "The 150 most common tokens are:\n",
      "holdin: 26\n",
      "love: 16\n",
      "im: 15\n",
      "time: 11\n",
      "mind: 6\n",
      "made: 5\n",
      "mister: 5\n",
      "back: 5\n",
      "something: 2\n",
      "share: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_holysmoke:\n",
      "There are 176 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 835 characters in the data.\n",
      "The lexical diversity is 0.375 in the data.\n",
      "The 176 most common tokens are:\n",
      "holy: 16\n",
      "smoke: 16\n",
      "say: 11\n",
      "get: 6\n",
      "solve: 6\n",
      "could: 5\n",
      "would: 4\n",
      "talk: 3\n",
      "cheap: 3\n",
      "wont: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_homewardbound:\n",
      "There are 106 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 616 characters in the data.\n",
      "The lexical diversity is 0.604 in the data.\n",
      "The 106 most common tokens are:\n",
      "home: 9\n",
      "homeward: 7\n",
      "bound: 7\n",
      "silently: 4\n",
      "evry: 3\n",
      "wish: 3\n",
      "thoughts: 3\n",
      "escaping: 3\n",
      "musics: 3\n",
      "playing: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_houseisnotahome:\n",
      "There are 74 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 330 characters in the data.\n",
      "The lexical diversity is 0.635 in the data.\n",
      "The 74 most common tokens are:\n",
      "house: 7\n",
      "one: 5\n",
      "home: 4\n",
      "room: 4\n",
      "chair: 3\n",
      "still: 3\n",
      "theres: 3\n",
      "us: 3\n",
      "even: 2\n",
      "apart: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_howcanyoumendabrokenheart:\n",
      "There are 88 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 421 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "The 88 most common tokens are:\n",
      "mend: 7\n",
      "broken: 7\n",
      "heart: 5\n",
      "stop: 4\n",
      "man: 3\n",
      "could: 3\n",
      "never: 3\n",
      "days: 2\n",
      "see: 2\n",
      "tomorrow: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_howlonghasthisbeengoingon:\n",
      "There are 88 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 395 characters in the data.\n",
      "The lexical diversity is 0.455 in the data.\n",
      "The 88 most common tokens are:\n",
      "long: 9\n",
      "going: 9\n",
      "oh: 7\n",
      "heaven: 4\n",
      "could: 3\n",
      "ohh: 3\n",
      "like: 3\n",
      "kiss: 3\n",
      "dunce: 3\n",
      "feel: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ibelieve:\n",
      "There are 129 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 643 characters in the data.\n",
      "The lexical diversity is 0.442 in the data.\n",
      "The 129 most common tokens are:\n",
      "believe: 34\n",
      "man: 21\n",
      "got: 8\n",
      "every: 3\n",
      "baby: 3\n",
      "gave: 3\n",
      "flower: 2\n",
      "somewhere: 2\n",
      "someone: 2\n",
      "touch: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_idonthavetosleeptodream:\n",
      "There are 152 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 781 characters in the data.\n",
      "The lexical diversity is 0.395 in the data.\n",
      "The 152 most common tokens are:\n",
      "dream: 15\n",
      "youre: 10\n",
      "dont: 9\n",
      "sleep: 9\n",
      "never: 6\n",
      "ever: 5\n",
      "hardly: 4\n",
      "believe: 4\n",
      "see: 4\n",
      "stand: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_idratherbelieveinyou:\n",
      "There are 86 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 413 characters in the data.\n",
      "The lexical diversity is 0.570 in the data.\n",
      "The 86 most common tokens are:\n",
      "youve: 9\n",
      "loved: 8\n",
      "somebody: 7\n",
      "ive: 5\n",
      "like: 4\n",
      "little: 3\n",
      "aint: 3\n",
      "easy: 3\n",
      "go: 2\n",
      "get: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ifeelsomethingintheairmagicintheair:\n",
      "There are 93 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 437 characters in the data.\n",
      "The lexical diversity is 0.484 in the data.\n",
      "The 93 most common tokens are:\n",
      "feel: 6\n",
      "something: 5\n",
      "air: 5\n",
      "magic: 4\n",
      "used: 4\n",
      "fall: 4\n",
      "even: 3\n",
      "though: 3\n",
      "care: 3\n",
      "god: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ificouldturnbacktime:\n",
      "There are 204 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 888 characters in the data.\n",
      "The lexical diversity is 0.314 in the data.\n",
      "The 204 most common tokens are:\n",
      "could: 19\n",
      "back: 15\n",
      "turn: 11\n",
      "time: 11\n",
      "id: 7\n",
      "youd: 7\n",
      "find: 6\n",
      "way: 6\n",
      "hurt: 6\n",
      "like: 6\n",
      "\n",
      "Descriptive statistics for cher - cher_ifiknewthen:\n",
      "There are 75 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 359 characters in the data.\n",
      "The lexical diversity is 0.640 in the data.\n",
      "The 75 most common tokens are:\n",
      "know: 6\n",
      "knew: 5\n",
      "would: 4\n",
      "many: 4\n",
      "cowboy: 3\n",
      "lady: 3\n",
      "gran: 3\n",
      "dont: 2\n",
      "think: 2\n",
      "love: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ifoundsomeone:\n",
      "There are 134 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 678 characters in the data.\n",
      "The lexical diversity is 0.425 in the data.\n",
      "The 134 most common tokens are:\n",
      "away: 9\n",
      "take: 8\n",
      "baby: 7\n",
      "since: 7\n",
      "youve: 7\n",
      "gone: 7\n",
      "found: 6\n",
      "someone: 5\n",
      "eyes: 4\n",
      "maybe: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_ifoundyoulove:\n",
      "There are 147 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 694 characters in the data.\n",
      "The lexical diversity is 0.320 in the data.\n",
      "The 147 most common tokens are:\n",
      "love: 21\n",
      "gonna: 18\n",
      "found: 12\n",
      "hes: 6\n",
      "new: 5\n",
      "true: 5\n",
      "looking: 4\n",
      "life: 4\n",
      "filled: 4\n",
      "well: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_igotitbadandthataintgood:\n",
      "There are 59 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 239 characters in the data.\n",
      "The lexical diversity is 0.475 in the data.\n",
      "The 59 most common tokens are:\n",
      "bad: 13\n",
      "got: 7\n",
      "aint: 5\n",
      "good: 4\n",
      "way: 2\n",
      "heart: 2\n",
      "know: 2\n",
      "crying: 2\n",
      "love: 2\n",
      "ah: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_igotosleep:\n",
      "There are 72 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 345 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "The 72 most common tokens are:\n",
      "sleep: 13\n",
      "go: 7\n",
      "imagine: 6\n",
      "youre: 6\n",
      "look: 2\n",
      "day: 2\n",
      "alone: 2\n",
      "pillow: 1\n",
      "dream: 1\n",
      "though: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_igotyoubabe:\n",
      "There are 115 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 467 characters in the data.\n",
      "The lexical diversity is 0.513 in the data.\n",
      "The 115 most common tokens are:\n",
      "got: 25\n",
      "babe: 15\n",
      "dont: 4\n",
      "say: 3\n",
      "wont: 3\n",
      "know: 2\n",
      "thats: 2\n",
      "cause: 2\n",
      "love: 2\n",
      "im: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ihatetosleepalone:\n",
      "There are 59 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 267 characters in the data.\n",
      "The lexical diversity is 0.559 in the data.\n",
      "The 59 most common tokens are:\n",
      "alone: 6\n",
      "hate: 5\n",
      "sleep: 5\n",
      "know: 3\n",
      "could: 2\n",
      "love: 2\n",
      "known: 2\n",
      "leave: 2\n",
      "somebody: 2\n",
      "new: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ihopeyoufindit:\n",
      "There are 132 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 675 characters in the data.\n",
      "The lexical diversity is 0.462 in the data.\n",
      "The 132 most common tokens are:\n",
      "hope: 15\n",
      "find: 8\n",
      "youre: 6\n",
      "know: 5\n",
      "id: 3\n",
      "words: 3\n",
      "looking: 3\n",
      "everything: 3\n",
      "dreamed: 3\n",
      "life: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_iknowyoudontloveme:\n",
      "There are 45 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 168 characters in the data.\n",
      "The lexical diversity is 0.400 in the data.\n",
      "The 45 most common tokens are:\n",
      "love: 7\n",
      "uh: 6\n",
      "yes: 5\n",
      "know: 4\n",
      "dont: 4\n",
      "ill: 3\n",
      "say: 3\n",
      "baby: 2\n",
      "anything: 2\n",
      "uhuhuh: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_illneverstoplovingyou:\n",
      "There are 110 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 540 characters in the data.\n",
      "The lexical diversity is 0.536 in the data.\n",
      "The 110 most common tokens are:\n",
      "stop: 7\n",
      "ill: 6\n",
      "never: 6\n",
      "loving: 6\n",
      "one: 4\n",
      "time: 4\n",
      "broke: 3\n",
      "promises: 3\n",
      "sometimes: 3\n",
      "lied: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_ilovemakinlovetoyou:\n",
      "There are 111 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 510 characters in the data.\n",
      "The lexical diversity is 0.604 in the data.\n",
      "The 111 most common tokens are:\n",
      "love: 16\n",
      "making: 6\n",
      "oh: 6\n",
      "ooh: 5\n",
      "dont: 3\n",
      "makes: 3\n",
      "chorus: 3\n",
      "honey: 2\n",
      "sweet: 2\n",
      "stop: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_imblowinaway:\n",
      "There are 50 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 240 characters in the data.\n",
      "The lexical diversity is 0.720 in the data.\n",
      "The 50 most common tokens are:\n",
      "love: 4\n",
      "im: 3\n",
      "blowin: 3\n",
      "away: 3\n",
      "ive: 2\n",
      "blind: 2\n",
      "find: 2\n",
      "cause: 2\n",
      "shadows: 2\n",
      "romanced: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_iminthemiddle:\n",
      "There are 121 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 646 characters in the data.\n",
      "The lexical diversity is 0.364 in the data.\n",
      "The 121 most common tokens are:\n",
      "dont: 12\n",
      "im: 10\n",
      "middle: 9\n",
      "something: 8\n",
      "understand: 8\n",
      "cause: 4\n",
      "reasons: 4\n",
      "living: 4\n",
      "slip: 4\n",
      "right: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_impossibledream:\n",
      "There are 72 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 393 characters in the data.\n",
      "The lexical diversity is 0.861 in the data.\n",
      "The 72 most common tokens are:\n",
      "dream: 3\n",
      "impossible: 2\n",
      "fight: 2\n",
      "right: 2\n",
      "reach: 2\n",
      "unreachable: 2\n",
      "quest: 2\n",
      "star: 2\n",
      "matter: 2\n",
      "unbeatable: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_inforthenight:\n",
      "There are 77 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 397 characters in the data.\n",
      "The lexical diversity is 0.727 in the data.\n",
      "The 77 most common tokens are:\n",
      "like: 4\n",
      "chorus: 3\n",
      "night: 2\n",
      "mama: 2\n",
      "used: 2\n",
      "weathers: 2\n",
      "kind: 2\n",
      "cold: 2\n",
      "dont: 2\n",
      "care: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_iparalyze:\n",
      "There are 146 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 719 characters in the data.\n",
      "The lexical diversity is 0.425 in the data.\n",
      "The 146 most common tokens are:\n",
      "got: 9\n",
      "way: 8\n",
      "im: 6\n",
      "take: 6\n",
      "paralyze: 5\n",
      "like: 5\n",
      "time: 4\n",
      "body: 4\n",
      "soul: 4\n",
      "immobilize: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_isawamanandhedancedwithhiswife:\n",
      "There are 87 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 412 characters in the data.\n",
      "The lexical diversity is 0.724 in the data.\n",
      "The 87 most common tokens are:\n",
      "danced: 7\n",
      "saw: 4\n",
      "man: 3\n",
      "wife: 3\n",
      "kept: 3\n",
      "walked: 2\n",
      "song: 2\n",
      "heard: 2\n",
      "chorus: 2\n",
      "dancin: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_island:\n",
      "There are 68 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 348 characters in the data.\n",
      "The lexical diversity is 0.603 in the data.\n",
      "The 68 most common tokens are:\n",
      "cant: 4\n",
      "get: 4\n",
      "island: 3\n",
      "need: 3\n",
      "way: 3\n",
      "think: 2\n",
      "cause: 2\n",
      "honey: 2\n",
      "could: 2\n",
      "see: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_italladdsupnow:\n",
      "There are 70 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 324 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "The 70 most common tokens are:\n",
      "love: 4\n",
      "adds: 3\n",
      "way: 2\n",
      "look: 2\n",
      "heart: 2\n",
      "kind: 2\n",
      "two: 2\n",
      "use: 1\n",
      "things: 1\n",
      "different: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_itgetsmewhereiwanttogo:\n",
      "There are 56 tokens in the data.\n",
      "There are 31 unique tokens in the data.\n",
      "There are 260 characters in the data.\n",
      "The lexical diversity is 0.554 in the data.\n",
      "The 56 most common tokens are:\n",
      "eyes: 6\n",
      "gets: 4\n",
      "go: 4\n",
      "around: 4\n",
      "many: 4\n",
      "wanna: 3\n",
      "life: 3\n",
      "ride: 2\n",
      "world: 2\n",
      "corner: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ithrewitallaway:\n",
      "There are 69 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.768 in the data.\n",
      "The 69 most common tokens are:\n",
      "away: 6\n",
      "threw: 4\n",
      "love: 4\n",
      "dont: 2\n",
      "cant: 2\n",
      "take: 2\n",
      "one: 2\n",
      "throw: 2\n",
      "held: 1\n",
      "arms: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_itmightaswellstaymondayfromnowon:\n",
      "There are 89 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 433 characters in the data.\n",
      "The lexical diversity is 0.472 in the data.\n",
      "The 89 most common tokens are:\n",
      "monday: 8\n",
      "might: 7\n",
      "well: 7\n",
      "stay: 7\n",
      "nothing: 6\n",
      "got: 2\n",
      "dont: 2\n",
      "cause: 2\n",
      "seems: 2\n",
      "one: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_itsacryinshame:\n",
      "There are 137 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 631 characters in the data.\n",
      "The lexical diversity is 0.511 in the data.\n",
      "The 137 most common tokens are:\n",
      "shame: 12\n",
      "cryin: 11\n",
      "love: 11\n",
      "dont: 10\n",
      "happen: 9\n",
      "walk: 8\n",
      "door: 8\n",
      "im: 2\n",
      "memory: 2\n",
      "baby: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_itsamansmansmansworld:\n",
      "There are 102 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 482 characters in the data.\n",
      "The lexical diversity is 0.431 in the data.\n",
      "The 102 most common tokens are:\n",
      "man: 10\n",
      "mans: 7\n",
      "hes: 7\n",
      "lost: 7\n",
      "nothing: 6\n",
      "world: 5\n",
      "made: 5\n",
      "girl: 4\n",
      "wouldnt: 3\n",
      "without: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_itsnotunusual:\n",
      "There are 56 tokens in the data.\n",
      "There are 31 unique tokens in the data.\n",
      "There are 278 characters in the data.\n",
      "The lexical diversity is 0.554 in the data.\n",
      "The 56 most common tokens are:\n",
      "unusual: 9\n",
      "anyone: 6\n",
      "love: 5\n",
      "see: 3\n",
      "find: 3\n",
      "anytime: 2\n",
      "ever: 2\n",
      "want: 2\n",
      "happens: 2\n",
      "fun: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_itstoolatetolovemenow:\n",
      "There are 84 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 366 characters in the data.\n",
      "The lexical diversity is 0.345 in the data.\n",
      "The 84 most common tokens are:\n",
      "late: 11\n",
      "love: 11\n",
      "say: 4\n",
      "bad: 4\n",
      "know: 4\n",
      "day: 4\n",
      "turned: 4\n",
      "want: 2\n",
      "great: 2\n",
      "need: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_iwalkalone:\n",
      "There are 191 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 892 characters in the data.\n",
      "The lexical diversity is 0.398 in the data.\n",
      "The 191 most common tokens are:\n",
      "time: 31\n",
      "theres: 17\n",
      "walk: 14\n",
      "alone: 14\n",
      "gotta: 10\n",
      "got: 6\n",
      "oh: 4\n",
      "ive: 4\n",
      "dance: 3\n",
      "laugh: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_iwalkonguildedsplinters:\n",
      "There are 161 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 770 characters in the data.\n",
      "The lexical diversity is 0.416 in the data.\n",
      "The 161 most common tokens are:\n",
      "come: 24\n",
      "till: 16\n",
      "burn: 16\n",
      "walk: 12\n",
      "guilded: 10\n",
      "splinters: 10\n",
      "get: 8\n",
      "see: 3\n",
      "poison: 2\n",
      "ill: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_iwantyou:\n",
      "There are 102 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 501 characters in the data.\n",
      "The lexical diversity is 0.716 in the data.\n",
      "The 102 most common tokens are:\n",
      "want: 17\n",
      "bad: 4\n",
      "honey: 4\n",
      "wait: 3\n",
      "wasnt: 2\n",
      "knows: 2\n",
      "doesnt: 2\n",
      "like: 2\n",
      "took: 2\n",
      "guilty: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_iwasntready:\n",
      "There are 67 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.537 in the data.\n",
      "The 67 most common tokens are:\n",
      "wasnt: 6\n",
      "ready: 6\n",
      "walked: 6\n",
      "last: 3\n",
      "night: 3\n",
      "heaven: 3\n",
      "hurt: 3\n",
      "guess: 3\n",
      "love: 3\n",
      "today: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_iwillwaitforyou:\n",
      "There are 54 tokens in the data.\n",
      "There are 24 unique tokens in the data.\n",
      "There are 280 characters in the data.\n",
      "The lexical diversity is 0.444 in the data.\n",
      "The 54 most common tokens are:\n",
      "wait: 8\n",
      "till: 5\n",
      "forever: 4\n",
      "takes: 3\n",
      "thousand: 3\n",
      "summers: 3\n",
      "youre: 2\n",
      "back: 2\n",
      "beside: 2\n",
      "im: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_iwouldnttreatadogthewayyoutreatedme:\n",
      "There are 99 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 455 characters in the data.\n",
      "The lexical diversity is 0.586 in the data.\n",
      "The 99 most common tokens are:\n",
      "way: 8\n",
      "treated: 8\n",
      "wouldnt: 7\n",
      "dog: 7\n",
      "treat: 6\n",
      "baby: 4\n",
      "got: 3\n",
      "need: 2\n",
      "time: 2\n",
      "love: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_jolsonmedley:\n",
      "There are 122 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 555 characters in the data.\n",
      "The lexical diversity is 0.574 in the data.\n",
      "The 122 most common tokens are:\n",
      "sonny: 6\n",
      "boy: 6\n",
      "im: 6\n",
      "forsake: 4\n",
      "coming: 4\n",
      "mammy: 3\n",
      "little: 3\n",
      "mam: 3\n",
      "sing: 3\n",
      "knee: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_julie:\n",
      "There are 124 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 586 characters in the data.\n",
      "The lexical diversity is 0.524 in the data.\n",
      "The 124 most common tokens are:\n",
      "julie: 31\n",
      "lying: 6\n",
      "im: 5\n",
      "youre: 5\n",
      "well: 4\n",
      "taunt: 3\n",
      "haunt: 3\n",
      "oh: 3\n",
      "bitch: 3\n",
      "away: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_justenoughtokeepmehanginon:\n",
      "There are 94 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 464 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "The 94 most common tokens are:\n",
      "enough: 16\n",
      "keep: 9\n",
      "hangin: 8\n",
      "ah: 5\n",
      "honey: 4\n",
      "got: 3\n",
      "love: 3\n",
      "true: 2\n",
      "like: 2\n",
      "im: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_justlikejessejames:\n",
      "There are 210 tokens in the data.\n",
      "There are 105 unique tokens in the data.\n",
      "There are 965 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "The 210 most common tokens are:\n",
      "gonna: 10\n",
      "youre: 9\n",
      "come: 9\n",
      "baby: 9\n",
      "tonight: 8\n",
      "im: 8\n",
      "like: 5\n",
      "jesse: 5\n",
      "james: 5\n",
      "til: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_justthisonetime:\n",
      "There are 103 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 472 characters in the data.\n",
      "The lexical diversity is 0.621 in the data.\n",
      "The 103 most common tokens are:\n",
      "one: 8\n",
      "time: 8\n",
      "ive: 5\n",
      "got: 4\n",
      "believe: 4\n",
      "oh: 4\n",
      "need: 4\n",
      "someone: 3\n",
      "could: 2\n",
      "breathe: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_justwhativebeenlookinfor:\n",
      "There are 77 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 391 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "The 77 most common tokens are:\n",
      "ive: 6\n",
      "youre: 5\n",
      "looking: 5\n",
      "never: 3\n",
      "time: 3\n",
      "love: 3\n",
      "mind: 3\n",
      "something: 2\n",
      "thought: 2\n",
      "id: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_kisstokiss:\n",
      "There are 166 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 811 characters in the data.\n",
      "The lexical diversity is 0.476 in the data.\n",
      "The 166 most common tokens are:\n",
      "kiss: 36\n",
      "makin: 4\n",
      "miss: 4\n",
      "hiding: 4\n",
      "daylight: 4\n",
      "right: 4\n",
      "gonna: 3\n",
      "cause: 3\n",
      "cant: 3\n",
      "live: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_knockonwood:\n",
      "There are 127 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 652 characters in the data.\n",
      "The lexical diversity is 0.339 in the data.\n",
      "The 127 most common tokens are:\n",
      "knock: 28\n",
      "better: 12\n",
      "wood: 11\n",
      "love: 7\n",
      "thunder: 6\n",
      "lightning: 6\n",
      "think: 6\n",
      "baby: 4\n",
      "like: 3\n",
      "way: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_laplane:\n",
      "There are 144 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 692 characters in the data.\n",
      "The lexical diversity is 0.451 in the data.\n",
      "The 144 most common tokens are:\n",
      "get: 16\n",
      "im: 12\n",
      "coming: 6\n",
      "la: 5\n",
      "plane: 5\n",
      "tired: 5\n",
      "home: 4\n",
      "rain: 4\n",
      "warm: 4\n",
      "miles: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_laybabylay:\n",
      "There are 121 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 508 characters in the data.\n",
      "The lexical diversity is 0.421 in the data.\n",
      "The 121 most common tokens are:\n",
      "lay: 20\n",
      "stay: 12\n",
      "baby: 11\n",
      "across: 6\n",
      "big: 6\n",
      "brass: 6\n",
      "bed: 6\n",
      "night: 3\n",
      "make: 2\n",
      "woman: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_letmedowneasy:\n",
      "There are 65 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 256 characters in the data.\n",
      "The lexical diversity is 0.323 in the data.\n",
      "The 65 most common tokens are:\n",
      "let: 16\n",
      "easy: 7\n",
      "youre: 6\n",
      "ah: 6\n",
      "gonna: 5\n",
      "dont: 5\n",
      "please: 3\n",
      "set: 2\n",
      "slow: 2\n",
      "baby: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_letthisbealessontoyou:\n",
      "There are 144 tokens in the data.\n",
      "There are 85 unique tokens in the data.\n",
      "There are 699 characters in the data.\n",
      "The lexical diversity is 0.590 in the data.\n",
      "The 144 most common tokens are:\n",
      "shes: 13\n",
      "pretty: 9\n",
      "tied: 9\n",
      "hangin: 6\n",
      "upside: 6\n",
      "tell: 4\n",
      "right: 4\n",
      "nooh: 4\n",
      "ride: 3\n",
      "cant: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_lietome:\n",
      "There are 114 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 526 characters in the data.\n",
      "The lexical diversity is 0.596 in the data.\n",
      "The 114 most common tokens are:\n",
      "dont: 5\n",
      "lie: 4\n",
      "youre: 4\n",
      "us: 4\n",
      "one: 4\n",
      "oh: 3\n",
      "tell: 3\n",
      "till: 3\n",
      "look: 3\n",
      "wanna: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_likearollingstone:\n",
      "There are 149 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 794 characters in the data.\n",
      "The lexical diversity is 0.711 in the data.\n",
      "The 149 most common tokens are:\n",
      "like: 8\n",
      "feel: 6\n",
      "rolling: 4\n",
      "stone: 4\n",
      "used: 4\n",
      "without: 4\n",
      "home: 4\n",
      "complete: 4\n",
      "unknown: 4\n",
      "direction: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_livinginahousedivided:\n",
      "There are 52 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 275 characters in the data.\n",
      "The lexical diversity is 0.731 in the data.\n",
      "The 52 most common tokens are:\n",
      "living: 3\n",
      "sad: 3\n",
      "house: 2\n",
      "divided: 2\n",
      "look: 2\n",
      "us: 2\n",
      "long: 2\n",
      "ago: 2\n",
      "perfect: 2\n",
      "end: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_longdistanceloveaffair:\n",
      "There are 130 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 639 characters in the data.\n",
      "The lexical diversity is 0.431 in the data.\n",
      "The 130 most common tokens are:\n",
      "love: 14\n",
      "long: 9\n",
      "affair: 9\n",
      "distant: 8\n",
      "station: 8\n",
      "im: 6\n",
      "got: 5\n",
      "romance: 4\n",
      "youre: 4\n",
      "always: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_lookatme:\n",
      "There are 46 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 200 characters in the data.\n",
      "The lexical diversity is 0.457 in the data.\n",
      "The 46 most common tokens are:\n",
      "look: 7\n",
      "tell: 6\n",
      "see: 6\n",
      "nights: 2\n",
      "long: 2\n",
      "days: 2\n",
      "come: 2\n",
      "strong: 2\n",
      "pain: 2\n",
      "borne: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_loveandunderstanding:\n",
      "There are 154 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 826 characters in the data.\n",
      "The lexical diversity is 0.370 in the data.\n",
      "The 154 most common tokens are:\n",
      "enough: 21\n",
      "love: 18\n",
      "understanding: 9\n",
      "theres: 6\n",
      "got: 5\n",
      "oh: 5\n",
      "world: 4\n",
      "need: 4\n",
      "could: 4\n",
      "use: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_loveenough:\n",
      "There are 105 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 534 characters in the data.\n",
      "The lexical diversity is 0.552 in the data.\n",
      "The 105 most common tokens are:\n",
      "love: 10\n",
      "enough: 10\n",
      "gotta: 6\n",
      "comes: 4\n",
      "goes: 3\n",
      "ever: 3\n",
      "wont: 3\n",
      "long: 3\n",
      "given: 3\n",
      "youll: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_lovehurts:\n",
      "There are 110 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 500 characters in the data.\n",
      "The lexical diversity is 0.464 in the data.\n",
      "The 110 most common tokens are:\n",
      "love: 19\n",
      "hurts: 13\n",
      "know: 6\n",
      "lot: 5\n",
      "isnt: 4\n",
      "true: 4\n",
      "ive: 3\n",
      "learned: 3\n",
      "take: 2\n",
      "pain: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_lovehurts312103:\n",
      "There are 89 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 408 characters in the data.\n",
      "The lexical diversity is 0.562 in the data.\n",
      "The 89 most common tokens are:\n",
      "love: 13\n",
      "hurts: 8\n",
      "lot: 5\n",
      "know: 4\n",
      "ive: 3\n",
      "learned: 3\n",
      "oh: 3\n",
      "take: 2\n",
      "pain: 2\n",
      "like: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_loveisalonelyplacewithoutyou:\n",
      "There are 113 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 540 characters in the data.\n",
      "The lexical diversity is 0.398 in the data.\n",
      "The 113 most common tokens are:\n",
      "love: 15\n",
      "place: 10\n",
      "lonely: 7\n",
      "im: 7\n",
      "almost: 6\n",
      "without: 5\n",
      "youre: 4\n",
      "way: 4\n",
      "holding: 4\n",
      "know: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_loveisthegroove:\n",
      "There are 146 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 715 characters in the data.\n",
      "The lexical diversity is 0.575 in the data.\n",
      "The 146 most common tokens are:\n",
      "love: 16\n",
      "groove: 16\n",
      "move: 14\n",
      "get: 4\n",
      "like: 3\n",
      "another: 3\n",
      "promise: 3\n",
      "seek: 2\n",
      "home: 2\n",
      "heart: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_loveme:\n",
      "There are 68 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 296 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "The 68 most common tokens are:\n",
      "ill: 5\n",
      "love: 4\n",
      "oh: 4\n",
      "heart: 3\n",
      "ever: 3\n",
      "go: 3\n",
      "darling: 3\n",
      "lonely: 3\n",
      "treat: 2\n",
      "sad: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_loveonarooftop:\n",
      "There are 119 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 638 characters in the data.\n",
      "The lexical diversity is 0.437 in the data.\n",
      "The 119 most common tokens are:\n",
      "love: 12\n",
      "rooftop: 8\n",
      "remember: 7\n",
      "got: 6\n",
      "night: 5\n",
      "couldnt: 4\n",
      "make: 4\n",
      "stop: 4\n",
      "givin: 4\n",
      "never: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_loveoneanother:\n",
      "There are 120 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 650 characters in the data.\n",
      "The lexical diversity is 0.525 in the data.\n",
      "The 120 most common tokens are:\n",
      "love: 13\n",
      "one: 13\n",
      "another: 13\n",
      "everybody: 9\n",
      "needs: 3\n",
      "sisters: 3\n",
      "brothers: 3\n",
      "father: 3\n",
      "mother: 3\n",
      "let: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_lovepaintheresapaininmyheart:\n",
      "There are 108 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 534 characters in the data.\n",
      "The lexical diversity is 0.407 in the data.\n",
      "The 108 most common tokens are:\n",
      "pain: 8\n",
      "theres: 7\n",
      "well: 6\n",
      "guess: 6\n",
      "help: 6\n",
      "love: 5\n",
      "heart: 4\n",
      "coming: 4\n",
      "hunger: 4\n",
      "tearing: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_loversforever:\n",
      "There are 75 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 400 characters in the data.\n",
      "The lexical diversity is 0.813 in the data.\n",
      "The 75 most common tokens are:\n",
      "well: 4\n",
      "lovers: 3\n",
      "forever: 2\n",
      "offer: 2\n",
      "show: 2\n",
      "worlds: 2\n",
      "chorus: 2\n",
      "surrender: 2\n",
      "every: 2\n",
      "one: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_lovesohigh:\n",
      "There are 89 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 408 characters in the data.\n",
      "The lexical diversity is 0.663 in the data.\n",
      "The 89 most common tokens are:\n",
      "love: 7\n",
      "high: 4\n",
      "mi: 4\n",
      "amore: 4\n",
      "could: 4\n",
      "eyes: 3\n",
      "wake: 2\n",
      "sky: 2\n",
      "never: 2\n",
      "look: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_lovethedeviloutofya:\n",
      "There are 86 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 354 characters in the data.\n",
      "The lexical diversity is 0.674 in the data.\n",
      "The 86 most common tokens are:\n",
      "put: 11\n",
      "lid: 11\n",
      "right: 3\n",
      "time: 3\n",
      "whats: 2\n",
      "say: 2\n",
      "everything: 2\n",
      "ill: 2\n",
      "love: 1\n",
      "devil: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_mainman:\n",
      "There are 105 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 451 characters in the data.\n",
      "The lexical diversity is 0.343 in the data.\n",
      "The 105 most common tokens are:\n",
      "main: 15\n",
      "man: 15\n",
      "youre: 14\n",
      "oh: 8\n",
      "woman: 6\n",
      "ill: 3\n",
      "love: 3\n",
      "keep: 3\n",
      "call: 2\n",
      "heart: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_makethemanloveme:\n",
      "There are 111 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 465 characters in the data.\n",
      "The lexical diversity is 0.495 in the data.\n",
      "The 111 most common tokens are:\n",
      "make: 11\n",
      "man: 9\n",
      "love: 6\n",
      "lord: 5\n",
      "ah: 5\n",
      "wontcha: 4\n",
      "come: 4\n",
      "know: 3\n",
      "see: 3\n",
      "heart: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_mamawhenmydollieshavebabies:\n",
      "There are 108 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 512 characters in the data.\n",
      "The lexical diversity is 0.583 in the data.\n",
      "The 108 most common tokens are:\n",
      "mama: 10\n",
      "dollies: 5\n",
      "babies: 5\n",
      "away: 5\n",
      "big: 4\n",
      "lady: 4\n",
      "prince: 4\n",
      "come: 4\n",
      "take: 4\n",
      "love: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_mammamia:\n",
      "There are 159 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 692 characters in the data.\n",
      "The lexical diversity is 0.428 in the data.\n",
      "The 159 most common tokens are:\n",
      "mamma: 11\n",
      "mia: 11\n",
      "ive: 9\n",
      "go: 9\n",
      "know: 6\n",
      "look: 5\n",
      "let: 5\n",
      "since: 4\n",
      "ever: 4\n",
      "one: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_mastersofwar:\n",
      "There are 170 tokens in the data.\n",
      "There are 124 unique tokens in the data.\n",
      "There are 803 characters in the data.\n",
      "The lexical diversity is 0.729 in the data.\n",
      "The 170 most common tokens are:\n",
      "build: 4\n",
      "death: 4\n",
      "hide: 4\n",
      "see: 4\n",
      "im: 4\n",
      "war: 3\n",
      "know: 3\n",
      "never: 3\n",
      "world: 3\n",
      "like: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_melody:\n",
      "There are 78 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 397 characters in the data.\n",
      "The lexical diversity is 0.872 in the data.\n",
      "The 78 most common tokens are:\n",
      "melody: 6\n",
      "days: 3\n",
      "home: 2\n",
      "sleep: 2\n",
      "wont: 2\n",
      "youre: 1\n",
      "oldest: 1\n",
      "friend: 1\n",
      "talk: 1\n",
      "day: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_milord:\n",
      "There are 121 tokens in the data.\n",
      "There are 90 unique tokens in the data.\n",
      "There are 565 characters in the data.\n",
      "The lexical diversity is 0.744 in the data.\n",
      "The 121 most common tokens are:\n",
      "milord: 12\n",
      "come: 3\n",
      "lips: 3\n",
      "love: 3\n",
      "hearts: 3\n",
      "get: 2\n",
      "found: 2\n",
      "got: 2\n",
      "youre: 2\n",
      "let: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_mirrorimage:\n",
      "There are 173 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 893 characters in the data.\n",
      "The lexical diversity is 0.376 in the data.\n",
      "The 173 most common tokens are:\n",
      "mirror: 22\n",
      "image: 21\n",
      "see: 10\n",
      "life: 6\n",
      "think: 5\n",
      "want: 5\n",
      "looking: 5\n",
      "forget: 4\n",
      "isnt: 4\n",
      "black: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_misssubwayof1952:\n",
      "There are 118 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 617 characters in the data.\n",
      "The lexical diversity is 0.619 in the data.\n",
      "The 118 most common tokens are:\n",
      "little: 10\n",
      "may: 5\n",
      "miss: 4\n",
      "subway: 4\n",
      "1952: 4\n",
      "look: 3\n",
      "seen: 3\n",
      "21: 3\n",
      "years: 3\n",
      "ago: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_mommalooksharp:\n",
      "There are 80 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 335 characters in the data.\n",
      "The lexical diversity is 0.525 in the data.\n",
      "The 80 most common tokens are:\n",
      "momma: 10\n",
      "hey: 10\n",
      "look: 6\n",
      "sharp: 6\n",
      "ill: 3\n",
      "eyes: 3\n",
      "come: 2\n",
      "im: 2\n",
      "maple: 2\n",
      "tree: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_morethanyouknow:\n",
      "There are 86 tokens in the data.\n",
      "There are 37 unique tokens in the data.\n",
      "There are 384 characters in the data.\n",
      "The lexical diversity is 0.430 in the data.\n",
      "The 86 most common tokens are:\n",
      "know: 7\n",
      "loving: 6\n",
      "oh: 4\n",
      "youre: 3\n",
      "youll: 3\n",
      "ever: 3\n",
      "way: 3\n",
      "theres: 3\n",
      "nothing: 3\n",
      "may: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_moveme:\n",
      "There are 77 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 323 characters in the data.\n",
      "The lexical diversity is 0.234 in the data.\n",
      "The 77 most common tokens are:\n",
      "love: 23\n",
      "way: 9\n",
      "move: 8\n",
      "keep: 6\n",
      "groove: 4\n",
      "baby: 3\n",
      "doin: 3\n",
      "making: 2\n",
      "little: 2\n",
      "things: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_mrsoul:\n",
      "There are 87 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 434 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "The 87 most common tokens are:\n",
      "dont: 8\n",
      "change: 6\n",
      "strange: 5\n",
      "ask: 4\n",
      "face: 3\n",
      "know: 3\n",
      "mr: 2\n",
      "soul: 2\n",
      "head: 2\n",
      "could: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_mylove:\n",
      "There are 83 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 373 characters in the data.\n",
      "The lexical diversity is 0.337 in the data.\n",
      "The 83 most common tokens are:\n",
      "love: 32\n",
      "good: 10\n",
      "oh: 8\n",
      "everywhere: 4\n",
      "understood: 3\n",
      "whoa: 3\n",
      "dont: 2\n",
      "go: 1\n",
      "away: 1\n",
      "know: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_mylove318663:\n",
      "There are 114 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 524 characters in the data.\n",
      "The lexical diversity is 0.421 in the data.\n",
      "The 114 most common tokens are:\n",
      "love: 29\n",
      "know: 6\n",
      "youre: 5\n",
      "feel: 5\n",
      "somewhere: 4\n",
      "run: 4\n",
      "need: 4\n",
      "hold: 4\n",
      "wanna: 2\n",
      "somebody: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_mysongtoofargone:\n",
      "There are 116 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 508 characters in the data.\n",
      "The lexical diversity is 0.578 in the data.\n",
      "The 116 most common tokens are:\n",
      "far: 9\n",
      "gone: 9\n",
      "know: 9\n",
      "doesnt: 5\n",
      "son: 4\n",
      "loved: 3\n",
      "really: 3\n",
      "hell: 3\n",
      "never: 3\n",
      "get: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_needlesandpins:\n",
      "There are 113 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 514 characters in the data.\n",
      "The lexical diversity is 0.566 in the data.\n",
      "The 113 most common tokens are:\n",
      "needles: 7\n",
      "pins: 7\n",
      "stop: 5\n",
      "saw: 4\n",
      "face: 4\n",
      "away: 4\n",
      "somebody: 4\n",
      "love: 3\n",
      "go: 3\n",
      "cant: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_neverbeentospain:\n",
      "There are 88 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 437 characters in the data.\n",
      "The lexical diversity is 0.443 in the data.\n",
      "The 88 most common tokens are:\n",
      "never: 8\n",
      "well: 8\n",
      "dont: 4\n",
      "oklahoma: 4\n",
      "matter: 4\n",
      "spain: 3\n",
      "kinda: 3\n",
      "like: 3\n",
      "music: 2\n",
      "say: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_nevershouldvestarted:\n",
      "There are 102 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 463 characters in the data.\n",
      "The lexical diversity is 0.676 in the data.\n",
      "The 102 most common tokens are:\n",
      "cant: 7\n",
      "wait: 6\n",
      "holy: 5\n",
      "mother: 5\n",
      "ive: 3\n",
      "longer: 3\n",
      "tonight: 2\n",
      "feel: 2\n",
      "oh: 2\n",
      "need: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_notenoughloveintheworld:\n",
      "There are 130 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 666 characters in the data.\n",
      "The lexical diversity is 0.608 in the data.\n",
      "The 130 most common tokens are:\n",
      "world: 5\n",
      "know: 5\n",
      "enough: 4\n",
      "love: 4\n",
      "either: 4\n",
      "dont: 3\n",
      "standing: 3\n",
      "shadow: 3\n",
      "blocking: 3\n",
      "light: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_olmanriver:\n",
      "There are 126 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 564 characters in the data.\n",
      "The lexical diversity is 0.659 in the data.\n",
      "The 126 most common tokens are:\n",
      "man: 6\n",
      "river: 6\n",
      "dont: 6\n",
      "ol: 5\n",
      "keeps: 4\n",
      "rollin: 4\n",
      "white: 3\n",
      "ya: 3\n",
      "along: 3\n",
      "work: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_onebyone:\n",
      "There are 110 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 501 characters in the data.\n",
      "The lexical diversity is 0.518 in the data.\n",
      "The 110 most common tokens are:\n",
      "one: 19\n",
      "love: 6\n",
      "end: 4\n",
      "much: 4\n",
      "take: 4\n",
      "gonna: 4\n",
      "another: 4\n",
      "give: 3\n",
      "resisting: 3\n",
      "know: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_onehonestman:\n",
      "There are 116 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 551 characters in the data.\n",
      "The lexical diversity is 0.388 in the data.\n",
      "The 116 most common tokens are:\n",
      "man: 13\n",
      "cant: 13\n",
      "find: 12\n",
      "one: 11\n",
      "honest: 11\n",
      "dont: 6\n",
      "understand: 6\n",
      "aint: 3\n",
      "could: 2\n",
      "wanted: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_oneofus:\n",
      "There are 117 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 595 characters in the data.\n",
      "The lexical diversity is 0.462 in the data.\n",
      "The 117 most common tokens are:\n",
      "one: 13\n",
      "us: 13\n",
      "lonely: 5\n",
      "wishing: 5\n",
      "feeling: 4\n",
      "staring: 3\n",
      "ceiling: 3\n",
      "somewhere: 3\n",
      "else: 3\n",
      "instead: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_onesmallstep:\n",
      "There are 131 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 596 characters in the data.\n",
      "The lexical diversity is 0.412 in the data.\n",
      "The 131 most common tokens are:\n",
      "one: 18\n",
      "step: 17\n",
      "small: 16\n",
      "time: 7\n",
      "weve: 5\n",
      "got: 4\n",
      "take: 4\n",
      "never: 3\n",
      "theres: 3\n",
      "many: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_oogaboo:\n",
      "There are 108 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 467 characters in the data.\n",
      "The lexical diversity is 0.370 in the data.\n",
      "The 108 most common tokens are:\n",
      "ooga: 38\n",
      "boo: 13\n",
      "find: 7\n",
      "go: 5\n",
      "heres: 4\n",
      "message: 4\n",
      "never: 2\n",
      "feelings: 2\n",
      "whole: 2\n",
      "lifes: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_ourdaywillcome:\n",
      "There are 49 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 208 characters in the data.\n",
      "The lexical diversity is 0.510 in the data.\n",
      "The 49 most common tokens are:\n",
      "come: 7\n",
      "love: 6\n",
      "day: 5\n",
      "well: 4\n",
      "wait: 2\n",
      "dreams: 2\n",
      "magic: 2\n",
      "always: 2\n",
      "stay: 2\n",
      "way: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_ourladyofsanfrancisco:\n",
      "There are 94 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 467 characters in the data.\n",
      "The lexical diversity is 0.755 in the data.\n",
      "The 94 most common tokens are:\n",
      "san: 4\n",
      "francisco: 4\n",
      "day: 4\n",
      "lady: 3\n",
      "met: 3\n",
      "woman: 3\n",
      "nothin: 3\n",
      "street: 2\n",
      "feet: 2\n",
      "times: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_outrageous:\n",
      "There are 128 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 662 characters in the data.\n",
      "The lexical diversity is 0.547 in the data.\n",
      "The 128 most common tokens are:\n",
      "outrageous: 16\n",
      "im: 12\n",
      "rage: 7\n",
      "gonna: 6\n",
      "say: 3\n",
      "show: 3\n",
      "want: 3\n",
      "everything: 3\n",
      "tell: 3\n",
      "dont: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_paradiseishere:\n",
      "There are 115 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 598 characters in the data.\n",
      "The lexical diversity is 0.565 in the data.\n",
      "The 115 most common tokens are:\n",
      "need: 8\n",
      "right: 8\n",
      "paradise: 5\n",
      "loving: 5\n",
      "dont: 4\n",
      "want: 3\n",
      "place: 3\n",
      "future: 3\n",
      "tonight: 3\n",
      "talk: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_perfection:\n",
      "There are 241 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 1244 characters in the data.\n",
      "The lexical diversity is 0.361 in the data.\n",
      "The 241 most common tokens are:\n",
      "perfection: 18\n",
      "love: 15\n",
      "ive: 13\n",
      "driven: 11\n",
      "know: 9\n",
      "aint: 8\n",
      "ohh: 7\n",
      "nothings: 7\n",
      "perfect: 7\n",
      "right: 7\n",
      "\n",
      "Descriptive statistics for cher - cher_piedpiper:\n",
      "There are 84 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 376 characters in the data.\n",
      "The lexical diversity is 0.393 in the data.\n",
      "The 84 most common tokens are:\n",
      "pied: 9\n",
      "piper: 9\n",
      "im: 8\n",
      "follow: 6\n",
      "babe: 5\n",
      "come: 4\n",
      "ill: 4\n",
      "show: 4\n",
      "cant: 3\n",
      "see: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_pirate:\n",
      "There are 144 tokens in the data.\n",
      "There are 78 unique tokens in the data.\n",
      "There are 637 characters in the data.\n",
      "The lexical diversity is 0.542 in the data.\n",
      "The 144 most common tokens are:\n",
      "love: 11\n",
      "pirate: 8\n",
      "know: 8\n",
      "much: 5\n",
      "sea: 5\n",
      "sail: 3\n",
      "way: 3\n",
      "im: 3\n",
      "gonna: 3\n",
      "take: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_pleasedonttellme:\n",
      "There are 46 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 200 characters in the data.\n",
      "The lexical diversity is 0.783 in the data.\n",
      "The 46 most common tokens are:\n",
      "far: 2\n",
      "time: 2\n",
      "see: 2\n",
      "drift: 2\n",
      "machine: 2\n",
      "ago: 2\n",
      "one: 2\n",
      "youll: 2\n",
      "never: 2\n",
      "know: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_pride:\n",
      "There are 150 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 662 characters in the data.\n",
      "The lexical diversity is 0.333 in the data.\n",
      "The 150 most common tokens are:\n",
      "oh: 22\n",
      "pride: 12\n",
      "night: 8\n",
      "wont: 8\n",
      "stop: 8\n",
      "got: 7\n",
      "time: 5\n",
      "light: 4\n",
      "within: 4\n",
      "life: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_prisoner:\n",
      "There are 154 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 761 characters in the data.\n",
      "The lexical diversity is 0.338 in the data.\n",
      "The 154 most common tokens are:\n",
      "im: 18\n",
      "prisoner: 12\n",
      "love: 10\n",
      "hey: 8\n",
      "got: 4\n",
      "chained: 4\n",
      "someone: 4\n",
      "better: 4\n",
      "free: 4\n",
      "thoughts: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_rainrain:\n",
      "There are 101 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 484 characters in the data.\n",
      "The lexical diversity is 0.723 in the data.\n",
      "The 101 most common tokens are:\n",
      "rain: 15\n",
      "ooh: 3\n",
      "see: 3\n",
      "youre: 3\n",
      "chorus: 3\n",
      "everywhere: 2\n",
      "look: 2\n",
      "without: 2\n",
      "wish: 2\n",
      "cause: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_reallove:\n",
      "There are 121 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 547 characters in the data.\n",
      "The lexical diversity is 0.281 in the data.\n",
      "The 121 most common tokens are:\n",
      "time: 20\n",
      "love: 18\n",
      "real: 9\n",
      "still: 8\n",
      "believe: 8\n",
      "im: 6\n",
      "tell: 3\n",
      "bring: 3\n",
      "telling: 3\n",
      "dont: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_reasontobelieve:\n",
      "There are 73 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 378 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "The 73 most common tokens are:\n",
      "believe: 6\n",
      "find: 6\n",
      "reason: 4\n",
      "id: 3\n",
      "way: 3\n",
      "knowing: 3\n",
      "lied: 3\n",
      "straight: 3\n",
      "faced: 3\n",
      "cried: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_red:\n",
      "There are 144 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 626 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "The 144 most common tokens are:\n",
      "red: 27\n",
      "see: 9\n",
      "like: 5\n",
      "around: 4\n",
      "heart: 4\n",
      "blood: 4\n",
      "lips: 4\n",
      "told: 4\n",
      "done: 4\n",
      "cant: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_rescueme:\n",
      "There are 102 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 435 characters in the data.\n",
      "The lexical diversity is 0.245 in the data.\n",
      "The 102 most common tokens are:\n",
      "baby: 13\n",
      "rescue: 10\n",
      "come: 10\n",
      "im: 9\n",
      "lonely: 7\n",
      "take: 6\n",
      "cause: 6\n",
      "need: 6\n",
      "love: 6\n",
      "cant: 6\n",
      "\n",
      "Descriptive statistics for cher - cher_rockandrolldoctor:\n",
      "There are 154 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 693 characters in the data.\n",
      "The lexical diversity is 0.519 in the data.\n",
      "The 154 most common tokens are:\n",
      "feel: 8\n",
      "fine: 8\n",
      "hes: 7\n",
      "beat: 6\n",
      "rock: 5\n",
      "roll: 5\n",
      "doctor: 5\n",
      "like: 5\n",
      "man: 5\n",
      "meet: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_rudy:\n",
      "There are 131 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 659 characters in the data.\n",
      "The lexical diversity is 0.351 in the data.\n",
      "The 131 most common tokens are:\n",
      "rudy: 19\n",
      "youre: 16\n",
      "still: 8\n",
      "always: 8\n",
      "mind: 8\n",
      "toot: 5\n",
      "mine: 4\n",
      "remember: 4\n",
      "nights: 4\n",
      "shared: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_runaway:\n",
      "There are 135 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.311 in the data.\n",
      "The 135 most common tokens are:\n",
      "runaway: 23\n",
      "cant: 12\n",
      "love: 11\n",
      "find: 10\n",
      "gotta: 9\n",
      "nobody: 5\n",
      "heart: 5\n",
      "always: 4\n",
      "cause: 3\n",
      "place: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_runnin:\n",
      "There are 153 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 699 characters in the data.\n",
      "The lexical diversity is 0.536 in the data.\n",
      "The 153 most common tokens are:\n",
      "runnin: 25\n",
      "sail: 9\n",
      "keep: 5\n",
      "like: 5\n",
      "cant: 4\n",
      "im: 4\n",
      "stop: 4\n",
      "flyin: 4\n",
      "ive: 3\n",
      "cryin: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_savethechildren:\n",
      "There are 76 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 424 characters in the data.\n",
      "The lexical diversity is 0.842 in the data.\n",
      "The 76 most common tokens are:\n",
      "eyes: 4\n",
      "dont: 3\n",
      "youre: 3\n",
      "im: 2\n",
      "long: 2\n",
      "lonely: 2\n",
      "secretly: 2\n",
      "know: 2\n",
      "save: 1\n",
      "children: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_saveupallyourtears:\n",
      "There are 197 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 877 characters in the data.\n",
      "The lexical diversity is 0.320 in the data.\n",
      "The 197 most common tokens are:\n",
      "dont: 17\n",
      "tears: 13\n",
      "know: 13\n",
      "cryin: 13\n",
      "youll: 12\n",
      "save: 9\n",
      "need: 9\n",
      "baby: 8\n",
      "wont: 5\n",
      "cause: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_saytheword:\n",
      "There are 101 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.604 in the data.\n",
      "The 101 most common tokens are:\n",
      "blade: 7\n",
      "think: 6\n",
      "see: 6\n",
      "big: 6\n",
      "im: 4\n",
      "falling: 3\n",
      "coming: 3\n",
      "watch: 3\n",
      "sometime: 2\n",
      "soon: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_saywhatsonyourmind:\n",
      "There are 117 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 539 characters in the data.\n",
      "The lexical diversity is 0.427 in the data.\n",
      "The 117 most common tokens are:\n",
      "baby: 11\n",
      "want: 8\n",
      "dont: 7\n",
      "say: 6\n",
      "whats: 6\n",
      "mind: 5\n",
      "tell: 5\n",
      "got: 5\n",
      "something: 4\n",
      "know: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_sendthemanover:\n",
      "There are 138 tokens in the data.\n",
      "There are 99 unique tokens in the data.\n",
      "There are 740 characters in the data.\n",
      "The lexical diversity is 0.717 in the data.\n",
      "The 138 most common tokens are:\n",
      "send: 5\n",
      "today: 4\n",
      "man: 3\n",
      "call: 3\n",
      "say: 3\n",
      "way: 3\n",
      "actress: 2\n",
      "pay: 2\n",
      "called: 2\n",
      "agent: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_shadowdreamsong:\n",
      "There are 67 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 350 characters in the data.\n",
      "The lexical diversity is 0.687 in the data.\n",
      "The 67 most common tokens are:\n",
      "yeah: 5\n",
      "princess: 4\n",
      "prince: 4\n",
      "shadow: 3\n",
      "meant: 3\n",
      "could: 3\n",
      "song: 2\n",
      "evening: 2\n",
      "cant: 2\n",
      "laughing: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_shapeofthingstocome:\n",
      "There are 142 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 655 characters in the data.\n",
      "The lexical diversity is 0.387 in the data.\n",
      "The 142 most common tokens are:\n",
      "things: 19\n",
      "come: 17\n",
      "shape: 16\n",
      "two: 8\n",
      "one: 5\n",
      "sir: 4\n",
      "system: 3\n",
      "look: 3\n",
      "get: 3\n",
      "logic: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_shelovestohearthemusic:\n",
      "There are 154 tokens in the data.\n",
      "There are 91 unique tokens in the data.\n",
      "There are 745 characters in the data.\n",
      "The lexical diversity is 0.591 in the data.\n",
      "The 154 most common tokens are:\n",
      "loves: 8\n",
      "shes: 8\n",
      "hear: 7\n",
      "music: 6\n",
      "got: 6\n",
      "every: 4\n",
      "town: 4\n",
      "lyric: 3\n",
      "say: 3\n",
      "greatest: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_shoppin:\n",
      "There are 226 tokens in the data.\n",
      "There are 128 unique tokens in the data.\n",
      "There are 1117 characters in the data.\n",
      "The lexical diversity is 0.566 in the data.\n",
      "The 226 most common tokens are:\n",
      "shoppin: 26\n",
      "im: 14\n",
      "gonna: 8\n",
      "buy: 4\n",
      "blues: 4\n",
      "away: 4\n",
      "say: 3\n",
      "take: 3\n",
      "troubles: 3\n",
      "town: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_silverwingsgoldenrings:\n",
      "There are 109 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 525 characters in the data.\n",
      "The lexical diversity is 0.771 in the data.\n",
      "The 109 most common tokens are:\n",
      "love: 4\n",
      "back: 4\n",
      "silver: 3\n",
      "wings: 3\n",
      "golden: 3\n",
      "rings: 3\n",
      "time: 3\n",
      "night: 2\n",
      "lonely: 2\n",
      "seemed: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_singforyoursupper:\n",
      "There are 88 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 439 characters in the data.\n",
      "The lexical diversity is 0.443 in the data.\n",
      "The 88 most common tokens are:\n",
      "sing: 9\n",
      "youll: 8\n",
      "supper: 4\n",
      "get: 4\n",
      "swallow: 4\n",
      "said: 4\n",
      "fed: 4\n",
      "breakfast: 3\n",
      "songbirds: 3\n",
      "time: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_sirens:\n",
      "There are 101 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 481 characters in the data.\n",
      "The lexical diversity is 0.446 in the data.\n",
      "The 101 most common tokens are:\n",
      "sirens: 7\n",
      "sound: 6\n",
      "sky: 3\n",
      "leave: 3\n",
      "behind: 3\n",
      "city: 3\n",
      "rise: 3\n",
      "hold: 3\n",
      "hand: 3\n",
      "mine: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_sistersofmercy:\n",
      "There are 184 tokens in the data.\n",
      "There are 116 unique tokens in the data.\n",
      "There are 971 characters in the data.\n",
      "The lexical diversity is 0.630 in the data.\n",
      "The 184 most common tokens are:\n",
      "mercy: 17\n",
      "sisters: 9\n",
      "grace: 6\n",
      "place: 6\n",
      "shows: 5\n",
      "way: 5\n",
      "theres: 4\n",
      "baby: 4\n",
      "shes: 4\n",
      "gods: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_sittinonthedockofthebay:\n",
      "There are 97 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 456 characters in the data.\n",
      "The lexical diversity is 0.495 in the data.\n",
      "The 97 most common tokens are:\n",
      "sittin: 9\n",
      "dock: 8\n",
      "bay: 8\n",
      "roll: 5\n",
      "watchin: 4\n",
      "away: 4\n",
      "im: 4\n",
      "tide: 3\n",
      "wastin: 3\n",
      "time: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_skindeep:\n",
      "There are 149 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 655 characters in the data.\n",
      "The lexical diversity is 0.356 in the data.\n",
      "The 149 most common tokens are:\n",
      "skin: 16\n",
      "deep: 16\n",
      "bone: 13\n",
      "im: 9\n",
      "go: 8\n",
      "every: 6\n",
      "time: 6\n",
      "tellin: 6\n",
      "trouble: 6\n",
      "see: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_songcalledchildren:\n",
      "There are 74 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 378 characters in the data.\n",
      "The lexical diversity is 0.581 in the data.\n",
      "The 74 most common tokens are:\n",
      "children: 6\n",
      "still: 5\n",
      "time: 4\n",
      "close: 3\n",
      "theres: 3\n",
      "wish: 3\n",
      "stood: 3\n",
      "see: 2\n",
      "dancing: 2\n",
      "light: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_songforyou:\n",
      "There are 84 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 425 characters in the data.\n",
      "The lexical diversity is 0.655 in the data.\n",
      "The 84 most common tokens are:\n",
      "song: 7\n",
      "singing: 6\n",
      "life: 4\n",
      "ive: 3\n",
      "alone: 3\n",
      "im: 3\n",
      "love: 3\n",
      "time: 2\n",
      "darling: 2\n",
      "see: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_sos:\n",
      "There are 110 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 507 characters in the data.\n",
      "The lexical diversity is 0.400 in the data.\n",
      "The 110 most common tokens are:\n",
      "youre: 11\n",
      "gone: 8\n",
      "try: 8\n",
      "love: 5\n",
      "though: 5\n",
      "near: 4\n",
      "even: 4\n",
      "go: 4\n",
      "carry: 4\n",
      "used: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_spring:\n",
      "There are 147 tokens in the data.\n",
      "There are 91 unique tokens in the data.\n",
      "There are 737 characters in the data.\n",
      "The lexical diversity is 0.619 in the data.\n",
      "The 147 most common tokens are:\n",
      "spring: 10\n",
      "long: 10\n",
      "time: 7\n",
      "said: 4\n",
      "ah: 4\n",
      "woman: 3\n",
      "child: 3\n",
      "winters: 3\n",
      "passing: 3\n",
      "never: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_stars:\n",
      "There are 160 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 774 characters in the data.\n",
      "The lexical diversity is 0.662 in the data.\n",
      "The 160 most common tokens are:\n",
      "come: 9\n",
      "never: 6\n",
      "singing: 5\n",
      "go: 4\n",
      "make: 4\n",
      "stars: 3\n",
      "know: 3\n",
      "like: 3\n",
      "theyre: 3\n",
      "perhaps: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_startingover:\n",
      "There are 129 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 648 characters in the data.\n",
      "The lexical diversity is 0.589 in the data.\n",
      "The 129 most common tokens are:\n",
      "startin: 12\n",
      "back: 6\n",
      "let: 4\n",
      "time: 4\n",
      "around: 4\n",
      "ooo: 4\n",
      "take: 4\n",
      "oh: 3\n",
      "second: 3\n",
      "coming: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_still:\n",
      "There are 122 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 611 characters in the data.\n",
      "The lexical diversity is 0.467 in the data.\n",
      "The 122 most common tokens are:\n",
      "love: 15\n",
      "still: 12\n",
      "baby: 6\n",
      "oh: 5\n",
      "cried: 4\n",
      "heart: 3\n",
      "cause: 3\n",
      "prisoner: 3\n",
      "ocean: 2\n",
      "pain: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_stillinlovewithyou:\n",
      "There are 99 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 447 characters in the data.\n",
      "The lexical diversity is 0.465 in the data.\n",
      "The 99 most common tokens are:\n",
      "know: 11\n",
      "still: 9\n",
      "love: 9\n",
      "lying: 6\n",
      "ive: 5\n",
      "baby: 5\n",
      "dont: 4\n",
      "true: 3\n",
      "hold: 2\n",
      "long: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_strongenough:\n",
      "There are 175 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 822 characters in the data.\n",
      "The lexical diversity is 0.354 in the data.\n",
      "The 175 most common tokens are:\n",
      "enough: 20\n",
      "strong: 16\n",
      "im: 13\n",
      "say: 7\n",
      "know: 7\n",
      "gotta: 7\n",
      "go: 7\n",
      "hear: 5\n",
      "theres: 4\n",
      "live: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_sunny:\n",
      "There are 80 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 364 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "The 80 most common tokens are:\n",
      "sunny: 14\n",
      "love: 8\n",
      "thank: 6\n",
      "true: 4\n",
      "gave: 4\n",
      "days: 2\n",
      "sunshine: 2\n",
      "brought: 2\n",
      "way: 2\n",
      "feel: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_superstar:\n",
      "There are 93 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 388 characters in the data.\n",
      "The lexical diversity is 0.495 in the data.\n",
      "The 93 most common tokens are:\n",
      "baby: 11\n",
      "ah: 7\n",
      "oh: 6\n",
      "love: 5\n",
      "ooh: 5\n",
      "back: 4\n",
      "really: 3\n",
      "dont: 3\n",
      "guitar: 2\n",
      "remember: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_takeitfromtheboys:\n",
      "There are 211 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 998 characters in the data.\n",
      "The lexical diversity is 0.336 in the data.\n",
      "The 211 most common tokens are:\n",
      "take: 31\n",
      "boys: 31\n",
      "well: 10\n",
      "might: 9\n",
      "wise: 9\n",
      "back: 7\n",
      "meet: 6\n",
      "streets: 6\n",
      "theyre: 5\n",
      "cant: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_takeitlikeaman:\n",
      "There are 196 tokens in the data.\n",
      "There are 77 unique tokens in the data.\n",
      "There are 883 characters in the data.\n",
      "The lexical diversity is 0.393 in the data.\n",
      "The 196 most common tokens are:\n",
      "gotta: 11\n",
      "heart: 10\n",
      "take: 9\n",
      "like: 9\n",
      "man: 9\n",
      "better: 9\n",
      "want: 7\n",
      "love: 7\n",
      "ive: 6\n",
      "feel: 6\n",
      "\n",
      "Descriptive statistics for cher - cher_takemeforalittlewhile:\n",
      "There are 55 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 256 characters in the data.\n",
      "The lexical diversity is 0.727 in the data.\n",
      "The 55 most common tokens are:\n",
      "love: 5\n",
      "little: 3\n",
      "dont: 3\n",
      "forever: 3\n",
      "take: 2\n",
      "ive: 2\n",
      "make: 2\n",
      "want: 2\n",
      "cause: 2\n",
      "trying: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_takemehome:\n",
      "There are 222 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 987 characters in the data.\n",
      "The lexical diversity is 0.248 in the data.\n",
      "The 222 most common tokens are:\n",
      "home: 39\n",
      "take: 37\n",
      "heaven: 11\n",
      "wanna: 7\n",
      "baby: 7\n",
      "want: 5\n",
      "ooh: 5\n",
      "next: 5\n",
      "right: 5\n",
      "get: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_takinbackmyheart:\n",
      "There are 237 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 1122 characters in the data.\n",
      "The lexical diversity is 0.291 in the data.\n",
      "The 237 most common tokens are:\n",
      "back: 27\n",
      "baby: 25\n",
      "heart: 23\n",
      "takin: 17\n",
      "im: 15\n",
      "taking: 10\n",
      "dont: 9\n",
      "repossessing: 7\n",
      "affection: 7\n",
      "gonna: 6\n",
      "\n",
      "Descriptive statistics for cher - cher_taxitaxi:\n",
      "There are 179 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 802 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "The 179 most common tokens are:\n",
      "taxi: 32\n",
      "ride: 18\n",
      "im: 17\n",
      "gonna: 17\n",
      "night: 13\n",
      "take: 8\n",
      "sing: 5\n",
      "give: 5\n",
      "side: 4\n",
      "turn: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_thebellsofrhymney:\n",
      "There are 76 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 363 characters in the data.\n",
      "The lexical diversity is 0.368 in the data.\n",
      "The 76 most common tokens are:\n",
      "bells: 13\n",
      "say: 12\n",
      "rhymney: 5\n",
      "give: 4\n",
      "sad: 4\n",
      "oh: 2\n",
      "hope: 2\n",
      "future: 2\n",
      "brown: 2\n",
      "murther: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thebiggertheycomethehardertheyfall:\n",
      "There are 105 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.571 in the data.\n",
      "The 105 most common tokens are:\n",
      "bigger: 5\n",
      "come: 5\n",
      "harder: 5\n",
      "fall: 5\n",
      "oh: 4\n",
      "get: 4\n",
      "sure: 3\n",
      "sun: 3\n",
      "shine: 3\n",
      "iaam: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_thebookoflove:\n",
      "There are 132 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 651 characters in the data.\n",
      "The lexical diversity is 0.576 in the data.\n",
      "The 132 most common tokens are:\n",
      "love: 15\n",
      "book: 13\n",
      "writing: 6\n",
      "heyho: 3\n",
      "broken: 3\n",
      "hearts: 3\n",
      "z: 3\n",
      "dos: 3\n",
      "donts: 3\n",
      "bended: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_thecruelwar:\n",
      "There are 78 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 339 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "The 78 most common tokens are:\n",
      "love: 6\n",
      "wont: 4\n",
      "let: 4\n",
      "go: 4\n",
      "ill: 4\n",
      "sonny: 3\n",
      "oh: 3\n",
      "cruel: 2\n",
      "war: 2\n",
      "want: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thefallkurtsblues:\n",
      "There are 108 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 544 characters in the data.\n",
      "The lexical diversity is 0.778 in the data.\n",
      "The 108 most common tokens are:\n",
      "heard: 3\n",
      "news: 3\n",
      "fall: 2\n",
      "knew: 2\n",
      "well: 2\n",
      "one: 2\n",
      "could: 2\n",
      "prison: 2\n",
      "death: 2\n",
      "pain: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thefirsttime:\n",
      "There are 73 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 311 characters in the data.\n",
      "The lexical diversity is 0.671 in the data.\n",
      "The 73 most common tokens are:\n",
      "dont: 4\n",
      "think: 3\n",
      "see: 3\n",
      "anymore: 3\n",
      "know: 3\n",
      "oh: 2\n",
      "heard: 2\n",
      "rooster: 2\n",
      "crow: 2\n",
      "better: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thegirlfromipanema:\n",
      "There are 85 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 401 characters in the data.\n",
      "The lexical diversity is 0.471 in the data.\n",
      "The 85 most common tokens are:\n",
      "passes: 6\n",
      "goes: 5\n",
      "girl: 4\n",
      "ipanema: 4\n",
      "tall: 3\n",
      "tan: 3\n",
      "young: 3\n",
      "walking: 3\n",
      "walks: 3\n",
      "lovely: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thegreatestsongieverheard:\n",
      "There are 99 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 474 characters in the data.\n",
      "The lexical diversity is 0.727 in the data.\n",
      "The 99 most common tokens are:\n",
      "ever: 8\n",
      "greatest: 4\n",
      "song: 4\n",
      "heard: 4\n",
      "ive: 3\n",
      "one: 3\n",
      "sung: 3\n",
      "day: 3\n",
      "said: 2\n",
      "never: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thegreatestthing:\n",
      "There are 203 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 1031 characters in the data.\n",
      "The lexical diversity is 0.286 in the data.\n",
      "The 203 most common tokens are:\n",
      "greatest: 20\n",
      "thing: 14\n",
      "youre: 12\n",
      "see: 8\n",
      "ill: 8\n",
      "hope: 7\n",
      "know: 6\n",
      "fall: 6\n",
      "apart: 6\n",
      "broken: 6\n",
      "\n",
      "Descriptive statistics for cher - cher_thegunman:\n",
      "There are 90 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 428 characters in the data.\n",
      "The lexical diversity is 0.533 in the data.\n",
      "The 90 most common tokens are:\n",
      "love: 11\n",
      "gunman: 10\n",
      "mercy: 5\n",
      "time: 3\n",
      "sights: 3\n",
      "hell: 2\n",
      "hunt: 2\n",
      "day: 2\n",
      "death: 2\n",
      "sets: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thelongandwindingroad:\n",
      "There are 87 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 398 characters in the data.\n",
      "The lexical diversity is 0.460 in the data.\n",
      "The 87 most common tokens are:\n",
      "long: 9\n",
      "road: 6\n",
      "winding: 5\n",
      "door: 4\n",
      "standing: 4\n",
      "many: 4\n",
      "times: 4\n",
      "oh: 3\n",
      "ive: 3\n",
      "left: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_themanilove:\n",
      "There are 105 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 458 characters in the data.\n",
      "The lexical diversity is 0.486 in the data.\n",
      "The 105 most common tokens are:\n",
      "gonna: 7\n",
      "hes: 6\n",
      "man: 5\n",
      "love: 4\n",
      "ill: 4\n",
      "grab: 4\n",
      "know: 4\n",
      "wont: 4\n",
      "say: 4\n",
      "maybe: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_themanthatgotaway:\n",
      "There are 88 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 429 characters in the data.\n",
      "The lexical diversity is 0.841 in the data.\n",
      "The 88 most common tokens are:\n",
      "man: 5\n",
      "got: 3\n",
      "away: 3\n",
      "night: 2\n",
      "dreams: 2\n",
      "gone: 2\n",
      "every: 2\n",
      "fools: 2\n",
      "tomorrow: 2\n",
      "bitter: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_themusicsnogoodwithoutyou:\n",
      "There are 87 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 450 characters in the data.\n",
      "The lexical diversity is 0.759 in the data.\n",
      "The 87 most common tokens are:\n",
      "good: 5\n",
      "musics: 4\n",
      "chorus: 4\n",
      "come: 4\n",
      "back: 4\n",
      "without: 3\n",
      "like: 2\n",
      "baby: 2\n",
      "cause: 2\n",
      "everyone: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_thenameofthegame:\n",
      "There are 226 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1076 characters in the data.\n",
      "The lexical diversity is 0.319 in the data.\n",
      "The 226 most common tokens are:\n",
      "doodoo: 28\n",
      "name: 11\n",
      "game: 11\n",
      "whats: 9\n",
      "know: 7\n",
      "feel: 7\n",
      "lot: 6\n",
      "way: 6\n",
      "make: 6\n",
      "im: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_thepower:\n",
      "There are 146 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 706 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "The 146 most common tokens are:\n",
      "power: 14\n",
      "every: 6\n",
      "believe: 5\n",
      "holds: 4\n",
      "hand: 4\n",
      "drives: 4\n",
      "crazy: 4\n",
      "man: 4\n",
      "real: 4\n",
      "yeah: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_therebutforfortune:\n",
      "There are 67 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 320 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "The 67 most common tokens are:\n",
      "show: 14\n",
      "fortune: 5\n",
      "ill: 4\n",
      "young: 4\n",
      "many: 4\n",
      "reasons: 4\n",
      "go: 4\n",
      "man: 3\n",
      "prison: 1\n",
      "jail: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_thesamemistake:\n",
      "There are 123 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 577 characters in the data.\n",
      "The lexical diversity is 0.512 in the data.\n",
      "The 123 most common tokens are:\n",
      "know: 7\n",
      "love: 6\n",
      "sometimes: 6\n",
      "mistake: 5\n",
      "give: 5\n",
      "take: 5\n",
      "door: 4\n",
      "way: 3\n",
      "world: 3\n",
      "go: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_thesedays:\n",
      "There are 82 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 386 characters in the data.\n",
      "The lexical diversity is 0.659 in the data.\n",
      "The 82 most common tokens are:\n",
      "days: 9\n",
      "ive: 5\n",
      "oh: 5\n",
      "well: 4\n",
      "forgotten: 3\n",
      "dont: 2\n",
      "seem: 2\n",
      "things: 2\n",
      "lord: 2\n",
      "another: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_theshoopshoopsongitsinhiskiss:\n",
      "There are 93 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 409 characters in the data.\n",
      "The lexical diversity is 0.355 in the data.\n",
      "The 93 most common tokens are:\n",
      "thats: 14\n",
      "kiss: 13\n",
      "know: 7\n",
      "oh: 6\n",
      "want: 5\n",
      "loves: 5\n",
      "way: 4\n",
      "love: 3\n",
      "shoop: 2\n",
      "hug: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thesunaintgonnashineanymore:\n",
      "There are 118 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 581 characters in the data.\n",
      "The lexical diversity is 0.288 in the data.\n",
      "The 118 most common tokens are:\n",
      "aint: 11\n",
      "gonna: 11\n",
      "always: 7\n",
      "sun: 6\n",
      "shine: 6\n",
      "anymore: 6\n",
      "youre: 6\n",
      "without: 6\n",
      "moon: 5\n",
      "rise: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_thethoughtoflovingyou:\n",
      "There are 71 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 408 characters in the data.\n",
      "The lexical diversity is 0.775 in the data.\n",
      "The 71 most common tokens are:\n",
      "thought: 4\n",
      "loving: 4\n",
      "right: 4\n",
      "way: 2\n",
      "im: 2\n",
      "longing: 2\n",
      "thousand: 2\n",
      "dreams: 2\n",
      "near: 2\n",
      "love: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thetimestheyareachangin:\n",
      "There are 127 tokens in the data.\n",
      "There are 98 unique tokens in the data.\n",
      "There are 675 characters in the data.\n",
      "The lexical diversity is 0.772 in the data.\n",
      "The 127 most common tokens are:\n",
      "times: 6\n",
      "achangin: 6\n",
      "come: 5\n",
      "dont: 4\n",
      "later: 4\n",
      "soon: 3\n",
      "one: 3\n",
      "youll: 2\n",
      "theres: 2\n",
      "please: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thetwelfthofnever:\n",
      "There are 72 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 330 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "The 72 most common tokens are:\n",
      "long: 7\n",
      "never: 6\n",
      "twelfth: 5\n",
      "love: 4\n",
      "ill: 3\n",
      "till: 3\n",
      "thats: 3\n",
      "time: 3\n",
      "ask: 2\n",
      "need: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thewayoflove:\n",
      "There are 71 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 280 characters in the data.\n",
      "The lexical diversity is 0.423 in the data.\n",
      "The 71 most common tokens are:\n",
      "way: 9\n",
      "love: 7\n",
      "meet: 2\n",
      "boy: 2\n",
      "like: 2\n",
      "lot: 2\n",
      "fall: 2\n",
      "loves: 2\n",
      "flame: 2\n",
      "start: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thewinnertakesitall:\n",
      "There are 143 tokens in the data.\n",
      "There are 98 unique tokens in the data.\n",
      "There are 751 characters in the data.\n",
      "The lexical diversity is 0.685 in the data.\n",
      "The 143 most common tokens are:\n",
      "takes: 9\n",
      "winner: 7\n",
      "small: 5\n",
      "standing: 3\n",
      "feel: 3\n",
      "dont: 2\n",
      "wanna: 2\n",
      "talk: 2\n",
      "thats: 2\n",
      "youve: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thisgodforsakenday:\n",
      "There are 112 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 562 characters in the data.\n",
      "The lexical diversity is 0.714 in the data.\n",
      "The 112 most common tokens are:\n",
      "godforsaken: 5\n",
      "day: 5\n",
      "youve: 5\n",
      "gone: 5\n",
      "away: 5\n",
      "kids: 3\n",
      "got: 3\n",
      "get: 3\n",
      "night: 3\n",
      "read: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thisisasongforthelonely:\n",
      "There are 116 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 581 characters in the data.\n",
      "The lexical diversity is 0.595 in the data.\n",
      "The 116 most common tokens are:\n",
      "song: 7\n",
      "lonely: 6\n",
      "hear: 4\n",
      "dont: 4\n",
      "gonna: 4\n",
      "love: 3\n",
      "chorus: 3\n",
      "reason: 3\n",
      "alright: 3\n",
      "youre: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_thunderstorm:\n",
      "There are 112 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 581 characters in the data.\n",
      "The lexical diversity is 0.571 in the data.\n",
      "The 112 most common tokens are:\n",
      "thunderstorm: 4\n",
      "good: 4\n",
      "feel: 4\n",
      "knew: 3\n",
      "coming: 3\n",
      "last: 3\n",
      "night: 3\n",
      "swear: 3\n",
      "heard: 3\n",
      "north: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_time:\n",
      "There are 78 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 357 characters in the data.\n",
      "The lexical diversity is 0.551 in the data.\n",
      "The 78 most common tokens are:\n",
      "time: 9\n",
      "people: 5\n",
      "go: 5\n",
      "oh: 4\n",
      "good: 4\n",
      "never: 4\n",
      "sometimes: 4\n",
      "im: 3\n",
      "dont: 2\n",
      "roads: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_tonightillbestayingherewithyou:\n",
      "There are 73 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 389 characters in the data.\n",
      "The lexical diversity is 0.630 in the data.\n",
      "The 73 most common tokens are:\n",
      "throw: 6\n",
      "tonight: 5\n",
      "ill: 5\n",
      "staying: 5\n",
      "cause: 3\n",
      "ticket: 2\n",
      "window: 2\n",
      "suitcase: 2\n",
      "troubles: 2\n",
      "door: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_touchandgo:\n",
      "There are 85 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 367 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "The 85 most common tokens are:\n",
      "know: 6\n",
      "touch: 5\n",
      "go: 5\n",
      "weak: 4\n",
      "strong: 4\n",
      "didnt: 4\n",
      "hope: 4\n",
      "could: 4\n",
      "cope: 4\n",
      "ups: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_trainofthought:\n",
      "There are 130 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 616 characters in the data.\n",
      "The lexical diversity is 0.623 in the data.\n",
      "The 130 most common tokens are:\n",
      "gotta: 10\n",
      "get: 10\n",
      "train: 7\n",
      "thought: 5\n",
      "time: 4\n",
      "woowoo: 4\n",
      "oh: 3\n",
      "youre: 2\n",
      "one: 2\n",
      "starts: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_twopeopleclingingtoathread:\n",
      "There are 63 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 316 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "The 63 most common tokens are:\n",
      "two: 2\n",
      "people: 2\n",
      "clinging: 2\n",
      "thread: 2\n",
      "wake: 2\n",
      "night: 2\n",
      "pretend: 2\n",
      "go: 2\n",
      "day: 2\n",
      "everyday: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_untilitstimeforyoutogo:\n",
      "There are 69 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 299 characters in the data.\n",
      "The lexical diversity is 0.710 in the data.\n",
      "The 69 most common tokens are:\n",
      "time: 4\n",
      "go: 4\n",
      "youre: 3\n",
      "im: 3\n",
      "stay: 3\n",
      "dont: 3\n",
      "ask: 3\n",
      "love: 3\n",
      "well: 2\n",
      "ill: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_walkinginmemphis:\n",
      "There are 175 tokens in the data.\n",
      "There are 85 unique tokens in the data.\n",
      "There are 917 characters in the data.\n",
      "The lexical diversity is 0.486 in the data.\n",
      "The 175 most common tokens are:\n",
      "walking: 22\n",
      "memphis: 18\n",
      "feet: 10\n",
      "feel: 10\n",
      "got: 5\n",
      "ten: 5\n",
      "beale: 5\n",
      "really: 5\n",
      "way: 5\n",
      "walk: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_walkwithme:\n",
      "There are 93 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 436 characters in the data.\n",
      "The lexical diversity is 0.430 in the data.\n",
      "The 93 most common tokens are:\n",
      "walk: 17\n",
      "take: 4\n",
      "hand: 4\n",
      "count: 4\n",
      "troubles: 4\n",
      "times: 4\n",
      "ill: 4\n",
      "youve: 4\n",
      "understand: 4\n",
      "oh: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_walls:\n",
      "There are 141 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 712 characters in the data.\n",
      "The lexical diversity is 0.426 in the data.\n",
      "The 141 most common tokens are:\n",
      "see: 9\n",
      "walls: 8\n",
      "wanna: 8\n",
      "crashing: 7\n",
      "cause: 6\n",
      "save: 5\n",
      "look: 5\n",
      "riding: 4\n",
      "like: 3\n",
      "life: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_warpaintandsoftfeathers:\n",
      "There are 176 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 941 characters in the data.\n",
      "The lexical diversity is 0.432 in the data.\n",
      "The 176 most common tokens are:\n",
      "war: 9\n",
      "paint: 9\n",
      "soft: 9\n",
      "feathers: 9\n",
      "love: 6\n",
      "apache: 6\n",
      "cherokee: 6\n",
      "moon: 5\n",
      "blueeyed: 5\n",
      "forbid: 5\n",
      "\n",
      "Descriptive statistics for cher - cher_wasntitgood:\n",
      "There are 144 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 683 characters in the data.\n",
      "The lexical diversity is 0.458 in the data.\n",
      "The 144 most common tokens are:\n",
      "good: 16\n",
      "wasnt: 9\n",
      "know: 6\n",
      "party: 5\n",
      "baby: 5\n",
      "didnt: 4\n",
      "im: 4\n",
      "love: 4\n",
      "betcha: 3\n",
      "coming: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_waterloo:\n",
      "There are 107 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 619 characters in the data.\n",
      "The lexical diversity is 0.402 in the data.\n",
      "The 107 most common tokens are:\n",
      "waterloo: 21\n",
      "woah: 8\n",
      "ever: 4\n",
      "knowing: 4\n",
      "fate: 4\n",
      "woahoh: 4\n",
      "finally: 4\n",
      "facing: 4\n",
      "oh: 3\n",
      "couldnt: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_weallflyhome:\n",
      "There are 79 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 370 characters in the data.\n",
      "The lexical diversity is 0.532 in the data.\n",
      "The 79 most common tokens are:\n",
      "fly: 7\n",
      "home: 7\n",
      "well: 4\n",
      "sooner: 4\n",
      "later: 4\n",
      "night: 3\n",
      "cold: 3\n",
      "im: 2\n",
      "feel: 2\n",
      "right: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_weallsleepalone:\n",
      "There are 93 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 471 characters in the data.\n",
      "The lexical diversity is 0.613 in the data.\n",
      "The 93 most common tokens are:\n",
      "alone: 9\n",
      "sleep: 8\n",
      "youre: 3\n",
      "sooner: 3\n",
      "later: 3\n",
      "yeah: 3\n",
      "somebody: 2\n",
      "night: 2\n",
      "cause: 2\n",
      "heart: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_welcometoburlesque:\n",
      "There are 91 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 495 characters in the data.\n",
      "The lexical diversity is 0.637 in the data.\n",
      "The 91 most common tokens are:\n",
      "little: 8\n",
      "welcome: 6\n",
      "burlesque: 6\n",
      "show: 4\n",
      "less: 2\n",
      "add: 2\n",
      "smoke: 2\n",
      "dream: 2\n",
      "chorus: 2\n",
      "oh: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_weregonnamakeit:\n",
      "There are 117 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.462 in the data.\n",
      "The 117 most common tokens are:\n",
      "gonna: 16\n",
      "make: 16\n",
      "know: 9\n",
      "got: 6\n",
      "may: 5\n",
      "oh: 5\n",
      "yeah: 4\n",
      "love: 3\n",
      "mine: 3\n",
      "might: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_whataboutthemoonlight:\n",
      "There are 106 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 577 characters in the data.\n",
      "The lexical diversity is 0.585 in the data.\n",
      "The 106 most common tokens are:\n",
      "moonlight: 6\n",
      "way: 6\n",
      "loves: 5\n",
      "dreams: 5\n",
      "change: 5\n",
      "tomorrow: 5\n",
      "brings: 5\n",
      "sigh: 5\n",
      "touches: 5\n",
      "dont: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_whatllido:\n",
      "There are 25 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 133 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "The 25 most common tokens are:\n",
      "whatll: 8\n",
      "far: 2\n",
      "away: 2\n",
      "blue: 2\n",
      "im: 1\n",
      "wondering: 1\n",
      "kissing: 1\n",
      "photograph: 1\n",
      "tell: 1\n",
      "troubles: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_whenlovecallsyourname:\n",
      "There are 139 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 674 characters in the data.\n",
      "The lexical diversity is 0.489 in the data.\n",
      "The 139 most common tokens are:\n",
      "love: 11\n",
      "calls: 11\n",
      "name: 11\n",
      "theres: 4\n",
      "way: 4\n",
      "saying: 3\n",
      "follow: 3\n",
      "heart: 3\n",
      "wherever: 3\n",
      "goes: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_whenloversbecomestrangers:\n",
      "There are 160 tokens in the data.\n",
      "There are 75 unique tokens in the data.\n",
      "There are 847 characters in the data.\n",
      "The lexical diversity is 0.469 in the data.\n",
      "The 160 most common tokens are:\n",
      "shame: 13\n",
      "lovers: 9\n",
      "become: 9\n",
      "strangers: 9\n",
      "dont: 7\n",
      "know: 5\n",
      "youve: 4\n",
      "memories: 4\n",
      "shared: 4\n",
      "thats: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_whentheloveisgone:\n",
      "There are 119 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 557 characters in the data.\n",
      "The lexical diversity is 0.513 in the data.\n",
      "The 119 most common tokens are:\n",
      "love: 12\n",
      "gone: 10\n",
      "gotta: 7\n",
      "theres: 5\n",
      "strong: 4\n",
      "go: 4\n",
      "could: 3\n",
      "memories: 3\n",
      "tag: 3\n",
      "along: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_whenthemoneysgone:\n",
      "There are 109 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 493 characters in the data.\n",
      "The lexical diversity is 0.697 in the data.\n",
      "The 109 most common tokens are:\n",
      "moneys: 9\n",
      "gone: 9\n",
      "still: 4\n",
      "oh: 4\n",
      "want: 3\n",
      "chorus: 3\n",
      "baby: 3\n",
      "dont: 2\n",
      "life: 2\n",
      "pull: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_whenyoufindoutwhereyouregoinletmeknow:\n",
      "There are 82 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 358 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "The 82 most common tokens are:\n",
      "know: 8\n",
      "find: 7\n",
      "youre: 7\n",
      "goin: 6\n",
      "let: 6\n",
      "go: 2\n",
      "ill: 2\n",
      "never: 2\n",
      "say: 2\n",
      "matter: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_whenyouwalkaway:\n",
      "There are 173 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 785 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "The 173 most common tokens are:\n",
      "walk: 25\n",
      "away: 19\n",
      "wont: 11\n",
      "crying: 6\n",
      "cause: 5\n",
      "heart: 5\n",
      "dying: 5\n",
      "youre: 4\n",
      "come: 4\n",
      "back: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_wheredoyougo:\n",
      "There are 58 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 237 characters in the data.\n",
      "The lexical diversity is 0.724 in the data.\n",
      "The 58 most common tokens are:\n",
      "go: 5\n",
      "dont: 4\n",
      "know: 4\n",
      "right: 3\n",
      "youre: 2\n",
      "say: 2\n",
      "shes: 2\n",
      "dad: 2\n",
      "young: 1\n",
      "found: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_whoyougonnabelieve:\n",
      "There are 146 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 722 characters in the data.\n",
      "The lexical diversity is 0.555 in the data.\n",
      "The 146 most common tokens are:\n",
      "gonna: 13\n",
      "believe: 10\n",
      "oh: 5\n",
      "love: 4\n",
      "hope: 4\n",
      "never: 4\n",
      "could: 3\n",
      "dont: 3\n",
      "cause: 3\n",
      "need: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_whywasiborn:\n",
      "There are 45 tokens in the data.\n",
      "There are 24 unique tokens in the data.\n",
      "There are 188 characters in the data.\n",
      "The lexical diversity is 0.533 in the data.\n",
      "The 45 most common tokens are:\n",
      "born: 4\n",
      "tell: 3\n",
      "im: 3\n",
      "hope: 2\n",
      "try: 2\n",
      "draw: 2\n",
      "near: 2\n",
      "honey: 2\n",
      "cry: 2\n",
      "cause: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_willyoulovemetomorrow:\n",
      "There are 60 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 318 characters in the data.\n",
      "The lexical diversity is 0.583 in the data.\n",
      "The 60 most common tokens are:\n",
      "love: 11\n",
      "tomorrow: 7\n",
      "still: 5\n",
      "tonight: 3\n",
      "tell: 2\n",
      "wont: 2\n",
      "ask: 2\n",
      "youre: 1\n",
      "mine: 1\n",
      "completely: 1\n",
      "\n",
      "Descriptive statistics for cher - cher_willyouwaitforme:\n",
      "There are 130 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 618 characters in the data.\n",
      "The lexical diversity is 0.508 in the data.\n",
      "The 130 most common tokens are:\n",
      "wait: 12\n",
      "know: 6\n",
      "feels: 6\n",
      "love: 5\n",
      "youre: 4\n",
      "one: 4\n",
      "darling: 4\n",
      "im: 3\n",
      "nobody: 3\n",
      "said: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_withorwithoutyou:\n",
      "There are 99 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 499 characters in the data.\n",
      "The lexical diversity is 0.687 in the data.\n",
      "The 99 most common tokens are:\n",
      "heart: 5\n",
      "without: 4\n",
      "cause: 4\n",
      "alone: 3\n",
      "broken: 3\n",
      "fall: 3\n",
      "time: 3\n",
      "anymore: 3\n",
      "youre: 3\n",
      "dont: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_womansworld:\n",
      "There are 218 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 1108 characters in the data.\n",
      "The lexical diversity is 0.271 in the data.\n",
      "The 218 most common tokens are:\n",
      "world: 35\n",
      "womans: 33\n",
      "tell: 12\n",
      "truth: 12\n",
      "im: 10\n",
      "stronger: 8\n",
      "strong: 8\n",
      "enough: 8\n",
      "rise: 8\n",
      "broken: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_workinggirl:\n",
      "There are 110 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 558 characters in the data.\n",
      "The lexical diversity is 0.618 in the data.\n",
      "The 110 most common tokens are:\n",
      "girl: 11\n",
      "working: 9\n",
      "shes: 6\n",
      "livin: 3\n",
      "mans: 3\n",
      "world: 3\n",
      "gotta: 3\n",
      "take: 3\n",
      "stand: 3\n",
      "made: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_youbettersitdownkids:\n",
      "There are 138 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 605 characters in the data.\n",
      "The lexical diversity is 0.572 in the data.\n",
      "The 138 most common tokens are:\n",
      "kids: 13\n",
      "say: 5\n",
      "mother: 5\n",
      "ill: 4\n",
      "im: 4\n",
      "dont: 4\n",
      "eye: 4\n",
      "better: 3\n",
      "sit: 3\n",
      "try: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_youdonthavetosayyouloveme:\n",
      "There are 87 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 425 characters in the data.\n",
      "The lexical diversity is 0.448 in the data.\n",
      "The 87 most common tokens are:\n",
      "believe: 9\n",
      "dont: 8\n",
      "love: 7\n",
      "say: 4\n",
      "stay: 4\n",
      "left: 4\n",
      "close: 3\n",
      "hand: 3\n",
      "forever: 3\n",
      "understand: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_youhaventseenthelastofme:\n",
      "There are 134 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 596 characters in the data.\n",
      "The lexical diversity is 0.448 in the data.\n",
      "The 134 most common tokens are:\n",
      "havent: 8\n",
      "seen: 8\n",
      "last: 8\n",
      "im: 7\n",
      "ive: 6\n",
      "back: 6\n",
      "ill: 5\n",
      "dont: 4\n",
      "far: 4\n",
      "brought: 3\n",
      "\n",
      "Descriptive statistics for cher - cher_youknowit:\n",
      "There are 59 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 272 characters in the data.\n",
      "The lexical diversity is 0.644 in the data.\n",
      "The 59 most common tokens are:\n",
      "know: 9\n",
      "baby: 4\n",
      "well: 3\n",
      "love: 3\n",
      "youre: 2\n",
      "trance: 2\n",
      "pretty: 2\n",
      "want: 2\n",
      "show: 2\n",
      "chance: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_youngandpretty:\n",
      "There are 96 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 479 characters in the data.\n",
      "The lexical diversity is 0.562 in the data.\n",
      "The 96 most common tokens are:\n",
      "young: 6\n",
      "pretty: 6\n",
      "say: 5\n",
      "make: 5\n",
      "youre: 5\n",
      "come: 3\n",
      "city: 3\n",
      "inside: 3\n",
      "way: 3\n",
      "suburbs: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_yoursuntiltomorrow:\n",
      "There are 87 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 445 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "The 87 most common tokens are:\n",
      "tomorrow: 10\n",
      "let: 8\n",
      "tonight: 3\n",
      "one: 3\n",
      "night: 3\n",
      "till: 3\n",
      "cant: 2\n",
      "give: 2\n",
      "life: 2\n",
      "mine: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_youtakeitall:\n",
      "There are 74 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 332 characters in the data.\n",
      "The lexical diversity is 0.473 in the data.\n",
      "The 74 most common tokens are:\n",
      "every: 6\n",
      "ever: 6\n",
      "take: 5\n",
      "sometimes: 4\n",
      "ooh: 4\n",
      "like: 4\n",
      "sea: 4\n",
      "takes: 4\n",
      "land: 4\n",
      "feet: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_youvemademesoveryhappy:\n",
      "There are 83 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 358 characters in the data.\n",
      "The lexical diversity is 0.554 in the data.\n",
      "The 83 most common tokens are:\n",
      "made: 6\n",
      "happy: 6\n",
      "im: 6\n",
      "came: 6\n",
      "life: 5\n",
      "glad: 4\n",
      "baby: 4\n",
      "love: 3\n",
      "thank: 3\n",
      "much: 2\n",
      "\n",
      "Descriptive statistics for cher - cher_youvereallygotaholdonme:\n",
      "There are 132 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 569 characters in the data.\n",
      "The lexical diversity is 0.265 in the data.\n",
      "The 132 most common tokens are:\n",
      "hold: 25\n",
      "really: 17\n",
      "got: 17\n",
      "youve: 9\n",
      "want: 8\n",
      "oh: 6\n",
      "dont: 5\n",
      "love: 5\n",
      "please: 5\n",
      "aw: 4\n",
      "\n",
      "Descriptive statistics for cher - cher_youwouldntknowlove:\n",
      "There are 172 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 858 characters in the data.\n",
      "The lexical diversity is 0.302 in the data.\n",
      "The 172 most common tokens are:\n",
      "know: 29\n",
      "wouldnt: 27\n",
      "love: 23\n",
      "knocked: 5\n",
      "door: 5\n",
      "never: 5\n",
      "knew: 5\n",
      "landed: 5\n",
      "hands: 5\n",
      "heavens: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_88days:\n",
      "There are 205 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 887 characters in the data.\n",
      "The lexical diversity is 0.390 in the data.\n",
      "The 205 most common tokens are:\n",
      "got: 24\n",
      "work: 16\n",
      "88: 14\n",
      "days: 14\n",
      "ive: 11\n",
      "til: 8\n",
      "sun: 7\n",
      "done: 6\n",
      "still: 5\n",
      "cant: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_aintnothing:\n",
      "There are 66 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 305 characters in the data.\n",
      "The lexical diversity is 0.606 in the data.\n",
      "The 66 most common tokens are:\n",
      "really: 6\n",
      "thing: 5\n",
      "want: 4\n",
      "baby: 4\n",
      "aint: 3\n",
      "boy: 3\n",
      "verse: 2\n",
      "gotta: 2\n",
      "need: 2\n",
      "im: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_anytimeyoulike:\n",
      "There are 119 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 578 characters in the data.\n",
      "The lexical diversity is 0.471 in the data.\n",
      "The 119 most common tokens are:\n",
      "pressure: 13\n",
      "tell: 10\n",
      "like: 9\n",
      "boy: 8\n",
      "anytime: 5\n",
      "love: 5\n",
      "time: 4\n",
      "yeah: 4\n",
      "ya: 4\n",
      "alright: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_babyforgiveme:\n",
      "There are 77 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 343 characters in the data.\n",
      "The lexical diversity is 0.442 in the data.\n",
      "The 77 most common tokens are:\n",
      "baby: 19\n",
      "forgive: 13\n",
      "wont: 3\n",
      "give: 3\n",
      "chance: 3\n",
      "one: 3\n",
      "try: 3\n",
      "yeah: 2\n",
      "got: 2\n",
      "know: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_beach2k20:\n",
      "There are 174 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.190 in the data.\n",
      "The 174 most common tokens are:\n",
      "party: 19\n",
      "go: 15\n",
      "beach: 14\n",
      "ok: 14\n",
      "lets: 10\n",
      "place: 8\n",
      "come: 6\n",
      "wanna: 5\n",
      "call: 5\n",
      "someone: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_becauseitsinthemusic:\n",
      "There are 129 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 639 characters in the data.\n",
      "The lexical diversity is 0.457 in the data.\n",
      "The 129 most common tokens are:\n",
      "music: 7\n",
      "anyway: 7\n",
      "yeah: 6\n",
      "im: 6\n",
      "right: 6\n",
      "back: 6\n",
      "moment: 6\n",
      "makes: 6\n",
      "want: 6\n",
      "cry: 6\n",
      "\n",
      "Descriptive statistics for robyn - robyn_bemine:\n",
      "There are 175 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 857 characters in the data.\n",
      "The lexical diversity is 0.474 in the data.\n",
      "The 175 most common tokens are:\n",
      "never: 29\n",
      "mine: 14\n",
      "theres: 5\n",
      "every: 5\n",
      "cause: 5\n",
      "thing: 4\n",
      "youre: 4\n",
      "like: 4\n",
      "time: 4\n",
      "good: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_betweenthelines:\n",
      "There are 166 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 785 characters in the data.\n",
      "The lexical diversity is 0.307 in the data.\n",
      "The 166 most common tokens are:\n",
      "lines: 20\n",
      "reading: 19\n",
      "like: 15\n",
      "baby: 13\n",
      "im: 8\n",
      "got: 7\n",
      "even: 5\n",
      "dont: 4\n",
      "know: 4\n",
      "hit: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_bigcity:\n",
      "There are 181 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 856 characters in the data.\n",
      "The lexical diversity is 0.420 in the data.\n",
      "The 181 most common tokens are:\n",
      "wont: 8\n",
      "dont: 7\n",
      "really: 7\n",
      "city: 6\n",
      "make: 6\n",
      "take: 4\n",
      "im: 4\n",
      "care: 4\n",
      "need: 4\n",
      "lately: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_bionicwoman:\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "The 10 most common tokens are:\n",
      "bionic: 1\n",
      "woman: 1\n",
      "good: 1\n",
      "evening: 1\n",
      "ladies: 1\n",
      "captain: 1\n",
      "speaking: 1\n",
      "attempt: 1\n",
      "crashlanding: 1\n",
      "going: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_blowmymind:\n",
      "There are 91 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 436 characters in the data.\n",
      "The lexical diversity is 0.582 in the data.\n",
      "The 91 most common tokens are:\n",
      "youre: 10\n",
      "blow: 7\n",
      "mind: 7\n",
      "baby: 4\n",
      "ill: 4\n",
      "way: 3\n",
      "talking: 3\n",
      "cool: 3\n",
      "anything: 3\n",
      "hey: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_breakdownintermission:\n",
      "There are 64 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 342 characters in the data.\n",
      "The lexical diversity is 0.703 in the data.\n",
      "The 64 most common tokens are:\n",
      "break: 3\n",
      "suckers: 3\n",
      "nobody: 2\n",
      "knows: 2\n",
      "whats: 2\n",
      "point: 2\n",
      "view: 2\n",
      "goes: 2\n",
      "chorus: 2\n",
      "wanna: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_buffalostance:\n",
      "There are 277 tokens in the data.\n",
      "There are 132 unique tokens in the data.\n",
      "There are 1384 characters in the data.\n",
      "The lexical diversity is 0.477 in the data.\n",
      "The 277 most common tokens are:\n",
      "love: 13\n",
      "looking: 11\n",
      "moneyman: 11\n",
      "win: 10\n",
      "good: 8\n",
      "whos: 6\n",
      "get: 6\n",
      "dont: 6\n",
      "ill: 6\n",
      "buffalo: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_bumlikeyou:\n",
      "There are 169 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 823 characters in the data.\n",
      "The lexical diversity is 0.373 in the data.\n",
      "The 169 most common tokens are:\n",
      "like: 12\n",
      "wasting: 12\n",
      "time: 12\n",
      "bum: 11\n",
      "new: 11\n",
      "favourite: 10\n",
      "thing: 10\n",
      "could: 9\n",
      "youre: 4\n",
      "fall: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_bumpyride:\n",
      "There are 138 tokens in the data.\n",
      "There are 97 unique tokens in the data.\n",
      "There are 690 characters in the data.\n",
      "The lexical diversity is 0.703 in the data.\n",
      "The 138 most common tokens are:\n",
      "bumpy: 4\n",
      "ride: 4\n",
      "whos: 4\n",
      "hold: 4\n",
      "youve: 4\n",
      "gotta: 4\n",
      "never: 4\n",
      "keep: 4\n",
      "dont: 3\n",
      "sometimes: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_callyourgirlfriend:\n",
      "There are 130 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 640 characters in the data.\n",
      "The lexical diversity is 0.362 in the data.\n",
      "The 130 most common tokens are:\n",
      "call: 7\n",
      "girlfriend: 7\n",
      "give: 6\n",
      "tell: 6\n",
      "time: 5\n",
      "talk: 5\n",
      "reasons: 5\n",
      "say: 5\n",
      "fault: 5\n",
      "met: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_cobrastyle:\n",
      "There are 351 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 1542 characters in the data.\n",
      "The lexical diversity is 0.177 in the data.\n",
      "The 351 most common tokens are:\n",
      "deng: 32\n",
      "digi: 32\n",
      "bomb: 16\n",
      "di: 16\n",
      "gi: 16\n",
      "dont: 14\n",
      "press: 12\n",
      "something: 12\n",
      "get: 8\n",
      "style: 8\n",
      "\n",
      "Descriptive statistics for robyn - robyn_crashandburngirl:\n",
      "There are 152 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 729 characters in the data.\n",
      "The lexical diversity is 0.553 in the data.\n",
      "The 152 most common tokens are:\n",
      "girl: 10\n",
      "crash: 7\n",
      "burn: 7\n",
      "dont: 6\n",
      "mind: 5\n",
      "fall: 5\n",
      "face: 5\n",
      "hits: 5\n",
      "ground: 5\n",
      "whats: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_criminalintent:\n",
      "There are 283 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 1638 characters in the data.\n",
      "The lexical diversity is 0.314 in the data.\n",
      "The 283 most common tokens are:\n",
      "criminal: 17\n",
      "intent: 17\n",
      "somebody: 16\n",
      "alert: 16\n",
      "authorities: 16\n",
      "got: 16\n",
      "imma: 14\n",
      "grind: 8\n",
      "say: 8\n",
      "conspiracy: 7\n",
      "\n",
      "Descriptive statistics for robyn - robyn_crywhenyougetolder:\n",
      "There are 158 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 735 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "The 158 most common tokens are:\n",
      "get: 11\n",
      "cry: 7\n",
      "older: 7\n",
      "never: 7\n",
      "told: 7\n",
      "light: 6\n",
      "love: 6\n",
      "hurts: 6\n",
      "right: 6\n",
      "mama: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_curriculumvitae:\n",
      "There are 170 tokens in the data.\n",
      "There are 162 unique tokens in the data.\n",
      "There are 1042 characters in the data.\n",
      "The lexical diversity is 0.953 in the data.\n",
      "The 170 most common tokens are:\n",
      "konichiwa: 2\n",
      "records: 2\n",
      "get: 2\n",
      "listen: 2\n",
      "turn: 2\n",
      "world: 2\n",
      "record: 2\n",
      "shes: 2\n",
      "curriculum: 1\n",
      "vitae: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dancehallqueen:\n",
      "There are 187 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 879 characters in the data.\n",
      "The lexical diversity is 0.332 in the data.\n",
      "The 187 most common tokens are:\n",
      "wow: 16\n",
      "like: 12\n",
      "queen: 9\n",
      "dancehall: 8\n",
      "thing: 8\n",
      "jaw: 7\n",
      "dropped: 7\n",
      "music: 7\n",
      "stop: 7\n",
      "know: 7\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dancehallqueen114530:\n",
      "There are 187 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 879 characters in the data.\n",
      "The lexical diversity is 0.332 in the data.\n",
      "The 187 most common tokens are:\n",
      "wow: 16\n",
      "like: 12\n",
      "queen: 9\n",
      "dancehall: 8\n",
      "thing: 8\n",
      "jaw: 7\n",
      "dropped: 7\n",
      "music: 7\n",
      "stop: 7\n",
      "know: 7\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dancingonmyown:\n",
      "There are 137 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.416 in the data.\n",
      "The 137 most common tokens are:\n",
      "im: 19\n",
      "oh: 11\n",
      "dancing: 10\n",
      "keep: 9\n",
      "youre: 5\n",
      "see: 5\n",
      "corner: 4\n",
      "watching: 4\n",
      "kiss: 4\n",
      "giving: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dancingonmyown114521:\n",
      "There are 137 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 614 characters in the data.\n",
      "The lexical diversity is 0.423 in the data.\n",
      "The 137 most common tokens are:\n",
      "im: 19\n",
      "dancing: 10\n",
      "keep: 9\n",
      "ohh: 7\n",
      "youre: 5\n",
      "see: 5\n",
      "corner: 4\n",
      "watching: 4\n",
      "kiss: 4\n",
      "giving: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_doitagain:\n",
      "There are 81 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 375 characters in the data.\n",
      "The lexical diversity is 0.358 in the data.\n",
      "The 81 most common tokens are:\n",
      "lets: 9\n",
      "dont: 6\n",
      "know: 5\n",
      "wait: 4\n",
      "hurts: 4\n",
      "good: 4\n",
      "well: 4\n",
      "care: 3\n",
      "say: 3\n",
      "wanna: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dontfuckingtellmewhattodo:\n",
      "There are 177 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 1078 characters in the data.\n",
      "The lexical diversity is 0.226 in the data.\n",
      "The 177 most common tokens are:\n",
      "killing: 68\n",
      "drinking: 16\n",
      "dont: 10\n",
      "fucking: 10\n",
      "tell: 10\n",
      "youre: 8\n",
      "smoking: 3\n",
      "tv: 3\n",
      "nagging: 3\n",
      "cant: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dontfuckingtellmewhattodo114520:\n",
      "There are 177 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 1078 characters in the data.\n",
      "The lexical diversity is 0.226 in the data.\n",
      "The 177 most common tokens are:\n",
      "killing: 68\n",
      "drinking: 16\n",
      "dont: 10\n",
      "fucking: 10\n",
      "tell: 10\n",
      "youre: 8\n",
      "smoking: 3\n",
      "tv: 3\n",
      "nagging: 3\n",
      "cant: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dontstopthemusic:\n",
      "There are 151 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 731 characters in the data.\n",
      "The lexical diversity is 0.576 in the data.\n",
      "The 151 most common tokens are:\n",
      "come: 10\n",
      "stop: 9\n",
      "music: 8\n",
      "baby: 8\n",
      "dont: 6\n",
      "keep: 6\n",
      "dancing: 5\n",
      "got: 4\n",
      "world: 4\n",
      "make: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_dontwantyouback:\n",
      "There are 67 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 297 characters in the data.\n",
      "The lexical diversity is 0.522 in the data.\n",
      "The 67 most common tokens are:\n",
      "dont: 7\n",
      "wanna: 7\n",
      "back: 5\n",
      "want: 3\n",
      "even: 3\n",
      "know: 2\n",
      "man: 2\n",
      "get: 2\n",
      "free: 2\n",
      "girl: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_doyouknowwhatittakes:\n",
      "There are 95 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 447 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "The 95 most common tokens are:\n",
      "know: 9\n",
      "takes: 6\n",
      "around: 4\n",
      "got: 4\n",
      "love: 4\n",
      "always: 3\n",
      "dont: 2\n",
      "cheap: 2\n",
      "talk: 2\n",
      "need: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_doyoureallywantmeshowrespect:\n",
      "There are 111 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 520 characters in the data.\n",
      "The lexical diversity is 0.631 in the data.\n",
      "The 111 most common tokens are:\n",
      "really: 4\n",
      "want: 4\n",
      "right: 4\n",
      "show: 3\n",
      "respect: 3\n",
      "know: 3\n",
      "treat: 3\n",
      "love: 3\n",
      "gotta: 3\n",
      "little: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_eclipse:\n",
      "There are 103 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 550 characters in the data.\n",
      "The lexical diversity is 0.621 in the data.\n",
      "The 103 most common tokens are:\n",
      "right: 9\n",
      "youre: 4\n",
      "words: 4\n",
      "unspoken: 4\n",
      "falls: 4\n",
      "apart: 4\n",
      "theres: 3\n",
      "eye: 3\n",
      "day: 3\n",
      "break: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_electric:\n",
      "There are 153 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 818 characters in the data.\n",
      "The lexical diversity is 0.412 in the data.\n",
      "The 153 most common tokens are:\n",
      "electric: 28\n",
      "cant: 6\n",
      "deny: 6\n",
      "natural: 5\n",
      "high: 5\n",
      "dont: 5\n",
      "always: 5\n",
      "know: 5\n",
      "keep: 5\n",
      "ego: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_everagain:\n",
      "There are 199 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 993 characters in the data.\n",
      "The lexical diversity is 0.302 in the data.\n",
      "The 199 most common tokens are:\n",
      "gonna: 23\n",
      "never: 21\n",
      "ever: 18\n",
      "brokenhearted: 11\n",
      "come: 8\n",
      "lets: 8\n",
      "im: 7\n",
      "let: 6\n",
      "happen: 6\n",
      "nothing: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_everylittlething:\n",
      "There are 96 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 459 characters in the data.\n",
      "The lexical diversity is 0.302 in the data.\n",
      "The 96 most common tokens are:\n",
      "know: 12\n",
      "waiting: 10\n",
      "every: 9\n",
      "little: 9\n",
      "thing: 9\n",
      "im: 8\n",
      "baby: 6\n",
      "say: 4\n",
      "really: 4\n",
      "fought: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_fembot:\n",
      "There are 246 tokens in the data.\n",
      "There are 113 unique tokens in the data.\n",
      "There are 1352 characters in the data.\n",
      "The lexical diversity is 0.459 in the data.\n",
      "The 246 most common tokens are:\n",
      "back: 8\n",
      "got: 7\n",
      "gone: 7\n",
      "tech: 7\n",
      "never: 7\n",
      "going: 7\n",
      "aint: 6\n",
      "baby: 6\n",
      "hit: 5\n",
      "ive: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_fembot114519:\n",
      "There are 246 tokens in the data.\n",
      "There are 113 unique tokens in the data.\n",
      "There are 1352 characters in the data.\n",
      "The lexical diversity is 0.459 in the data.\n",
      "The 246 most common tokens are:\n",
      "back: 8\n",
      "got: 7\n",
      "gone: 7\n",
      "tech: 7\n",
      "never: 7\n",
      "going: 7\n",
      "aint: 6\n",
      "baby: 6\n",
      "hit: 5\n",
      "ive: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_getmyselftogether:\n",
      "There are 224 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 999 characters in the data.\n",
      "The lexical diversity is 0.357 in the data.\n",
      "The 224 most common tokens are:\n",
      "got: 35\n",
      "get: 21\n",
      "together: 11\n",
      "gone: 10\n",
      "cant: 8\n",
      "hurt: 7\n",
      "like: 6\n",
      "wrong: 5\n",
      "theres: 5\n",
      "tell: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_givingyouback:\n",
      "There are 112 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 530 characters in the data.\n",
      "The lexical diversity is 0.616 in the data.\n",
      "The 112 most common tokens are:\n",
      "right: 7\n",
      "back: 6\n",
      "im: 6\n",
      "giving: 4\n",
      "nothing: 4\n",
      "another: 3\n",
      "time: 3\n",
      "would: 3\n",
      "wrong: 3\n",
      "chorus: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_gottoworkitout:\n",
      "There are 216 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 897 characters in the data.\n",
      "The lexical diversity is 0.236 in the data.\n",
      "The 216 most common tokens are:\n",
      "work: 43\n",
      "got: 21\n",
      "shake: 16\n",
      "make: 12\n",
      "fit: 12\n",
      "body: 11\n",
      "til: 8\n",
      "c√¢mon: 8\n",
      "break: 7\n",
      "to√¢: 6\n",
      "\n",
      "Descriptive statistics for robyn - robyn_handleme:\n",
      "There are 254 tokens in the data.\n",
      "There are 96 unique tokens in the data.\n",
      "There are 1240 characters in the data.\n",
      "The lexical diversity is 0.378 in the data.\n",
      "The 254 most common tokens are:\n",
      "handle: 18\n",
      "cant: 17\n",
      "youre: 11\n",
      "sure: 10\n",
      "yeah: 8\n",
      "see: 7\n",
      "get: 6\n",
      "act: 6\n",
      "got: 5\n",
      "simple: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_hangwithme:\n",
      "There are 134 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.313 in the data.\n",
      "The 134 most common tokens are:\n",
      "hang: 12\n",
      "gonna: 12\n",
      "im: 7\n",
      "right: 6\n",
      "guess: 4\n",
      "dont: 4\n",
      "fall: 4\n",
      "recklessly: 4\n",
      "headlessly: 4\n",
      "love: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_hangwithme114525:\n",
      "There are 134 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 713 characters in the data.\n",
      "The lexical diversity is 0.313 in the data.\n",
      "The 134 most common tokens are:\n",
      "hang: 12\n",
      "gonna: 12\n",
      "im: 7\n",
      "right: 6\n",
      "guess: 4\n",
      "dont: 4\n",
      "fall: 4\n",
      "recklessly: 4\n",
      "headlessly: 4\n",
      "love: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_hangwithmeacousticversion:\n",
      "There are 102 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 547 characters in the data.\n",
      "The lexical diversity is 0.431 in the data.\n",
      "The 102 most common tokens are:\n",
      "hang: 11\n",
      "gonna: 8\n",
      "im: 5\n",
      "right: 4\n",
      "guess: 3\n",
      "dont: 3\n",
      "fall: 3\n",
      "recklessly: 3\n",
      "headlessly: 3\n",
      "love: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_healthylove:\n",
      "There are 150 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 748 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "The 150 most common tokens are:\n",
      "love: 12\n",
      "healthy: 11\n",
      "ever: 9\n",
      "strange: 7\n",
      "feeling: 7\n",
      "hes: 6\n",
      "cant: 6\n",
      "something: 4\n",
      "dont: 4\n",
      "give: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_herewego:\n",
      "There are 48 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 246 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "The 48 most common tokens are:\n",
      "go: 5\n",
      "thought: 3\n",
      "makin: 3\n",
      "another: 2\n",
      "baby: 2\n",
      "friends: 2\n",
      "time: 2\n",
      "nothing: 2\n",
      "changed: 2\n",
      "love: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_honey:\n",
      "There are 221 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 1028 characters in the data.\n",
      "The lexical diversity is 0.271 in the data.\n",
      "The 221 most common tokens are:\n",
      "get: 19\n",
      "baby: 18\n",
      "honey: 14\n",
      "want: 13\n",
      "need: 10\n",
      "come: 10\n",
      "youre: 7\n",
      "gonna: 7\n",
      "every: 6\n",
      "let: 6\n",
      "\n",
      "Descriptive statistics for robyn - robyn_how:\n",
      "There are 92 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 455 characters in the data.\n",
      "The lexical diversity is 0.696 in the data.\n",
      "The 92 most common tokens are:\n",
      "know: 5\n",
      "time: 5\n",
      "right: 4\n",
      "cause: 3\n",
      "long: 3\n",
      "let: 3\n",
      "tonight: 3\n",
      "things: 2\n",
      "dont: 2\n",
      "timing: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_humanbeing:\n",
      "There are 113 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 479 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "The 113 most common tokens are:\n",
      "move: 15\n",
      "human: 13\n",
      "im: 12\n",
      "dont: 8\n",
      "body: 8\n",
      "baby: 4\n",
      "give: 4\n",
      "know: 3\n",
      "theres: 3\n",
      "yeah: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_includemeout:\n",
      "There are 234 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 1186 characters in the data.\n",
      "The lexical diversity is 0.380 in the data.\n",
      "The 234 most common tokens are:\n",
      "include: 19\n",
      "dont: 16\n",
      "world: 8\n",
      "fall: 8\n",
      "apart: 8\n",
      "room: 8\n",
      "inside: 8\n",
      "heart: 8\n",
      "day: 6\n",
      "beat: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_indestructible:\n",
      "There are 182 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 894 characters in the data.\n",
      "The lexical diversity is 0.313 in the data.\n",
      "The 182 most common tokens are:\n",
      "im: 22\n",
      "love: 20\n",
      "like: 13\n",
      "gonna: 12\n",
      "indestructible: 9\n",
      "never: 8\n",
      "ones: 6\n",
      "let: 5\n",
      "go: 5\n",
      "ive: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_indestructibleacousticversion:\n",
      "There are 182 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 887 characters in the data.\n",
      "The lexical diversity is 0.330 in the data.\n",
      "The 182 most common tokens are:\n",
      "im: 20\n",
      "love: 18\n",
      "like: 13\n",
      "gonna: 10\n",
      "indestructible: 9\n",
      "never: 8\n",
      "ones: 6\n",
      "let: 5\n",
      "go: 5\n",
      "ive: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_inmyeyes:\n",
      "There are 146 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 670 characters in the data.\n",
      "The lexical diversity is 0.452 in the data.\n",
      "The 146 most common tokens are:\n",
      "eyes: 9\n",
      "look: 8\n",
      "ok: 7\n",
      "like: 6\n",
      "think: 6\n",
      "little: 5\n",
      "better: 5\n",
      "know: 4\n",
      "say: 4\n",
      "feel: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_inmyeyes114532:\n",
      "There are 146 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 670 characters in the data.\n",
      "The lexical diversity is 0.452 in the data.\n",
      "The 146 most common tokens are:\n",
      "eyes: 9\n",
      "look: 8\n",
      "ok: 7\n",
      "like: 6\n",
      "think: 6\n",
      "little: 5\n",
      "better: 5\n",
      "know: 4\n",
      "say: 4\n",
      "feel: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_inmyheart:\n",
      "There are 59 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 279 characters in the data.\n",
      "The lexical diversity is 0.542 in the data.\n",
      "The 59 most common tokens are:\n",
      "gonna: 7\n",
      "heart: 6\n",
      "im: 4\n",
      "never: 4\n",
      "think: 3\n",
      "keep: 3\n",
      "better: 2\n",
      "dont: 2\n",
      "change: 2\n",
      "leave: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_iwish:\n",
      "There are 109 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 478 characters in the data.\n",
      "The lexical diversity is 0.495 in the data.\n",
      "The 109 most common tokens are:\n",
      "wish: 11\n",
      "baby: 6\n",
      "know: 4\n",
      "could: 4\n",
      "day: 4\n",
      "stars: 4\n",
      "would: 4\n",
      "away: 3\n",
      "love: 3\n",
      "dont: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_jackuoff:\n",
      "There are 146 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 554 characters in the data.\n",
      "The lexical diversity is 0.418 in the data.\n",
      "The 146 most common tokens are:\n",
      "u: 31\n",
      "jack: 25\n",
      "ill: 16\n",
      "youre: 5\n",
      "go: 3\n",
      "take: 3\n",
      "movie: 2\n",
      "show: 2\n",
      "back: 2\n",
      "cant: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_jagvetendejligrosa:\n",
      "There are 101 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 432 characters in the data.\n",
      "The lexical diversity is 0.752 in the data.\n",
      "The 101 most common tokens are:\n",
      "jag: 4\n",
      "och: 4\n",
      "som: 4\n",
      "en: 3\n",
      "n√£r: 3\n",
      "s√£: 3\n",
      "hj√£rtans: 3\n",
      "vet: 2\n",
      "dejlig: 2\n",
      "rosa: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_justanothergirlfriend:\n",
      "There are 83 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 400 characters in the data.\n",
      "The lexical diversity is 0.687 in the data.\n",
      "The 83 most common tokens are:\n",
      "another: 5\n",
      "baby: 5\n",
      "girlfriend: 4\n",
      "say: 4\n",
      "stay: 4\n",
      "cause: 3\n",
      "gilr: 2\n",
      "something: 2\n",
      "want: 2\n",
      "gonna: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_keepthisfireburning:\n",
      "There are 180 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 812 characters in the data.\n",
      "The lexical diversity is 0.317 in the data.\n",
      "The 180 most common tokens are:\n",
      "ill: 17\n",
      "keep: 16\n",
      "fire: 8\n",
      "burning: 8\n",
      "even: 8\n",
      "baby: 6\n",
      "side: 5\n",
      "right: 4\n",
      "behind: 4\n",
      "goin: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_konichiwabitches:\n",
      "There are 195 tokens in the data.\n",
      "There are 151 unique tokens in the data.\n",
      "There are 994 characters in the data.\n",
      "The lexical diversity is 0.774 in the data.\n",
      "The 195 most common tokens are:\n",
      "like: 10\n",
      "im: 7\n",
      "konichiwa: 4\n",
      "bitches: 4\n",
      "wanna: 4\n",
      "ill: 3\n",
      "put: 3\n",
      "cause: 3\n",
      "zoom: 3\n",
      "know: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_longgone:\n",
      "There are 164 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 731 characters in the data.\n",
      "The lexical diversity is 0.488 in the data.\n",
      "The 164 most common tokens are:\n",
      "gone: 26\n",
      "long: 23\n",
      "im: 18\n",
      "today: 5\n",
      "coming: 4\n",
      "time: 3\n",
      "chorus: 3\n",
      "hear: 2\n",
      "cause: 2\n",
      "next: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_losecontrol:\n",
      "There are 121 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 530 characters in the data.\n",
      "The lexical diversity is 0.289 in the data.\n",
      "The 121 most common tokens are:\n",
      "cant: 23\n",
      "control: 16\n",
      "dont: 16\n",
      "like: 15\n",
      "hey: 4\n",
      "wanna: 3\n",
      "know: 3\n",
      "lose: 2\n",
      "hold: 2\n",
      "shut: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_loveisfree:\n",
      "There are 315 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 1426 characters in the data.\n",
      "The lexical diversity is 0.152 in the data.\n",
      "The 315 most common tokens are:\n",
      "boom: 46\n",
      "free: 32\n",
      "baby: 24\n",
      "love: 19\n",
      "give: 18\n",
      "chica: 16\n",
      "ima: 14\n",
      "sometimes: 10\n",
      "cant: 8\n",
      "slow: 8\n",
      "\n",
      "Descriptive statistics for robyn - robyn_lovekills:\n",
      "There are 246 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 1162 characters in the data.\n",
      "The lexical diversity is 0.171 in the data.\n",
      "The 246 most common tokens are:\n",
      "love: 25\n",
      "know: 16\n",
      "kills: 15\n",
      "cus: 14\n",
      "dont: 11\n",
      "cold: 10\n",
      "hard: 10\n",
      "world: 10\n",
      "protect: 8\n",
      "youll: 7\n",
      "\n",
      "Descriptive statistics for robyn - robyn_lovekills114524:\n",
      "There are 247 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 1167 characters in the data.\n",
      "The lexical diversity is 0.170 in the data.\n",
      "The 247 most common tokens are:\n",
      "love: 25\n",
      "know: 16\n",
      "kills: 15\n",
      "cus: 14\n",
      "dont: 11\n",
      "cold: 10\n",
      "hard: 10\n",
      "world: 10\n",
      "protect: 8\n",
      "youll: 7\n",
      "\n",
      "Descriptive statistics for robyn - robyn_mainthing:\n",
      "There are 147 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 659 characters in the data.\n",
      "The lexical diversity is 0.463 in the data.\n",
      "The 147 most common tokens are:\n",
      "work: 13\n",
      "lets: 12\n",
      "thing: 8\n",
      "weve: 6\n",
      "got: 6\n",
      "youre: 6\n",
      "know: 5\n",
      "im: 5\n",
      "still: 4\n",
      "wrong: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_missingu:\n",
      "There are 188 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 1003 characters in the data.\n",
      "The lexical diversity is 0.404 in the data.\n",
      "The 188 most common tokens are:\n",
      "space: 10\n",
      "left: 10\n",
      "theres: 10\n",
      "empty: 9\n",
      "behind: 9\n",
      "youre: 8\n",
      "keep: 7\n",
      "still: 6\n",
      "clock: 5\n",
      "stopped: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_mondaymorning:\n",
      "There are 98 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 531 characters in the data.\n",
      "The lexical diversity is 0.643 in the data.\n",
      "The 98 most common tokens are:\n",
      "love: 7\n",
      "morning: 4\n",
      "friday: 4\n",
      "saturday: 4\n",
      "sunday: 4\n",
      "could: 4\n",
      "monday: 3\n",
      "always: 3\n",
      "chorus: 3\n",
      "know: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_monument:\n",
      "There are 82 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 350 characters in the data.\n",
      "The lexical diversity is 0.366 in the data.\n",
      "The 82 most common tokens are:\n",
      "life: 14\n",
      "gone: 7\n",
      "love: 7\n",
      "let: 6\n",
      "monument: 4\n",
      "im: 4\n",
      "go: 4\n",
      "make: 3\n",
      "body: 3\n",
      "moment: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_moonlight:\n",
      "There are 74 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.554 in the data.\n",
      "The 74 most common tokens are:\n",
      "baby: 8\n",
      "dont: 8\n",
      "go: 5\n",
      "moonlight: 3\n",
      "oh: 3\n",
      "let: 2\n",
      "wait: 2\n",
      "stay: 2\n",
      "verse: 2\n",
      "feel: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_myonlyreason:\n",
      "There are 100 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 510 characters in the data.\n",
      "The lexical diversity is 0.680 in the data.\n",
      "The 100 most common tokens are:\n",
      "love: 5\n",
      "chorus: 4\n",
      "thats: 4\n",
      "im: 4\n",
      "dont: 4\n",
      "yeah: 4\n",
      "reason: 3\n",
      "still: 3\n",
      "make: 2\n",
      "crazy: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_mytruth:\n",
      "There are 102 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 524 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "The 102 most common tokens are:\n",
      "truth: 12\n",
      "dont: 6\n",
      "cause: 4\n",
      "chorus: 4\n",
      "cant: 3\n",
      "thats: 3\n",
      "say: 2\n",
      "want: 2\n",
      "help: 2\n",
      "doesnt: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_noneofdem:\n",
      "There are 126 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 555 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "The 126 most common tokens are:\n",
      "none: 14\n",
      "take: 6\n",
      "away: 6\n",
      "ive: 4\n",
      "im: 4\n",
      "got: 4\n",
      "get: 3\n",
      "bored: 3\n",
      "town: 3\n",
      "play: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_noneofdem114527:\n",
      "There are 126 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 555 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "The 126 most common tokens are:\n",
      "none: 14\n",
      "take: 6\n",
      "away: 6\n",
      "ive: 4\n",
      "im: 4\n",
      "got: 4\n",
      "get: 3\n",
      "bored: 3\n",
      "town: 3\n",
      "play: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_notontheinside:\n",
      "There are 94 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 486 characters in the data.\n",
      "The lexical diversity is 0.745 in the data.\n",
      "The 94 most common tokens are:\n",
      "never: 4\n",
      "hes: 4\n",
      "isnt: 3\n",
      "chorus: 3\n",
      "loveless: 3\n",
      "inside: 2\n",
      "people: 2\n",
      "watch: 2\n",
      "struggle: 2\n",
      "fact: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_obaby:\n",
      "There are 90 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 417 characters in the data.\n",
      "The lexical diversity is 0.689 in the data.\n",
      "The 90 most common tokens are:\n",
      "baby: 6\n",
      "dont: 6\n",
      "chorus: 4\n",
      "youre: 4\n",
      "say: 4\n",
      "wont: 3\n",
      "oh: 2\n",
      "make: 2\n",
      "cry: 2\n",
      "time: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_play:\n",
      "There are 96 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 448 characters in the data.\n",
      "The lexical diversity is 0.573 in the data.\n",
      "The 96 most common tokens are:\n",
      "yeah: 14\n",
      "say: 7\n",
      "people: 6\n",
      "never: 5\n",
      "play: 3\n",
      "yet: 3\n",
      "chorus: 3\n",
      "lives: 2\n",
      "scared: 2\n",
      "even: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_psycho:\n",
      "There are 194 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 954 characters in the data.\n",
      "The lexical diversity is 0.376 in the data.\n",
      "The 194 most common tokens are:\n",
      "baby: 20\n",
      "psycho: 19\n",
      "youre: 15\n",
      "know: 11\n",
      "dont: 9\n",
      "wanna: 6\n",
      "turning: 6\n",
      "leave: 5\n",
      "better: 4\n",
      "rethink: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_robotboy:\n",
      "There are 52 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 257 characters in the data.\n",
      "The lexical diversity is 0.750 in the data.\n",
      "The 52 most common tokens are:\n",
      "boy: 5\n",
      "hey: 4\n",
      "little: 3\n",
      "lost: 2\n",
      "robot: 2\n",
      "droid: 2\n",
      "youve: 2\n",
      "robotboy: 1\n",
      "smashed: 1\n",
      "toy: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_robynishere:\n",
      "There are 137 tokens in the data.\n",
      "There are 94 unique tokens in the data.\n",
      "There are 701 characters in the data.\n",
      "The lexical diversity is 0.686 in the data.\n",
      "The 137 most common tokens are:\n",
      "im: 8\n",
      "robyn: 7\n",
      "make: 4\n",
      "let: 3\n",
      "know: 3\n",
      "hear: 3\n",
      "flow: 3\n",
      "feel: 3\n",
      "cause: 3\n",
      "gotta: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_sayit:\n",
      "There are 52 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 222 characters in the data.\n",
      "The lexical diversity is 0.269 in the data.\n",
      "The 52 most common tokens are:\n",
      "want: 28\n",
      "say: 9\n",
      "woman: 3\n",
      "ready: 2\n",
      "sayit: 1\n",
      "bitches: 1\n",
      "wait: 1\n",
      "stop: 1\n",
      "almost: 1\n",
      "pleasure: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_sendtorobinimmediately:\n",
      "There are 104 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.365 in the data.\n",
      "The 104 most common tokens are:\n",
      "got: 11\n",
      "say: 11\n",
      "baby: 9\n",
      "something: 6\n",
      "tonight: 5\n",
      "mean: 5\n",
      "right: 5\n",
      "need: 4\n",
      "hear: 4\n",
      "away: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_setmefree:\n",
      "There are 138 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 510 characters in the data.\n",
      "The lexical diversity is 0.181 in the data.\n",
      "The 138 most common tokens are:\n",
      "set: 44\n",
      "got: 34\n",
      "free: 18\n",
      "know: 15\n",
      "body: 4\n",
      "hearts: 2\n",
      "desire: 2\n",
      "youve: 2\n",
      "see: 1\n",
      "wildest: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_shouldhaveknown:\n",
      "There are 97 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 480 characters in the data.\n",
      "The lexical diversity is 0.680 in the data.\n",
      "The 97 most common tokens are:\n",
      "known: 9\n",
      "let: 3\n",
      "even: 3\n",
      "know: 3\n",
      "believe: 3\n",
      "like: 3\n",
      "never: 3\n",
      "seen: 2\n",
      "coming: 2\n",
      "fucking: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_shouldhaveknown106828:\n",
      "There are 97 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 480 characters in the data.\n",
      "The lexical diversity is 0.680 in the data.\n",
      "The 97 most common tokens are:\n",
      "known: 9\n",
      "let: 3\n",
      "even: 3\n",
      "know: 3\n",
      "believe: 3\n",
      "like: 3\n",
      "never: 3\n",
      "seen: 2\n",
      "coming: 2\n",
      "fucking: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_showmelove:\n",
      "There are 181 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 815 characters in the data.\n",
      "The lexical diversity is 0.315 in the data.\n",
      "The 181 most common tokens are:\n",
      "show: 34\n",
      "love: 24\n",
      "alright: 10\n",
      "baby: 8\n",
      "one: 7\n",
      "life: 6\n",
      "youre: 6\n",
      "ever: 6\n",
      "needed: 6\n",
      "got: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_stars4ever:\n",
      "There are 85 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 450 characters in the data.\n",
      "The lexical diversity is 0.376 in the data.\n",
      "The 85 most common tokens are:\n",
      "stars: 12\n",
      "forever: 10\n",
      "together: 6\n",
      "4x: 6\n",
      "right: 6\n",
      "next: 6\n",
      "connected: 3\n",
      "night: 2\n",
      "2x: 2\n",
      "shinning: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_stillyourgirl:\n",
      "There are 180 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 829 characters in the data.\n",
      "The lexical diversity is 0.494 in the data.\n",
      "The 180 most common tokens are:\n",
      "still: 12\n",
      "im: 9\n",
      "know: 7\n",
      "let: 6\n",
      "girl: 5\n",
      "make: 5\n",
      "wanna: 5\n",
      "way: 4\n",
      "alright: 4\n",
      "dont: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_tellyoutoday:\n",
      "There are 74 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 338 characters in the data.\n",
      "The lexical diversity is 0.378 in the data.\n",
      "The 74 most common tokens are:\n",
      "tell: 21\n",
      "today: 11\n",
      "want: 8\n",
      "chance: 2\n",
      "dance: 2\n",
      "makes: 2\n",
      "come: 2\n",
      "alive: 2\n",
      "remember: 2\n",
      "look: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_thelasttime:\n",
      "There are 171 tokens in the data.\n",
      "There are 96 unique tokens in the data.\n",
      "There are 779 characters in the data.\n",
      "The lexical diversity is 0.561 in the data.\n",
      "The 171 most common tokens are:\n",
      "back: 8\n",
      "always: 7\n",
      "time: 6\n",
      "love: 6\n",
      "cause: 6\n",
      "want: 6\n",
      "ever: 5\n",
      "dont: 5\n",
      "come: 5\n",
      "im: 5\n",
      "\n",
      "Descriptive statistics for robyn - robyn_timemachine:\n",
      "There are 129 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 624 characters in the data.\n",
      "The lexical diversity is 0.411 in the data.\n",
      "The 129 most common tokens are:\n",
      "back: 14\n",
      "taking: 12\n",
      "time: 6\n",
      "could: 5\n",
      "one: 5\n",
      "machine: 4\n",
      "rewind: 4\n",
      "take: 4\n",
      "go: 4\n",
      "hey: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_tomteverkstan:\n",
      "There are 29 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 127 characters in the data.\n",
      "The lexical diversity is 0.552 in the data.\n",
      "The 29 most common tokens are:\n",
      "uh: 6\n",
      "like: 4\n",
      "dont: 2\n",
      "gimme: 2\n",
      "somethin: 2\n",
      "know: 2\n",
      "yeah: 2\n",
      "tomteverkstan: 1\n",
      "mmmm: 1\n",
      "one: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_underneaththeheart:\n",
      "There are 108 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 521 characters in the data.\n",
      "The lexical diversity is 0.769 in the data.\n",
      "The 108 most common tokens are:\n",
      "see: 4\n",
      "love: 4\n",
      "underneath: 3\n",
      "heart: 3\n",
      "chorus: 3\n",
      "place: 2\n",
      "like: 2\n",
      "yet: 2\n",
      "baby: 2\n",
      "right: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_universalwoman:\n",
      "There are 83 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 435 characters in the data.\n",
      "The lexical diversity is 0.711 in the data.\n",
      "The 83 most common tokens are:\n",
      "love: 6\n",
      "shes: 5\n",
      "universal: 3\n",
      "woman: 3\n",
      "chorus: 3\n",
      "x1: 3\n",
      "verse: 2\n",
      "1: 2\n",
      "place: 2\n",
      "made: 2\n",
      "\n",
      "Descriptive statistics for robyn - robyn_ushouldknowbetter:\n",
      "There are 323 tokens in the data.\n",
      "There are 183 unique tokens in the data.\n",
      "There are 1600 characters in the data.\n",
      "The lexical diversity is 0.567 in the data.\n",
      "The 323 most common tokens are:\n",
      "better: 42\n",
      "know: 33\n",
      "fuck: 15\n",
      "im: 8\n",
      "yyou: 7\n",
      "knew: 7\n",
      "would: 7\n",
      "like: 5\n",
      "even: 4\n",
      "got: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_ushouldknowbetter114529:\n",
      "There are 323 tokens in the data.\n",
      "There are 183 unique tokens in the data.\n",
      "There are 1600 characters in the data.\n",
      "The lexical diversity is 0.567 in the data.\n",
      "The 323 most common tokens are:\n",
      "better: 42\n",
      "know: 33\n",
      "fuck: 15\n",
      "im: 8\n",
      "yyou: 7\n",
      "knew: 7\n",
      "would: 7\n",
      "like: 5\n",
      "even: 4\n",
      "got: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_wedancetothebeat:\n",
      "There are 222 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1129 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "The 222 most common tokens are:\n",
      "dance: 68\n",
      "beat: 67\n",
      "dont: 7\n",
      "stop: 7\n",
      "loud: 3\n",
      "proud: 3\n",
      "another: 2\n",
      "continents: 1\n",
      "shifting: 1\n",
      "feet: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_wedancetothebeat114528:\n",
      "There are 222 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1129 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "The 222 most common tokens are:\n",
      "dance: 68\n",
      "beat: 67\n",
      "dont: 7\n",
      "stop: 7\n",
      "loud: 3\n",
      "proud: 3\n",
      "another: 2\n",
      "continents: 1\n",
      "shifting: 1\n",
      "feet: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_wheredidourlovego:\n",
      "There are 55 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 266 characters in the data.\n",
      "The lexical diversity is 0.764 in the data.\n",
      "The 55 most common tokens are:\n",
      "love: 5\n",
      "thinkin: 3\n",
      "go: 2\n",
      "thoughts: 2\n",
      "used: 2\n",
      "one: 2\n",
      "new: 2\n",
      "baby: 2\n",
      "know: 2\n",
      "strong: 1\n",
      "\n",
      "Descriptive statistics for robyn - robyn_whosthatgirl:\n",
      "There are 170 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 762 characters in the data.\n",
      "The lexical diversity is 0.318 in the data.\n",
      "The 170 most common tokens are:\n",
      "girl: 24\n",
      "whos: 19\n",
      "cant: 10\n",
      "take: 7\n",
      "pressure: 7\n",
      "like: 6\n",
      "im: 6\n",
      "love: 6\n",
      "good: 4\n",
      "girls: 4\n",
      "\n",
      "Descriptive statistics for robyn - robyn_witheveryheartbeat:\n",
      "There are 106 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 537 characters in the data.\n",
      "The lexical diversity is 0.311 in the data.\n",
      "The 106 most common tokens are:\n",
      "every: 12\n",
      "heartbeat: 9\n",
      "hurts: 8\n",
      "could: 6\n",
      "dont: 6\n",
      "look: 6\n",
      "back: 6\n",
      "make: 4\n",
      "keep: 3\n",
      "trying: 3\n",
      "\n",
      "Descriptive statistics for robyn - robyn_youvegotthatsomething:\n",
      "There are 81 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 366 characters in the data.\n",
      "The lexical diversity is 0.691 in the data.\n",
      "The 81 most common tokens are:\n",
      "know: 5\n",
      "youve: 4\n",
      "got: 4\n",
      "ive: 3\n",
      "something: 2\n",
      "look: 2\n",
      "im: 2\n",
      "lovin: 2\n",
      "day: 2\n",
      "love: 2\n"
     ]
    }
   ],
   "source": [
    "# Lyrics data Descriptive Stats\n",
    "for artist, songs in clean_lyrics_data.items():\n",
    "    for song, tokens in songs.items():\n",
    "        print(f\"\\nDescriptive statistics for {artist} - {song}:\")\n",
    "        descriptive_stats(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: The stopwords includes common \"filler\" words such as the, and, to, etc. These words connect statements in lyrics and would most likely be the most common words in each song, which would provide a false sense of what the song actually is. \n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: I would have assumed that the lexical diversity would be on the higher side as each song has it's own meaning, story, and many words to be created by. Certain songs appear to have a relatively high diversity, suggesting that within an entire length of the song, outside of stopwords, there are many unique words. If we were to go beyond the most common tokens, it is likely that many words are only listed once throughout the song.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"‚ù§Ô∏è\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis üòÅ\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 emojis for cher: [('‚ù§', 94506), ('üåà', 66291), ('‚ô•', 48059), ('üè≥', 47174), ('‚ú®', 45846), ('üåä', 31234), ('üíô', 31050), ('üèª', 25195), ('‚úå', 21963), ('üíú', 21571)]\n"
     ]
    }
   ],
   "source": [
    "# Function to get emojis\n",
    "def get_emoji_counts(texts):\n",
    "    emoji_counts = Counter()\n",
    "    for text in texts:\n",
    "        emojis = [char for char in text if emoji.is_emoji(char)]\n",
    "        emoji_counts.update(emojis)\n",
    "    return emoji_counts\n",
    "\n",
    "# Twitter emojis\n",
    "for artist, descriptions in twitter_data.items():\n",
    "    all_texts = ' '.join(descriptions)\n",
    "    emoji_counts = get_emoji_counts([all_texts])\n",
    "    print(f\"Top 10 emojis for {artist}: {emoji_counts.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 hashtags for cher: [('#BLM', 8388), ('#Resist', 5095), ('#BlackLivesMatter', 4414), ('#resist', 3180), ('#FBR', 2834), ('#1', 2580), ('#blacklivesmatter', 2551), ('#TheResistance', 2501), ('#', 2205), ('#Resistance', 1538)]\n"
     ]
    }
   ],
   "source": [
    "# Function to get hashtags\n",
    "def get_hashtag_counts(texts):\n",
    "    hashtag_counts = Counter()\n",
    "    for text in texts:\n",
    "        hashtags = [word for word in text.split() if word.startswith('#')]\n",
    "        hashtag_counts.update(hashtags)\n",
    "    return hashtag_counts\n",
    "\n",
    "# Twitter hashtages\n",
    "for artist, descriptions in twitter_data.items():\n",
    "    all_texts = ' '.join(descriptions)\n",
    "    hashtag_counts = get_hashtag_counts([all_texts])\n",
    "    print(f\"Top 10 hashtags for {artist}: {hashtag_counts.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in song titles for cher:\n",
      "  cher_88degrees: 1\n",
      "  cher_adifferentkindoflovesong: 1\n",
      "  cher_afterall: 1\n",
      "  cher_again: 1\n",
      "  cher_alfie: 1\n",
      "\n",
      "Most common words in song titles for robyn:\n",
      "  robyn_88days: 1\n",
      "  robyn_aintnothing: 1\n",
      "  robyn_anytimeyoulike: 1\n",
      "  robyn_babyforgiveme: 1\n",
      "  robyn_beach2k20: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_titles(titles):\n",
    "    \"\"\"Tokenize and clean song titles\"\"\"\n",
    "    # Combine all titles into one string\n",
    "    combined_titles = ' '.join(titles)\n",
    "    # Use the same whitespace collapse and split function\n",
    "    return tokenize_lyrics(combined_titles)\n",
    "\n",
    "# Function to find the five most common words in song titles by artist\n",
    "def most_common_words_by_artist(clean_lyrics_data):\n",
    "    common_words = {}\n",
    "    for artist, songs in clean_lyrics_data.items():\n",
    "        titles = songs.keys()  # Get song titles\n",
    "        tokens = tokenize_titles(titles)\n",
    "        word_counts = Counter(tokens)\n",
    "        # Get the five most common words\n",
    "        common_words[artist] = word_counts.most_common(5)\n",
    "    return common_words\n",
    "\n",
    "# Get the most common words\n",
    "common_words = most_common_words_by_artist(clean_lyrics_data)\n",
    "\n",
    "# Print the results\n",
    "for artist, words in common_words.items():\n",
    "    print(f\"Most common words in song titles for {artist}:\")\n",
    "    for word, count in words:\n",
    "        print(f\"  {word}: {count}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABAYAAAIhCAYAAADdI7htAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdoElEQVR4nO3deViVdf7/8ddhO7iBismSGy65r5AI5paFS5aWjtQU6pgmo+aCNYZLLpWklZq5ZZlmi9p8zWVm3NCMdERTw2WUzEkUM0jFFNNCgfv3hz/OdGIREDji/Xxc17nyfO73/bnf9+EOPa9zn/u2GIZhCAAAAAAAmJKToxsAAAAAAACOQzAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAADI1/Lly2WxWLR///5cl/fq1Ut16tSxG6tTp44GDRpUqO3s3r1bU6dO1aVLl4rWqAmtXr1aTZs2Vbly5WSxWHTw4ME8axMSEhQeHq66devK3d1d1apVU5s2bTRy5EilpaWVXtOF9OWXX8pisej//u//HN1Krq5du6apU6fqyy+/zLFs6tSpslgsunDhQolt32KxaOTIkSU2/x8dOXJEFotFrq6uSk5OLvT6M2bM0Lp163KMZ/+cc3sd87Nw4UItX7680H0AAOwRDAAAit3atWs1efLkQq2ze/duTZs2jWCggM6fP6/w8HDVq1dPmzdvVlxcnO67775ca+Pj4xUQEKBjx47p5Zdf1ubNm7V48WI98sgj2rJliy5evFjK3d89rl27pmnTphX6DW1Z9f7770uSMjIytGLFikKvn1cw0KZNG8XFxalNmzaFmo9gAACKh4ujGwAA3H1at27t6BYK7caNG7JYLHJxKRt/NX733Xe6ceOGnnnmGXXq1Cnf2rlz58rJyUlffvmlKlWqZBvv16+fXnnlFRmGUdLt4i6Qnp6uTz75RC1bttSFCxf0wQcfaPz48QVa99dff1W5cuXyXO7h4aF27doVV6sAgELijAEAQLH741cJsrKy9Oqrr6phw4YqV66cKleurBYtWujtt9+WdPOU6xdffFGS5O/vL4vFYndacVZWlmbNmqVGjRrJarWqevXqGjBggH744Qe77RqGoRkzZqh27dpyd3dXYGCgYmJi1LlzZ3Xu3NlWl33a8kcffaRx48bp3nvvldVq1X//+1+dP39ew4cPV5MmTVSxYkVVr15dDz74oHbu3Gm3rVOnTsliseiNN97QzJkzVadOHZUrV06dO3e2vWl/6aWX5OfnJ09PTz3++OM6d+5cgV6/DRs2KDg4WOXLl1elSpX08MMPKy4uzrZ80KBBeuCBByRJYWFhslgsdvv3R6mpqfLw8FDFihVzXW6xWOyef/DBB2rZsqXc3d1VtWpVPf7440pISLCrGTRokCpWrKj//ve/6tmzpypWrKiaNWtq3LhxSk9Pt6v94Ycf1K9fP1WqVEmVK1fW008/rX379slisRTbp70pKSkaNmyYatSoITc3N/n7+2vatGnKyMiw1WT/zN58803Nnj1b/v7+qlixooKDg7Vnz54cc7733nu67777ZLVa1aRJE3366acaNGiQ7aszp06d0j333CNJmjZtmu24/ePXaH766Sc99dRT8vT0lLe3twYPHqzLly/b1fz9739XUFCQPD09Vb58edWtW1eDBw8u8P6/++67dr2uWrXKbr9dXFwUHR2dY72vvvpKFotFf//732+5jXXr1ik1NVVDhgzRwIED9d1332nXrl056urUqaNevXrp888/V+vWreXu7m57fa5evaoPP/zQ9lplH7e5fZXg5MmTevLJJ+Xn5yer1Spvb2917drV9pWZOnXq6OjRo4qNjbXN98evNQEACqZsfCwCAHC4zMxMuzdZ2QryafOsWbM0depUTZo0SR07dtSNGzf07bff2r42MGTIEF28eFHvvPOOPv/8c/n6+kqSmjRpIkn661//qiVLlmjkyJHq1auXTp06pcmTJ+vLL7/UN998o2rVqkmSJk6cqOjoaD333HN64okndObMGQ0ZMkQ3btzI9TT7qKgoBQcHa/HixXJyclL16tV1/vx5SdKUKVPk4+OjX375RWvXrlXnzp21ffv2HG/AFyxYoBYtWmjBggW6dOmSxo0bp0cffVRBQUFydXXVBx98oNOnT+uFF17QkCFDtGHDhnxfq08//VRPP/20QkNDtXLlSqWnp2vWrFm27T/wwAOaPHmy2rZtqxEjRmjGjBnq0qWLPDw88pwzODhY//rXv/T0009r2LBhatu2bZ6f3kZHR2vChAl66qmnFB0drdTUVE2dOlXBwcHat2+fGjRoYKu9ceOGHnvsMT377LMaN26cvvrqK73yyivy9PTUyy+/LEm6evWqunTpoosXL2rmzJmqX7++Nm/erLCwsHxfh8JISUlR27Zt5eTkpJdffln16tVTXFycXn31VZ06dUrLli2zq1+wYIEaNWqkuXPnSpImT56snj17KjExUZ6enpKkJUuWaNiwYerbt6/mzJmjy5cva9q0aXahh6+vrzZv3qzu3bvr2Wef1ZAhQyTJFhZk69u3r8LCwvTss8/qyJEjioqKknQzgJGkuLg4hYWFKSwsTFOnTpW7u7tOnz6tL774okD7v2HDBu3YsUPTp09XhQoVtHDhQj311FNycXFRv379VKdOHT322GNavHix/va3v8nZ2dm27vz58+Xn56fHH3/8lttZunSprFarnn76aV28eFHR0dFaunSpLaT6vW+++UYJCQmaNGmS/P39VaFCBfXp00cPPvigunTpYvuqUX7Hbc+ePZWZmalZs2apVq1aunDhgnbv3m37vbF27Vr169dPnp6eWrhwoSTJarUW6DUDAPyBAQBAPpYtW2ZIyvdRu3Ztu3Vq165tDBw40Pa8V69eRqtWrfLdzhtvvGFIMhITE+3GExISDEnG8OHD7cb37t1rSDImTJhgGIZhXLx40bBarUZYWJhdXVxcnCHJ6NSpk21sx44dhiSjY8eOt9z/jIwM48aNG0bXrl2Nxx9/3DaemJhoSDJatmxpZGZm2sbnzp1rSDIee+wxu3nGjBljSDIuX76c57YyMzMNPz8/o3nz5nZzXrlyxahevboREhKSYx/+/ve/33IffvvtN6NPnz62n5ezs7PRunVrY+LEica5c+dsdT///LNRrlw5o2fPnnbrJyUlGVar1fjzn/9sGxs4cKAhyfjss8/sanv27Gk0bNjQ9nzBggWGJGPTpk12dcOGDTMkGcuWLcu394Ls57Bhw4yKFSsap0+ftht/8803DUnG0aNHDcP438+sefPmRkZGhq3u66+/NiQZK1euNAzj5s/Bx8fHCAoKspvv9OnThqurq93xfv78eUOSMWXKlBx9TZkyxZBkzJo1y258+PDhhru7u5GVlWXX56VLl/J9LXIjyShXrpyRkpJiG8vIyDAaNWpk1K9f3zaW/TquXbvWNnb27FnDxcXFmDZt2i23c+rUKcPJycl48sknbWOdOnUyKlSoYKSlpdnV1q5d23B2djaOHz+eY54KFSrY/W74Y387duwwDMMwLly4YEgy5s6dm29fTZs2tft/GwBQNHyVAABQICtWrNC+fftyPHL7tPCP2rZtq0OHDmn48OHasmVLoa6Cv2PHDknKcXp227Zt1bhxY23fvl2StGfPHqWnp6t///52de3atcvz9OK+ffvmOr548WK1adNG7u7ucnFxkaurq7Zv357jdHrp5qeaTk7/++u0cePGkqRHHnnEri57PCkpKY89lY4fP64ff/xR4eHhdnNWrFhRffv21Z49e3Tt2rU818+L1WrV2rVrdezYMc2ZM0dPPvmkzp8/r9dee02NGzfW8ePHJd385PrXX3/N8VrXrFlTDz74oO21zmaxWPToo4/ajbVo0UKnT5+2PY+NjVWlSpXUvXt3u7qnnnqq0PuRl3/+85/q0qWL/Pz8lJGRYXv06NHD1sPvPfLII3afmrdo0UKSbH0fP35cKSkpOY6lWrVqqX379oXu77HHHrN73qJFC/3222+2r5bcf//9kqT+/fvrs88+09mzZws1f9euXeXt7W177uzsrLCwMP33v/+1fd2mc+fOatmypRYsWGCrW7x4sSwWi5577rlbbmPZsmXKysqy+3rD4MGDdfXqVa1evTpHfYsWLfK8GGZBVK1aVfXq1dMbb7yh2bNnKz4+XllZWUWeDwCQP4IBAECBNG7cWIGBgTke2ade5ycqKkpvvvmm9uzZox49esjLy0tdu3bN8xaIv5eamipJtq8X/J6fn59tefZ/f/8GKVtuY3nNOXv2bP31r39VUFCQ1qxZoz179mjfvn3q3r27fv311xz1VatWtXvu5uaW7/hvv/2Way+/34e89jUrK0s///xznuvfSuPGjTVmzBh9/PHHSkpK0uzZs5Wammo7rbugr3W28uXLy93d3W7MarXa7WNqamqhfiZF8dNPP+kf//iHXF1d7R5NmzaVpBy3C/Ty8srRsyTbz7cox1J+brW9jh07at26dcrIyNCAAQNUo0YNNWvWTCtXrizQ/D4+PnmO/f5nNmrUKG3fvl3Hjx/XjRs39N5776lfv365rv97WVlZWr58ufz8/BQQEKBLly7p0qVLeuihh1ShQgUtXbo0xzq5HUOFYbFYtH37dnXr1k2zZs1SmzZtdM8992jUqFG6cuXKbc0NAMiJawwAAEqci4uLIiMjFRkZqUuXLmnbtm2aMGGCunXrpjNnzqh8+fJ5rpv9pio5OVk1atSwW/bjjz/ari+QXffTTz/lmCMlJSXXswb+eNE9Sfr444/VuXNnLVq0yG68NN6M/H5f/+jHH3+Uk5OTqlSpUizbslgsGjt2rKZPn67//Oc/Bdp+9mtdGF5eXvr6669zjKekpBR6rrxUq1ZNLVq00GuvvZbrcj8/v0LNd6tjqST07t1bvXv3Vnp6uvbs2aPo6Gj9+c9/Vp06dRQcHJzvurn1lD32+1Diz3/+s8aPH68FCxaoXbt2SklJ0YgRI27Z27Zt22xnU/wx5JBunq1z7Ngx2zVBpNz/3yqs2rVr20KH7777Tp999pmmTp2q69eva/Hixbc9PwDgfzhjAABQqipXrqx+/fppxIgRunjxok6dOiUp56eo2R588EFJN9+w/96+ffuUkJCgrl27SpKCgoJktVpznNa8Z88eu1Pbb8ViseS4gNnhw4ft7gpQUho2bKh7771Xn376qd1FHa9evao1a9bY7lRQWLm90ZduvtlPS0uzvXEODg5WuXLlcrzWP/zwg7744gvba10YnTp10pUrV7Rp0ya78d9fNf929erVS//5z39Ur169XM9qKWww0LBhQ/n4+Oizzz6zG09KStLu3bvtxvI6bovKarWqU6dOmjlzpiQpPj7+luts377dLsTIzMzU6tWrVa9ePbswzd3dXc8995w+/PBDzZ49W61atSrQVyOWLl0qJycnrVu3Tjt27LB7fPTRR5L+dyHFguxfUV6r++67T5MmTVLz5s31zTff3PZ8AAB7nDEAAChxjz76qJo1a6bAwEDdc889On36tObOnavatWvbrnLfvHlzSdLbb7+tgQMHytXVVQ0bNlTDhg313HPP6Z133pGTk5N69OhhuytBzZo1NXbsWEk3T92PjIxUdHS0qlSposcff1w//PCDpk2bJl9fX7vv7OenV69eeuWVVzRlyhR16tRJx48f1/Tp0+Xv75/rXRmKk5OTk2bNmqWnn35avXr10rBhw5Senq433nhDly5d0uuvv16keZ977jldunRJffv2VbNmzeTs7Kxvv/1Wc+bMkZOTk+1e9JUrV9bkyZM1YcIEDRgwQE899ZRSU1M1bdo0ubu7a8qUKYXe9sCBAzVnzhw988wzevXVV1W/fn1t2rRJW7Zsse1zQeR2O0HpZvAwffp0xcTEKCQkRKNGjVLDhg3122+/6dSpU9q4caMWL16c42yT/Dg5OWnatGkaNmyY+vXrp8GDB+vSpUu5HkuVKlVS7dq1tX79enXt2lVVq1ZVtWrVCnXbvJdfflk//PCDunbtqho1aujSpUt6++235erqqk6dOt1y/WrVqunBBx/U5MmTbXcl+Pbbb3MNX4YPH65Zs2bpwIEDev/99285d2pqqtavX69u3bqpd+/eudbMmTNHK1asUHR0tFxdXfOdr3nz5vryyy/1j3/8Q76+vqpUqZIaNmyYo+7w4cMaOXKk/vSnP6lBgwZyc3PTF198ocOHD+ull16ym2/VqlVavXq16tatK3d3d9vvEgBAITj66ocAgDtb9l0J9u3bl+vyRx555JZ3JXjrrbeMkJAQo1q1aoabm5tRq1Yt49lnnzVOnTplt15UVJTh5+dnODk52V2hPDMz05g5c6Zx3333Ga6urka1atWMZ555xjhz5ozd+llZWcarr75q1KhRw3BzczNatGhh/POf/zRatmxpd0eB/K50n56ebrzwwgvGvffea7i7uxtt2rQx1q1bZwwcONBuP7OvcP/GG2/YrZ/X3Ld6HX9v3bp1RlBQkOHu7m5UqFDB6Nq1q/Hvf/+7QNvJzZYtW4zBgwcbTZo0MTw9PQ0XFxfD19fXeOKJJ4y4uLgc9e+//77RokULw83NzfD09DR69+5tu7J/toEDBxoVKlTIsW72lfh/LykpyXjiiSeMihUrGpUqVTL69u1rbNy40ZBkrF+/Pt/es/czr0f2MXL+/Hlj1KhRhr+/v+Hq6mpUrVrVCAgIMCZOnGj88ssvhmHk/TMzDCPXOwssWbLEqF+/vuHm5mbcd999xgcffGD07t3baN26tV3dtm3bjNatWxtWq9WQZDv2s1+L8+fP29VnHwvZd+D45z//afTo0cO49957DTc3N6N69epGz549jZ07d+b72mT3PWLECGPhwoVGvXr1DFdXV6NRo0bGJ598kuc6nTt3NqpWrWpcu3btlvNn32Vj3bp1edYsXrzYkGSsWbPGMIyb//8/8sgjudYePHjQaN++vVG+fHm7u4X88a4EP/30kzFo0CCjUaNGRoUKFYyKFSsaLVq0MObMmWN3R4lTp04ZoaGhRqVKlXK9QwoAoGAshlGAG1ADAFBGJSYmqlGjRpoyZYomTJjg6Hbw/82YMUOTJk1SUlJSoT7Nd6RLly7pvvvuU58+fbRkyRJHt1Mk586dU+3atfX8889r1qxZjm4HAHCH4KsEAIC7xqFDh7Ry5UqFhITIw8NDx48f16xZs+Th4aFnn33W0e2Z1vz58yVJjRo10o0bN/TFF19o3rx5euaZZ+7YUCAlJUWvvfaaunTpIi8vL50+fVpz5szRlStXNHr0aEe3V2g//PCDTp48qTfeeENOTk5lch8AACWHYAAAcNeoUKGC9u/fr6VLl+rSpUvy9PRU586d9dprrxXr7fFQOOXLl9ecOXN06tQppaenq1atWho/frwmTZrk6NbyZLVaderUKQ0fPlwXL15U+fLl1a5dOy1evNh2G8Sy5P3339f06dNVp04dffLJJ7r33nsd3RIA4A7CVwkAAAAAADAxblcIAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYFx8soqysLP3444+qVKmSLBaLo9sBAAAAANzlDMPQlStX5OfnJyen4vucn2CgiH788UfVrFnT0W0AAAAAAEzmzJkzxXrLX4KBIqpUqZKkmz8QDw8PB3cDAAAAALjbpaWlqWbNmrb3o8WFYKCIsr8+4OHhQTAAAAAAACg1xf11di4+CAAAAACAiREMAAAAAABgYgQDAAAAAACYGNcYAAAAAAAUmGEYysjIUGZmpqNbues4OzvLxcWl2K8hcCsEAwAAAACAArl+/bqSk5N17do1R7dy1ypfvrx8fX3l5uZWatskGAAAAAAA3FJWVpYSExPl7OwsPz8/ubm5lfon23czwzB0/fp1nT9/XomJiWrQoIGcnErn2/8EAwAAAACAW7p+/bqysrJUs2ZNlS9f3tHt3JXKlSsnV1dXnT59WtevX5e7u3upbJeLDwIAAAAACqy0PsU2K0e8vvxEAQAAAAAwMYIBAAAAAABMjGsMAAAAAABuy5yY70p1e2Mfvq/Y5jp16pT8/f0VHx+vVq1aFdu8ZQlnDAAAAAAAYGIEAwAAAAAAFLPr1687uoUCIxgAAAAAANz1srKyNHPmTNWvX19Wq1W1atXSa6+9Zlt+8uRJdenSReXLl1fLli0VFxdnt/7u3bvVsWNHlStXTjVr1tSoUaN09epV2/I6dero1Vdf1aBBg+Tp6amhQ4eW2r7dLoIBAAAAAMBdLyoqSjNnztTkyZN17Ngxffrpp/L29rYtnzhxol544QUdPHhQ9913n5566illZGRIko4cOaJu3brpiSee0OHDh7V69Wrt2rVLI0eOtNvGG2+8oWbNmunAgQOaPHlyqe7f7bAYhmE4uomyKC0tTZ6enrp8+bI8PDwc3Q4AAAAAlKjffvtNiYmJ8vf3l7u7u92yO/3ig1euXNE999yj+fPna8iQIXbLsi8++P777+vZZ5+VJB07dkxNmzZVQkKCGjVqpAEDBqhcuXJ69913bevt2rVLnTp10tWrV+Xu7q46deqodevWWrt27W3tW36vc0m9D+WMAQAAAADAXS0hIUHp6enq2rVrnjUtWrSw/dnX11eSdO7cOUnSgQMHtHz5clWsWNH26Natm7KyspSYmGhbLzAwsIT2oGRxu0IAAAAAwF2tXLlyt6xxdXW1/dlisUi6eV2C7P8OGzZMo0aNyrFerVq1bH+uUKHC7bbqEAQDAAAAAIC7WoMGDVSuXDlt3749x1cJCqJNmzY6evSo6tevXwLdOR7BAADcAUr7e3m5Kex39QAAAMoKd3d3jR8/Xn/729/k5uam9u3b6/z58zp69Gi+Xy/INn78eLVr104jRozQ0KFDVaFCBSUkJCgmJkbvvPNOKexBySIYAAAAAADclrLwAcPkyZPl4uKil19+WT/++KN8fX0VERFRoHVbtGih2NhYTZw4UR06dJBhGKpXr57CwsJKuOvSQTAAAAAAALjrOTk5aeLEiZo4cWKOZX+8WV/lypVzjN1///3aunVrnvOfOnWqWPp0BIfflWDhwoW22zAEBARo586d+dbHxsYqICBA7u7uqlu3rhYvXpyjZs2aNWrSpImsVquaNGmS6+0izp49q2eeeUZeXl4qX768WrVqpQMHDhTbfgEAAAAAUBY4NBhYvXq1xowZo4kTJyo+Pl4dOnRQjx49lJSUlGt9YmKievbsqQ4dOig+Pl4TJkzQqFGjtGbNGltNXFycwsLCFB4erkOHDik8PFz9+/fX3r17bTU///yz2rdvL1dXV23atEnHjh3TW2+9pcqVK5f0LgMAAAAAcEexGH88P6IUBQUFqU2bNlq0aJFtrHHjxurTp4+io6Nz1I8fP14bNmxQQkKCbSwiIkKHDh1SXFycJCksLExpaWnatGmTraZ79+6qUqWKVq5cKUl66aWX9O9///uWZyf8Xnp6utLT023P09LSVLNmTV2+fFkeHh4F32kAyAUXHwQAAHe63377TYmJibYzvlEy8nud09LS5OnpWezvQx12xsD169d14MABhYaG2o2HhoZq9+7dua4TFxeXo75bt27av3+/bty4kW/N7+fcsGGDAgMD9ac//UnVq1dX69at9d577+Xbb3R0tDw9PW2PmjVrFnhfAQAAAAC4UzksGLhw4YIyMzPl7e1tN+7t7a2UlJRc10lJScm1PiMjQxcuXMi35vdznjx5UosWLVKDBg20ZcsWRUREaNSoUVqxYkWe/UZFReny5cu2x5kzZwq1vwAAAAAA3IkcflcCi8Vi99wwjBxjt6r/4/it5szKylJgYKBmzJghSWrdurWOHj2qRYsWacCAAblu12q1ymq1FmCPAAAAAAAoOxx2xkC1atXk7Oyc4+yAc+fO5fjEP5uPj0+u9S4uLvLy8sq35vdz+vr6qkmTJnY1jRs3zvOihwAAAAAA3K0cFgy4ubkpICBAMTExduMxMTEKCQnJdZ3g4OAc9Vu3blVgYKBcXV3zrfn9nO3bt9fx48ftar777jvVrl27yPsDAAAAAEBZ5NCvEkRGRio8PFyBgYEKDg7WkiVLlJSUpIiICEk3v9d/9uxZ23f/IyIiNH/+fEVGRmro0KGKi4vT0qVLbXcbkKTRo0erY8eOmjlzpnr37q3169dr27Zt2rVrl61m7NixCgkJ0YwZM9S/f399/fXXWrJkiZYsWVK6LwAAAAAAAA7m0GAgLCxMqampmj59upKTk9WsWTNt3LjR9sl9cnKy3en9/v7+2rhxo8aOHasFCxbIz89P8+bNU9++fW01ISEhWrVqlSZNmqTJkyerXr16Wr16tYKCgmw1999/v9auXauoqChNnz5d/v7+mjt3rp5++unS23kAAAAAuFvsyHm7+RLVJarEN9G5c2e1atVKc+fOLfFtOZrDLz44fPhwDR8+PNdly5cvzzHWqVMnffPNN/nO2a9fP/Xr1y/fml69eqlXr14F7hMAAAAAgLuRw64xAAAAAACAI1y/ft3RLdxRCAYAAAAAAHe1zp07a+TIkYqMjFS1atX08MMPKzY2Vm3btpXVapWvr69eeuklZWRk2K2XkZGhkSNHqnLlyvLy8tKkSZNkGIYkafr06WrevHmObQUEBOjll1+WJA0aNEh9+vTRm2++KV9fX3l5eWnEiBG6ceNGye90IRAMAAAAAADueh9++KFcXFz073//WzNmzFDPnj11//3369ChQ1q0aJGWLl2qV199Ndd19u7dq3nz5mnOnDl6//33JUmDBw/WsWPHtG/fPlv94cOHFR8fr0GDBtnGduzYoe+//147duzQhx9+qOXLl+f6tXlHcvg1BgAAAAAAKGn169fXrFmzJEkrVqxQzZo1NX/+fFksFjVq1Eg//vijxo8fr5dffllOTjc/Q69Zs6bmzJkji8Wihg0b6siRI5ozZ46GDh2qGjVqqFu3blq2bJnuv/9+SdKyZcvUqVMn1a1b17bdKlWqaP78+XJ2dlajRo30yCOPaPv27Ro6dGjpvwh54IwBAAAAAMBdLzAw0PbnhIQEBQcHy2Kx2Mbat2+vX375RT/88INtrF27dnY1wcHBOnHihDIzMyVJQ4cO1cqVK/Xbb7/pxo0b+uSTTzR48GC77TZt2lTOzs62576+vjp37lyx79/t4IwBAAAAAMBdr0KFCrY/G4Zh94Y/e0xSjvH8PProo7JarVq7dq2sVqvS09PVt29fuxpXV1e75xaLRVlZWYVtv0QRDAAAAAAATKVJkyZas2aNXUCwe/duVapUSffee6+tbs+ePXbr7dmzRw0aNLCdAeDi4qKBAwdq2bJlslqtevLJJ1W+fPnS25FiwlcJAAAAAACmMnz4cJ05c0bPP/+8vv32W61fv15TpkxRZGSk7foCknTmzBlFRkbq+PHjWrlypd555x2NHj3abq4hQ4boiy++0KZNm3J8jaCs4IwBAAAAAMDt6RLl6A4K5d5779XGjRv14osvqmXLlqpataqeffZZTZo0ya5uwIAB+vXXX9W2bVs5Ozvr+eef13PPPWdX06BBA4WEhCg1NVVBQUGluRvFhmAAAAAAAHBX+/LLL3OMderUSV9//XWB1lm0aFGedYZh6KefftKwYcNyLMvttoRz587Nr1WHIBgAAAAAAKAIzp07p48++khnz57VX/7yF0e3U2QEAwAAAAAAFIG3t7eqVaumJUuWqEqVKo5up8gIBgAAAAAAKILsWxyWdQQDAExvTsx3jm4BAAAAcBhuVwgAAAAAKLC75VPyO5UjXl+CAQAAAADALbm6ukqSrl275uBO7m7Zr2/2610a+CoBAAAAAOCWnJ2dVblyZZ07d06SVL58eVksFgd3dfcwDEPXrl3TuXPnVLlyZTk7O5fatgkGAAAAAAAF4uPjI0m2cADFr3LlyrbXubQQDAAAAAAACsRiscjX11fVq1fXjRs3HN3OXcfV1bVUzxTIRjAAAAAAACgUZ2dnh7yBRcng4oMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmJjDg4GFCxfK399f7u7uCggI0M6dO/Otj42NVUBAgNzd3VW3bl0tXrw4R82aNWvUpEkTWa1WNWnSRGvXrrVbPnXqVFksFruHj49Pse4XAAAAAABlgUODgdWrV2vMmDGaOHGi4uPj1aFDB/Xo0UNJSUm51icmJqpnz57q0KGD4uPjNWHCBI0aNUpr1qyx1cTFxSksLEzh4eE6dOiQwsPD1b9/f+3du9durqZNmyo5Odn2OHLkSInuKwAAAAAAdyKLYRiGozYeFBSkNm3aaNGiRbaxxo0bq0+fPoqOjs5RP378eG3YsEEJCQm2sYiICB06dEhxcXGSpLCwMKWlpWnTpk22mu7du6tKlSpauXKlpJtnDKxbt04HDx4scu9paWny9PTU5cuX5eHhUeR5ADjenJjvHN3CHWHsw/c5ugUAAADko6TehzrsjIHr16/rwIEDCg0NtRsPDQ3V7t27c10nLi4uR323bt20f/9+3bhxI9+aP8554sQJ+fn5yd/fX08++aROnjyZb7/p6elKS0uzewAAAAAAUNY5LBi4cOGCMjMz5e3tbTfu7e2tlJSUXNdJSUnJtT4jI0MXLlzIt+b3cwYFBWnFihXasmWL3nvvPaWkpCgkJESpqal59hsdHS1PT0/bo2bNmoXaXwAAAAAA7kQOv/igxWKxe24YRo6xW9X/cfxWc/bo0UN9+/ZV8+bN9dBDD+lf//qXJOnDDz/Mc7tRUVG6fPmy7XHmzJlb7BkAAAAAAHc+F0dtuFq1anJ2ds5xdsC5c+dyfOKfzcfHJ9d6FxcXeXl55VuT15ySVKFCBTVv3lwnTpzIs8Zqtcpqtea7TwAAAAAAlDUOO2PAzc1NAQEBiomJsRuPiYlRSEhIrusEBwfnqN+6dasCAwPl6uqab01ec0o3rx+QkJAgX1/fouwKAAAAAABllkO/ShAZGan3339fH3zwgRISEjR27FglJSUpIiJC0s3T9wcMGGCrj4iI0OnTpxUZGamEhAR98MEHWrp0qV544QVbzejRo7V161bNnDlT3377rWbOnKlt27ZpzJgxtpoXXnhBsbGxSkxM1N69e9WvXz+lpaVp4MCBpbbvAAAAAADcCRz2VQLp5q0FU1NTNX36dCUnJ6tZs2bauHGjateuLUlKTk5WUlKSrd7f318bN27U2LFjtWDBAvn5+WnevHnq27evrSYkJESrVq3SpEmTNHnyZNWrV0+rV69WUFCQreaHH37QU089pQsXLuiee+5Ru3bttGfPHtt2AQAAAAAwC4uRffU+FEpJ3T8SQOmbE/Odo1u4I4x9+D5HtwAAAIB8lNT7UIfflQAAAAAAADgOwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiLo5uAACA35sT852jW9DYh+9zdAsAAAClhjMGAAAAAAAwMYIBAAAAAABMjGAAAAAAAAATIxgAAAAAAMDECAYAAAAAADAxggEAAAAAAEyMYAAAAAAAABMjGAAAAAAAwMQIBgAAAAAAMDGCAQAAAAAATIxgAAAAAAAAEyMYAAAAAADAxAgGAAAAAAAwMYIBAAAAAABMjGAAAAAAAAATIxgAAAAAAMDECAYAAAAAADAxggEAAAAAAEyMYAAAAAAAABNzcXQDAIDi1y5pSeFX2uFV/I3cSpeo0t8mAAAA7HDGAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYmMODgYULF8rf31/u7u4KCAjQzp07862PjY1VQECA3N3dVbduXS1evDhHzZo1a9SkSRNZrVY1adJEa9euzXO+6OhoWSwWjRkz5nZ3BQAAAACAMsehwcDq1as1ZswYTZw4UfHx8erQoYN69OihpKSkXOsTExPVs2dPdejQQfHx8ZowYYJGjRqlNWvW2Gri4uIUFham8PBwHTp0SOHh4erfv7/27t2bY759+/ZpyZIlatGiRYntIwAAAAAAdzKHBgOzZ8/Ws88+qyFDhqhx48aaO3euatasqUWLFuVav3jxYtWqVUtz585V48aNNWTIEA0ePFhvvvmmrWbu3Ll6+OGHFRUVpUaNGikqKkpdu3bV3Llz7eb65Zdf9PTTT+u9995TlSpVSnI3AQAAAAC4YzksGLh+/boOHDig0NBQu/HQ0FDt3r0713Xi4uJy1Hfr1k379+/XjRs38q3545wjRozQI488ooceeqhA/aanpystLc3uAQAAAABAWeewYODChQvKzMyUt7e33bi3t7dSUlJyXSclJSXX+oyMDF24cCHfmt/PuWrVKn3zzTeKjo4ucL/R0dHy9PS0PWrWrFngdQEAAAAAuFM5/OKDFovF7rlhGDnGblX/x/H85jxz5oxGjx6tjz/+WO7u7gXuMyoqSpcvX7Y9zpw5U+B1AQAAAAC4U7k4asPVqlWTs7NzjrMDzp07l+MT/2w+Pj651ru4uMjLyyvfmuw5Dxw4oHPnzikgIMC2PDMzU1999ZXmz5+v9PR0OTs759i21WqV1Wot/I4CAAAAAHAHc9gZA25ubgoICFBMTIzdeExMjEJCQnJdJzg4OEf91q1bFRgYKFdX13xrsufs2rWrjhw5ooMHD9oegYGBevrpp3Xw4MFcQwEAAAAAAO5WDjtjQJIiIyMVHh6uwMBABQcHa8mSJUpKSlJERISkm6fvnz17VitWrJAkRUREaP78+YqMjNTQoUMVFxenpUuXauXKlbY5R48erY4dO2rmzJnq3bu31q9fr23btmnXrl2SpEqVKqlZs2Z2fVSoUEFeXl45xgEAAAAAuNs5NBgICwtTamqqpk+fruTkZDVr1kwbN25U7dq1JUnJyclKSkqy1fv7+2vjxo0aO3asFixYID8/P82bN099+/a11YSEhGjVqlWaNGmSJk+erHr16mn16tUKCgoq9f0DAAAAAOBOZzGyr96HQklLS5Onp6cuX74sDw8PR7cD4DbMifnO0S0Uu3ZJSwq9TnBdrxLo5Ba6ROUYuhN+HmMfvs/RLQAAAORQUu9DHX5XAgAAAAAA4DgEAwAAAAAAmBjBAAAAAAAAJubQiw8CAO4ccSdTS32bezIcfz0BAAAAs+OMAQAAAAAATIxgAAAAAAAAEyMYAAAAAADAxAgGAAAAAAAwMYIBAAAAAABMjGAAAAAAAAATIxgAAAAAAMDECAYAAAAAADAxF0c3AMDc5sR85+gWAAAAAFPjjAEAAAAAAEyMYAAAAAAAABMjGAAAAAAAwMQIBgAAAAAAMDGCAQAAAAAATIxgAAAAAAAAEyMYAAAAAADAxAgGAAAAAAAwMYIBAAAAAABMjGAAAAAAAAATIxgAAAAAAMDECAYAAAAAADAxF0c3AABAmbAj2tEdFEyXKEd3AAAAyhjOGAAAAAAAwMQIBgAAAAAAMLEiBQOJiYnF3QcAAAAAAHCAIgUD9evXV5cuXfTxxx/rt99+K+6eAAAAAABAKSlSMHDo0CG1bt1a48aNk4+Pj4YNG6avv/66uHsDAAAAAAAlrEjBQLNmzTR79mydPXtWy5YtU0pKih544AE1bdpUs2fP1vnz54u7TwAAAAAAUAJu6+KDLi4uevzxx/XZZ59p5syZ+v777/XCCy+oRo0aGjBggJKTk4urTwAAAAAAUAJuKxjYv3+/hg8fLl9fX82ePVsvvPCCvv/+e33xxRc6e/asevfuXVx9AgAAAACAEuBSlJVmz56tZcuW6fjx4+rZs6dWrFihnj17ysnpZs7g7++vd999V40aNSrWZgEAAAAAQPEqUjCwaNEiDR48WH/5y1/k4+OTa02tWrW0dOnS22oOAAAAAACUrCIFAzExMapVq5btDIFshmHozJkzqlWrltzc3DRw4MBiaRIAAAAAAJSMIl1joF69erpw4UKO8YsXL8rf3/+2mwIAAAAAAKWjSMGAYRi5jv/yyy9yd3e/rYYAAAAAAEDpKdRXCSIjIyVJFotFL7/8ssqXL29blpmZqb1796pVq1bF2iCAkjMn5jtHtwAAAADAwQoVDMTHx0u6ecbAkSNH5ObmZlvm5uamli1b6oUXXijeDgEAAAAAQIkpVDCwY8cOSdJf/vIXvf322/Lw8CiRpgAAAAAAQOko0l0Jli1bVtx9AAAAAAAAByhwMPDEE09o+fLl8vDw0BNPPJFv7eeff37bjQEAAAAAgJJX4GDA09NTFovF9mcAAAAAAFD2FTgY+P3XB/gqAQAAAAAAdwenoqz066+/6tq1a7bnp0+f1ty5c7V169ZiawwAAAAAAJS8IgUDvXv31ooVKyRJly5dUtu2bfXWW2+pd+/eWrRoUbE2CAAAAAAASk6RgoFvvvlGHTp0kCT93//9n3x8fHT69GmtWLFC8+bNK9YGAQAAAABAySlSMHDt2jVVqlRJkrR161Y98cQTcnJyUrt27XT69OlibRAAAAAAAJScIgUD9evX17p163TmzBlt2bJFoaGhkqRz587Jw8OjWBsEAAAAAAAlp0jBwMsvv6wXXnhBderUUVBQkIKDgyXdPHugdevWxdogAAAAAAAoOQW+XeHv9evXTw888ICSk5PVsmVL23jXrl31+OOPF1tzAAAAAACgZBUpGJAkHx8f+fj42I21bdv2thsCAAAAAAClp0jBwNWrV/X6669r+/btOnfunLKysuyWnzx5sliaAwAAAAAAJatIwcCQIUMUGxur8PBw+fr6ymKxFHdfAAAAAACgFBQpGNi0aZP+9a9/qX379sXdDwAAAAAAKEVFuitBlSpVVLVq1eLuBQAAAAAAlLIiBQOvvPKKXn75ZV27dq24+wEAAAAAAKWoSF8leOutt/T999/L29tbderUkaurq93yb775pliaAwAAAAAAJatIwUCfPn2KuQ0AAAAAAOAIRQoGpkyZUmwNLFy4UG+88YaSk5PVtGlTzZ07Vx06dMizPjY2VpGRkTp69Kj8/Pz0t7/9TREREXY1a9as0eTJk/X999+rXr16eu211/T444/bli9atEiLFi3SqVOnJElNmzbVyy+/rB49ehTbfgEAAAAAUBYU6RoDknTp0iW9//77ioqK0sWLFyXd/ArB2bNnCzzH6tWrNWbMGE2cOFHx8fHq0KGDevTooaSkpFzrExMT1bNnT3Xo0EHx8fGaMGGCRo0apTVr1thq4uLiFBYWpvDwcB06dEjh4eHq37+/9u7da6upUaOGXn/9de3fv1/79+/Xgw8+qN69e+vo0aNFfDUAAAAAACibLIZhGIVd6fDhw3rooYfk6empU6dO6fjx46pbt64mT56s06dPa8WKFQWaJygoSG3atNGiRYtsY40bN1afPn0UHR2do378+PHasGGDEhISbGMRERE6dOiQ4uLiJElhYWFKS0vTpk2bbDXdu3dXlSpVtHLlyjx7qVq1qt544w09++yzBeo9LS1Nnp6eunz5sjw8PAq0DnCnmRPznaNbQAlpl7TE0S0UyJ5azzm6hVyNffi+nIM7cv69dEfqEuXoDgAAQAkpqfehRTpjIDIyUoMGDdKJEyfk7u5uG+/Ro4e++uqrAs1x/fp1HThwQKGhoXbjoaGh2r17d67rxMXF5ajv1q2b9u/frxs3buRbk9ecmZmZWrVqla5evarg4OA8+01PT1daWprdAwAAAACAsq5IwcC+ffs0bNiwHOP33nuvUlJSCjTHhQsXlJmZKW9vb7txb2/vPOdISUnJtT4jI0MXLlzIt+aPcx45ckQVK1aU1WpVRESE1q5dqyZNmuTZb3R0tDw9PW2PmjVrFmg/AQAAAAC4kxXp4oPu7u65fmJ+/Phx3XPPPYWay2Kx2D03DCPH2K3q/zhekDkbNmyogwcP6tKlS1qzZo0GDhyo2NjYPMOBqKgoRUZG2p6npaURDgDAXSq3r9m0S0ot1R6C63qV6vYAAIB5FSkY6N27t6ZPn67PPvtM0s034klJSXrppZfUt2/fAs1RrVo1OTs75/gk/9y5czk+8c/m4+OTa72Li4u8vLzyrfnjnG5ubqpfv74kKTAwUPv27dPbb7+td999N9dtW61WWa3WAu0bAAC4Ba7ZAADAHaNIXyV48803df78eVWvXl2//vqrOnXqpPr166tSpUp67bXXCjSHm5ubAgICFBMTYzceExOjkJCQXNcJDg7OUb9161YFBgbK1dU135q85sxmGIbS09ML1DsAAAAAAHeLIp0x4OHhoV27dmnHjh06cOCAsrKy1KZNGz300EOFmicyMlLh4eEKDAxUcHCwlixZoqSkJEVEREi6efr+2bNnbXc5iIiI0Pz58xUZGamhQ4cqLi5OS5cutbvbwOjRo9WxY0fNnDlTvXv31vr167Vt2zbt2rXLVjNhwgT16NFDNWvW1JUrV7Rq1Sp9+eWX2rx5c1FeDgAAAAAAyqxCBwNZWVlavny5Pv/8c506dUoWi0X+/v7y8fG55fUB/igsLEypqamaPn26kpOT1axZM23cuFG1a9eWJCUnJyspKclW7+/vr40bN2rs2LFasGCB/Pz8NG/ePLuvL4SEhGjVqlWaNGmSJk+erHr16mn16tUKCgqy1fz0008KDw9XcnKyPD091aJFC23evFkPP/xwYV8OAAAAAADKNIuRffW+AjAMQ48++qg2btyoli1bqlGjRjIMQwkJCTpy5Igee+wxrVu3rgTbvXOU1P0jgdKU2wXWcHdol7TE0S0UyJ5azzm6hQIr7de0yBcfLCvfiecaAwAAFFpJvQ8t1BkDy5cv11dffaXt27erS5cudsu++OIL9enTRytWrNCAAQOKrUEAAAAAAFByCnXxwZUrV2rChAk5QgFJevDBB/XSSy/pk08+KbbmAAAAAABAySpUMHD48GF17949z+U9evTQoUOHbrspAAAAAABQOgr1VYKLFy/K29s7z+Xe3t76+eefb7spAADMLu5kapHW25NRfNcOGfvwfcU2FwAAuHMV6oyBzMxMubjknSU4OzsrIyPjtpsCAAAAAAClo1BnDBiGoUGDBslqtea6PD09vViaAgAAAAAApaNQwcDAgQNvWcMdCQAAAAAAKDsKFQwsW7aspPoAAAAAAAAOUKhrDAAAAAAAgLsLwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYmIujGwAAALhj7Yh2dAcF1yXK0R0AAMoozhgAAAAAAMDECAYAAAAAADAxggEAAAAAAEyMYAAAAAAAABMjGAAAAAAAwMQIBgAAAAAAMDGCAQAAAAAATIxgAAAAAAAAEyMYAAAAAADAxAgGAAAAAAAwMYIBAAAAAABMjGAAAAAAAAATIxgAAAAAAMDECAYAAAAAADAxggEAAAAAAEyMYAAAAAAAABMjGAAAAAAAwMQIBgAAAAAAMDGCAQAAAAAATIxgAAAAAAAAE3NxdAMAStCO6HwXt0tKLaVGAJSWdklLim+yHV7FNxcAALhjccYAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiTk8GFi4cKH8/f3l7u6ugIAA7dy5M9/62NhYBQQEyN3dXXXr1tXixYtz1KxZs0ZNmjSR1WpVkyZNtHbtWrvl0dHRuv/++1WpUiVVr15dffr00fHjx4t1vwAAAAAAKAscGgysXr1aY8aM0cSJExUfH68OHTqoR48eSkpKyrU+MTFRPXv2VIcOHRQfH68JEyZo1KhRWrNmja0mLi5OYWFhCg8P16FDhxQeHq7+/ftr7969tprY2FiNGDFCe/bsUUxMjDIyMhQaGqqrV6+W+D4DAAAAAHAnsRiGYThq40FBQWrTpo0WLVpkG2vcuLH69Omj6OjoHPXjx4/Xhg0blJCQYBuLiIjQoUOHFBcXJ0kKCwtTWlqaNm3aZKvp3r27qlSpopUrV+bax/nz51W9enXFxsaqY8eOBeo9LS1Nnp6eunz5sjw8PAq0DlDqduT8/+j34k6mllIjQO721HrO0S0UWLukJY5uodQF1/VydAsojC5Rju4AAFDCSup9qMPOGLh+/boOHDig0NBQu/HQ0FDt3r0713Xi4uJy1Hfr1k379+/XjRs38q3Ja05Junz5siSpatWqedakp6crLS3N7gEAAAAAQFnnsGDgwoULyszMlLe3t924t7e3UlJScl0nJSUl1/qMjAxduHAh35q85jQMQ5GRkXrggQfUrFmzPPuNjo6Wp6en7VGzZs1b7iMAAAAAAHc6h1980GKx2D03DCPH2K3q/zhemDlHjhypw4cP5/k1g2xRUVG6fPmy7XHmzJl86wEAAAAAKAtcHLXhatWqydnZOccn+efOncvxiX82Hx+fXOtdXFzk5eWVb01ucz7//PPasGGDvvrqK9WoUSPffq1Wq6xW6y33CwAAAACAssRhZwy4ubkpICBAMTExduMxMTEKCQnJdZ3g4OAc9Vu3blVgYKBcXV3zrfn9nIZhaOTIkfr888/1xRdfyN/fvzh2CQAAAACAMsdhZwxIUmRkpMLDwxUYGKjg4GAtWbJESUlJioiIkHTz9P2zZ89qxYoVkm7egWD+/PmKjIzU0KFDFRcXp6VLl9p9DWD06NHq2LGjZs6cqd69e2v9+vXatm2bdu3aZasZMWKEPv30U61fv16VKlWynWHg6empcuXKleIrAAAAAACAYzk0GAgLC1NqaqqmT5+u5ORkNWvWTBs3blTt2rUlScnJyUpKSrLV+/v7a+PGjRo7dqwWLFggPz8/zZs3T3379rXVhISEaNWqVZo0aZImT56sevXqafXq1QoKCrLVZN8esXPnznb9LFu2TIMGDSq5HQYAAAAA4A5jMbKv3odCKan7RwLFakd0vovjTqaWUiNA7vbUes7RLRRYu6Qljm6h1AXX9XJ0CyiMLlGO7gAAUMJK6n2ow+9KAAAAAAAAHIdgAAAAAAAAEyMYAAAAAADAxAgGAAAAAAAwMYIBAAAAAABMjGAAAAAAAAATIxgAAAAAAMDECAYAAAAAADAxggEAAAAAAEzMxdENAAAAwER2RDu6g4LpEuXoDgCg1HDGAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACbm4ugGAADm1S5piaNbAAAAMD3OGAAAAAAAwMQIBgAAAAAAMDGCAQAAAAAATIxgAAAAAAAAEyMYAAAAAADAxAgGAAAAAAAwMYIBAAAAAABMzMXRDQAAgDtT3MlUR7eg4Lpejm4BAIC7HmcMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIm5OLoBAACAvMSdTHV0C5Kk4Lpejm4BAIASwxkDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYmMODgYULF8rf31/u7u4KCAjQzp07862PjY1VQECA3N3dVbduXS1evDhHzZo1a9SkSRNZrVY1adJEa9eutVv+1Vdf6dFHH5Wfn58sFovWrVtXnLsEAAAAAECZ4dBgYPXq1RozZowmTpyo+Ph4dejQQT169FBSUlKu9YmJierZs6c6dOig+Ph4TZgwQaNGjdKaNWtsNXFxcQoLC1N4eLgOHTqk8PBw9e/fX3v37rXVXL16VS1bttT8+fNLfB8BAAAAALiTWQzDMBy18aCgILVp00aLFi2yjTVu3Fh9+vRRdHR0jvrx48drw4YNSkhIsI1FRETo0KFDiouLkySFhYUpLS1NmzZtstV0795dVapU0cqVK3PMabFYtHbtWvXp06dQvaelpcnT01OXL1+Wh4dHodYFSs2OnP8f/V7cydRSagQAyrbgul6ObuHWukQ5uoOCucXfTXeMsvJ6AjCVknof6rAzBq5fv64DBw4oNDTUbjw0NFS7d+/OdZ24uLgc9d26ddP+/ft148aNfGvymrOg0tPTlZaWZvcAAAAAAKCsc1gwcOHCBWVmZsrb29tu3NvbWykpKbmuk5KSkmt9RkaGLly4kG9NXnMWVHR0tDw9PW2PmjVr3tZ8AAAAAADcCRx+8UGLxWL33DCMHGO3qv/jeGHnLIioqChdvnzZ9jhz5sxtzQcAAAAAwJ3AxVEbrlatmpydnXN8kn/u3Lkcn/hn8/HxybXexcVFXl5e+dbkNWdBWa1WWa3W25oDAAAAAIA7jcPOGHBzc1NAQIBiYmLsxmNiYhQSEpLrOsHBwTnqt27dqsDAQLm6uuZbk9ecAAAAAACYmcPOGJCkyMhIhYeHKzAwUMHBwVqyZImSkpIUEREh6ebp+2fPntWKFSsk3bwDwfz58xUZGamhQ4cqLi5OS5cutbvbwOjRo9WxY0fNnDlTvXv31vr167Vt2zbt2rXLVvPLL7/ov//9r+15YmKiDh48qKpVq6pWrVqltPcAAAAAADieQ4OBsLAwpaamavr06UpOTlazZs20ceNG1a5dW5KUnJyspKQkW72/v782btyosWPHasGCBfLz89O8efPUt29fW01ISIhWrVqlSZMmafLkyapXr55Wr16toKAgW83+/fvVpUsX2/PIyEhJ0sCBA7V8+fIS3msAAAAAAO4cFiP76n0olJK6fyRQrG5xr+i4k6ml1AgAlG3Bdb0c3cKtdYlydAcFc4u/m+4YZeX1BGAqJfU+1KFnDAAAAKCYlJU33GUFr2fxI2wB7lgOv10hAAAAAABwHIIBAAAAAABMjGAAAAAAAAATIxgAAAAAAMDECAYAAAAAADAxggEAAAAAAEyMYAAAAAAAABMjGAAAAAAAwMQIBgAAAAAAMDGCAQAAAAAATMzF0Q0ApW1OzHeObkFjH77P0S0AAAoh7mSqo1tQcF0vR7cAALhLccYAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmJiLoxsAzGhOzHelsp12Samlsh0AAAAAZRdnDAAAAAAAYGKcMWAGO6Id3UHBdYlydAcAAAAAilNZeT9i4vcinDEAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJiYi6MbAAAAAAoq7mSqo1tQcF0vR7cAAMWKMwYAAAAAADAxggEAAAAAAEyMYAAAAAAAABMjGAAAAAAAwMS4+CBQBO2Slji6BQAAgLJlR7SjOyiYLlGO7gAodZwxAAAAAACAiREMAAAAAABgYgQDAAAAAACYGNcYAAAAKAPiTqY6ugUAwF2KMwYAAAAAADAxggEAAAAAAEyMYAAAAAAAABMjGAAAAAAAwMQIBgAAAAAAMDGCAQAAAAAATIxgAAAAAAAAEyMYAAAAAADAxFwc3QBKT9zJVEe3oOC6Xo5uAQAA4LbcCf+mulPwbzvg7sAZAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYg4PBhYuXCh/f3+5u7srICBAO3fuzLc+NjZWAQEBcnd3V926dbV48eIcNWvWrFGTJk1ktVrVpEkTrV279ra3CwAAAADA3cihwcDq1as1ZswYTZw4UfHx8erQoYN69OihpKSkXOsTExPVs2dPdejQQfHx8ZowYYJGjRqlNWvW2Gri4uIUFham8PBwHTp0SOHh4erfv7/27t1b5O0CAAAAAHC3shiGYThq40FBQWrTpo0WLVpkG2vcuLH69Omj6OjoHPXjx4/Xhg0blJCQYBuLiIjQoUOHFBcXJ0kKCwtTWlqaNm3aZKvp3r27qlSpopUrVxZpu7lJS0uTp6enLl++LA8Pj8LteGnbcXOf7oRb69zyljZdokq8hzkx3932HO2SlhRDJwAAAGXbXXm7wlL496jp7CjYeyyHKwM/+5J6H+pSbDMV0vXr13XgwAG99NJLduOhoaHavXt3ruvExcUpNDTUbqxbt25aunSpbty4IVdXV8XFxWns2LE5aubOnVvk7UpSenq60tPTbc8vX74s6eYP5o539beb//k1/RaFJS/t//eSd0HJv56/Xf3ltue4E15LAAAAR7vlv+3KorLw7/uypqwcJ2XgZ5/9/rO4P993WDBw4cIFZWZmytvb227c29tbKSkpua6TkpKSa31GRoYuXLggX1/fPGuy5yzKdiUpOjpa06ZNyzFes2bNvHcSRTDd0Q0AAADA1Pj3qHmVnZ/9lStX5OnpWWzzOSwYyGaxWOyeG4aRY+xW9X8cL8ichd1uVFSUIiMjbc+zsrJ08eJFeXl55btebtLS0lSzZk2dOXPmzv8aAkoMxwGycSxA4jjA/3AsQOI4wP9wLED633GQlJQki8UiPz+/Yp3fYcFAtWrV5OzsnONT+nPnzuX4ND+bj49PrvUuLi7y8vLKtyZ7zqJsV5KsVqusVqvdWOXKlfPewQLw8PDgf25wHMCGYwESxwH+h2MBEscB/odjAZLk6elZIseBw+5K4ObmpoCAAMXExNiNx8TEKCQkJNd1goODc9Rv3bpVgYGBcnV1zbcme86ibBcAAAAAgLuVQ79KEBkZqfDwcAUGBio4OFhLlixRUlKSIiIiJN08ff/s2bNasWKFpJt3IJg/f74iIyM1dOhQxcXFaenSpba7DUjS6NGj1bFjR82cOVO9e/fW+vXrtW3bNu3atavA2wUAAAAAwCwcGgyEhYUpNTVV06dPV3Jyspo1a6aNGzeqdu3akqTk5GQlJSXZ6v39/bVx40aNHTtWCxYskJ+fn+bNm6e+ffvaakJCQrRq1SpNmjRJkydPVr169bR69WoFBQUVeLslzWq1asqUKTm+mgBz4ThANo4FSBwH+B+OBUgcB/gfjgVIJX8cWIzivs8BAAAAAAAoMxx2jQEAAAAAAOB4BAMAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDJSyhQsXyt/fX+7u7goICNDOnTsd3RKK2VdffaVHH31Ufn5+slgsWrdund1ywzA0depU+fn5qVy5curcubOOHj1qV5Oenq7nn39e1apVU4UKFfTYY4/phx9+KMW9wO2Kjo7W/fffr0qVKql69erq06ePjh8/blfDsXD3W7RokVq0aCEPDw95eHgoODhYmzZtsi3nGDCn6OhoWSwWjRkzxjbGsWAOU6dOlcVisXv4+PjYlnMcmMvZs2f1zDPPyMvLS+XLl1erVq104MAB23KOh7tfnTp1cvxOsFgsGjFihKTSPQYIBkrR6tWrNWbMGE2cOFHx8fHq0KGDevToYXdLRpR9V69eVcuWLTV//vxcl8+aNUuzZ8/W/PnztW/fPvn4+Ojhhx/WlStXbDVjxozR2rVrtWrVKu3atUu//PKLevXqpczMzNLaDdym2NhYjRgxQnv27FFMTIwyMjIUGhqqq1ev2mo4Fu5+NWrU0Ouvv679+/dr//79evDBB9W7d2/bX+ocA+azb98+LVmyRC1atLAb51gwj6ZNmyo5Odn2OHLkiG0Zx4F5/Pzzz2rfvr1cXV21adMmHTt2TG+99ZYqV65sq+F4uPvt27fP7vdBTEyMJOlPf/qTpFI+BgyUmrZt2xoRERF2Y40aNTJeeuklB3WEkibJWLt2re15VlaW4ePjY7z++uu2sd9++83w9PQ0Fi9ebBiGYVy6dMlwdXU1Vq1aZas5e/as4eTkZGzevLnUekfxOnfunCHJiI2NNQyDY8HMqlSpYrz//vscAyZ05coVo0GDBkZMTIzRqVMnY/To0YZh8PvATKZMmWK0bNky12UcB+Yyfvx444EHHshzOceDOY0ePdqoV6+ekZWVVerHAGcMlJLr16/rwIEDCg0NtRsPDQ3V7t27HdQVSltiYqJSUlLsjgOr1apOnTrZjoMDBw7oxo0bdjV+fn5q1qwZx0oZdvnyZUlS1apVJXEsmFFmZqZWrVqlq1evKjg4mGPAhEaMGKFHHnlEDz30kN04x4K5nDhxQn5+fvL399eTTz6pkydPSuI4MJsNGzYoMDBQf/rTn1S9enW1bt1a7733nm05x4P5XL9+XR9//LEGDx4si8VS6scAwUApuXDhgjIzM+Xt7W037u3trZSUFAd1hdKW/bPO7zhISUmRm5ubqlSpkmcNyhbDMBQZGakHHnhAzZo1k8SxYCZHjhxRxYoVZbVaFRERobVr16pJkyYcAyazatUqffPNN4qOjs6xjGPBPIKCgrRixQpt2bJF7733nlJSUhQSEqLU1FSOA5M5efKkFi1apAYNGmjLli2KiIjQqFGjtGLFCkn8XjCjdevW6dKlSxo0aJCk0j8GXIrYN4rIYrHYPTcMI8cY7n5FOQ44VsqukSNH6vDhw9q1a1eOZRwLd7+GDRvq4MGDunTpktasWaOBAwcqNjbWtpxj4O535swZjR49Wlu3bpW7u3uedRwLd78ePXrY/ty8eXMFBwerXr16+vDDD9WuXTtJHAdmkZWVpcDAQM2YMUOS1Lp1ax09elSLFi3SgAEDbHUcD+axdOlS9ejRQ35+fnbjpXUMcMZAKalWrZqcnZ1zJDfnzp3LkQLh7pV95eH8jgMfHx9dv35dP//8c541KDuef/55bdiwQTt27FCNGjVs4xwL5uHm5qb69esrMDBQ0dHRatmypd5++22OARM5cOCAzp07p4CAALm4uMjFxUWxsbGaN2+eXFxcbD9LjgXzqVChgpo3b64TJ07wO8FkfH191aRJE7uxxo0b2y5KzvFgLqdPn9a2bds0ZMgQ21hpHwMEA6XEzc1NAQEBtitNZouJiVFISIiDukJp8/f3l4+Pj91xcP36dcXGxtqOg4CAALm6utrVJCcn6z//+Q/HShliGIZGjhypzz//XF988YX8/f3tlnMsmJdhGEpPT+cYMJGuXbvqyJEjOnjwoO0RGBiop59+WgcPHlTdunU5FkwqPT1dCQkJ8vX15XeCybRv3z7HbYy/++471a5dWxL/TjCbZcuWqXr16nrkkUdsY6V+DBTlaokomlWrVhmurq7G0qVLjWPHjhljxowxKlSoYJw6dcrRraEYXblyxYiPjzfi4+MNScbs2bON+Ph44/Tp04ZhGMbrr79ueHp6Gp9//rlx5MgR46mnnjJ8fX2NtLQ02xwRERFGjRo1jG3bthnffPON8eCDDxotW7Y0MjIyHLVbKKS//vWvhqenp/Hll18aycnJtse1a9dsNRwLd7+oqCjjq6++MhITE43Dhw8bEyZMMJycnIytW7cahsExYGa/vyuBYXAsmMW4ceOML7/80jh58qSxZ88eo1evXkalSpVs/xbkODCPr7/+2nBxcTFee+0148SJE8Ynn3xilC9f3vj4449tNRwP5pCZmWnUqlXLGD9+fI5lpXkMEAyUsgULFhi1a9c23NzcjDZt2thuXYa7x44dOwxJOR4DBw40DOPm7WemTJli+Pj4GFar1ejYsaNx5MgRuzl+/fVXY+TIkUbVqlWNcuXKGb169TKSkpIcsDcoqtyOAUnGsmXLbDUcC3e/wYMH237n33PPPUbXrl1toYBhcAyY2R+DAY4FcwgLCzN8fX0NV1dXw8/Pz3jiiSeMo0eP2pZzHJjLP/7xD6NZs2aG1Wo1GjVqZCxZssRuOceDOWzZssWQZBw/fjzHstI8BiyGYRiFPtcBAAAAAADcFbjGAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJEQwAAAAAAGBiBAMAAAAAAJgYwQAAACZ36tQpWSwWHTx40NGt2Hz77bdq166d3N3d1apVq2Kf32KxaN26dcU+LwAAZRHBAAAADjZo0CBZLBa9/vrrduPr1q2TxWJxUFeONWXKFFWoUEHHjx/X9u3bcyy3WCz5PgYNGlT6TQMAUEYRDAAAcAdwd3fXzJkz9fPPPzu6lWJz/fr1Iq/7/fff64EHHlDt2rXl5eWVY3lycrLtMXfuXHl4eNiNvf3227fTOgAApkIwAADAHeChhx6Sj4+PoqOj86yZOnVqjtPq586dqzp16tieDxo0SH369NGMGTPk7e2typUra9q0acrIyNCLL76oqlWrqkaNGvrggw9yzP/tt98qJCRE7u7uatq0qb788ku75ceOHVPPnj1VsWJFeXt7Kzw8XBcuXLAt79y5s0aOHKnIyEhVq1ZNDz/8cK77kZWVpenTp6tGjRqyWq1q1aqVNm/ebFtusVh04MABTZ8+XRaLRVOnTs0xh4+Pj+3h6ekpi8ViN/bpp5+qXr16cnNzU8OGDfXRRx/l+bpK0vTp0+Xt7W37OsXu3bvVsWNHlStXTjVr1tSoUaN09epVW32dOnU0Y8YMDR48WJUqVVKtWrW0ZMkS2/Lr169r5MiR8vX1lbu7u+rUqZPvzxYAAEciGAAA4A7g7OysGTNm6J133tEPP/xwW3N98cUX+vHHH/XVV19p9uzZmjp1qnr16qUqVapo7969ioiIUEREhM6cOWO33osvvqhx48YpPj5eISEheuyxx5Samirp5if0nTp1UqtWrbR//35t3rxZP/30k/r37283x4cffigXFxf9+9//1rvvvptrf2+//bbeeustvfnmmzp8+LC6deumxx57TCdOnLBtq2nTpho3bpySk5P1wgsvFGr/165dq9GjR2vcuHH6z3/+o2HDhukvf/mLduzYkaPWMAyNHj1aS5cu1a5du9SqVSsdOXJE3bp10xNPPKHDhw9r9erV2rVrl0aOHGm37ltvvaXAwEDFx8dr+PDh+utf/6pvv/1WkjRv3jxt2LBBn332mY4fP66PP/7YLsABAOCOYgAAAIcaOHCg0bt3b8MwDKNdu3bG4MGDDcMwjLVr1xq//6t6ypQpRsuWLe3WnTNnjlG7dm27uWrXrm1kZmbaxho2bGh06NDB9jwjI8OoUKGCsXLlSsMwDCMxMdGQZLz++uu2mhs3bhg1atQwZs6caRiGYUyePNkIDQ212/aZM2cMScbx48cNwzCMTp06Ga1atbrl/vr5+Rmvvfaa3dj9999vDB8+3Pa8ZcuWxpQpU245l2EYxrJlywxPT0/b85CQEGPo0KF2NX/605+Mnj172p5LMv7+978bzzzzjNGoUSPjzJkztmXh4eHGc889Z7f+zp07DScnJ+PXX381DMMwateubTzzzDO25VlZWUb16tWNRYsWGYZhGM8//7zx4IMPGllZWQXaBwAAHIkzBgAAuIPMnDlTH374oY4dO1bkOZo2bSonp//9Fe/t7a3mzZvbnjs7O8vLy0vnzp2zWy84ONj2ZxcXFwUGBiohIUGSdODAAe3YsUMVK1a0PRo1aiTp5vUAsgUGBubbW1pamn788Ue1b9/ebrx9+/a2bd2uhISEAs0/duxYxcXFaefOnapRo4Zt/MCBA1q+fLndvnbr1k1ZWVlKTEy01bVo0cL25+yvMmS/poMGDdLBgwfVsGFDjRo1Slu3bi2WfQMAoCQQDAAAcAfp2LGjunXrpgkTJuRY5uTkJMMw7MZu3LiRo87V1dXuucViyXUsKyvrlv1k3xUhKytLjz76qA4ePGj3OHHihDp27Girr1Chwi3n/P282QzDKNY7MBRk/ocfflhnz57Vli1b7MazsrI0bNgwu/08dOiQTpw4oXr16tnq8ntN27Rpo8TERL3yyiv69ddf1b9/f/Xr16/Y9g8AgOLk4ugGAACAvddff12tWrXSfffdZzd+zz33KCUlxe5NbvbF8orDnj17bG/yMzIydODAAdv36tu0aaM1a9aoTp06cnEp+j8fPDw85Ofnp127dtkFCrt371bbtm1vbwf+v8aNG2vXrl0aMGCA3fyNGze2q3vsscf06KOP6s9//rOcnZ315JNPSrq5r0ePHlX9+vVvqw8PDw+FhYUpLCxM/fr1U/fu3XXx4kVVrVr1tuYFAKC4EQwAAHCHad68uZ5++mm98847duOdO3fW+fPnNWvWLPXr10+bN2/Wpk2b5OHhUSzbXbBggRo0aKDGjRtrzpw5+vnnnzV48GBJ0ogRI/Tee+/pqaee0osvvqhq1arpv//9r1atWqX33ntPzs7OBd7Oiy++qClTpqhevXpq1aqVli1bpoMHD+qTTz4plv148cUX1b9/f7Vp00Zdu3bVP/7xD33++efatm1bjtrHH39cH330kcLDw+Xi4qJ+/fpp/PjxateunUaMGKGhQ4eqQoUKSkhIUExMTI6fSV7mzJkjX19ftWrVSk5OTvr73/8uHx8fVa5cuVj2EQCA4sRXCQAAuAO98sorOb420LhxYy1cuFALFixQy5Yt9fXXXxf6iv35ef311zVz5ky1bNlSO3fu1Pr161WtWjVJkp+fn/79738rMzNT3bp1U7NmzTR69Gh5enraXc+gIEaNGqVx48Zp3Lhxat68uTZv3qwNGzaoQYMGxbIfffr00dtvv6033nhDTZs21bvvvqtly5apc+fOudb369dPH374ocLDw/X555+rRYsWio2N1YkTJ9ShQwe1bt1akydPlq+vb4F7qFixombOnKnAwEDdf//9OnXqlDZu3Fjo1woAgNJgMf74rw4AAAAAAGAaxNYAAAAAAJgYwQAAAAAAACZGMAAAAAAAgIkRDAAAAAAAYGIEAwAAAAAAmBjBAAAAAAAAJkYwAAAAAACAiREMAAAAAABgYgQDAAAAAACYGMEAAAAAAAAmRjAAAAAAAICJ/T/CbctDapikgwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Dictionary to store the lyrics data\n",
    "song_lengths = defaultdict(list)\n",
    "\n",
    "# Read lyrics data\n",
    "for artist_folder in os.listdir(lyrics_folder):\n",
    "    artist_path = os.path.join(lyrics_folder, artist_folder)\n",
    "    \n",
    "    if os.path.isdir(artist_path):\n",
    "        for song_file in os.listdir(artist_path):\n",
    "            song_path = os.path.join(artist_path, song_file)\n",
    "            with open(song_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                song_name = song_file.replace(\".txt\", \"\")\n",
    "                lyrics = file.read()\n",
    "                token_count = len(tokenize(lyrics))\n",
    "                song_lengths[artist_folder].append(token_count)\n",
    "                \n",
    "# Convert song lengths data into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'artist': [artist for artist, lengths in song_lengths.items() for _ in lengths],\n",
    "    'length': [length for lengths in song_lengths.values() for length in lengths]\n",
    "})\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.groupby('artist')['length'].plot(kind='hist', density=True, alpha=0.5, legend=True, bins=20)\n",
    "plt.title('Histogram of Song Lengths by Artist')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: This regular expression matches any whitespace character, including spaces, tabs, newlines, and other Unicode whitespace characters. The '+' is a quantifier that means \"one or more.\" It makes the regular expression match consecutive occurrences of whitespace characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2294c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "# Dictionary to store the tokenized lyrics data\n",
    "song_lengths = defaultdict(list)\n",
    "\n",
    "# Read lyrics data\n",
    "for artist_folder in os.listdir(lyrics_folder):\n",
    "    artist_path = os.path.join(lyrics_folder, artist_folder)\n",
    "    \n",
    "    if os.path.isdir(artist_path):\n",
    "        for song_file in os.listdir(artist_path):\n",
    "            song_path = os.path.join(artist_path, song_file)\n",
    "            with open(song_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                lyrics = file.read()\n",
    "                tokens = tokenize_lyrics(lyrics)\n",
    "                token_count = len(tokens)\n",
    "                song_lengths[artist_folder].append(token_count)\n",
    "\n",
    "# Convert song lengths data into a DataFrame\n",
    "df2 = pd.DataFrame({\n",
    "    'artist': [artist for artist, lengths in song_lengths.items() for _ in lengths],\n",
    "    'length': [length for lengths in song_lengths.values() for length in lengths]\n",
    "})\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "df2.groupby('artist')['length'].plot(kind='hist', density=True, alpha=0.5, legend=True, bins=20)\n",
    "plt.title('Histogram of Song Lengths by Artist')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
